[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python for Data Science in Finance",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html",
    "href": "chapters/01_jumpstart/jumpstart.html",
    "title": "1  Python Jumpstart",
    "section": "",
    "text": "1.1 What is a Jupyter Notebook?\nThe purpose of this chapter is to introduce Jupyter notebook files and to give a glimpse of how to use them to work with financial data.\nIn particular, we will visualize stock index data to observe the leverage effect: when the market suffers losses, prices become more volatile.\nWe will move quickly without explaining all the details, so don’t worry if you aren’t able to follow everything. It may be worth coming back to this after you have completed the Basic Data Wrangling part of the the book.\nThe notebook format conveniently allows you to combine sentences, code, code outputs (including plots), and mathematical notation. Notebooks have proven to be a convenient and productive programming environment for data analysis.\nBehind the scenes of a Jupyter notebook is a kernel that is responsible for executing computations. The kernel can live locally on your machine or on a remote server.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#ides-for-jupyter-notebooks",
    "href": "chapters/01_jumpstart/jumpstart.html#ides-for-jupyter-notebooks",
    "title": "1  Python Jumpstart",
    "section": "1.2 IDEs for Jupyter Notebooks",
    "text": "1.2 IDEs for Jupyter Notebooks\nYou will need another piece of software called an integrated development environment (IDE) to actually work with Jupyter notebooks; here are three popular and free IDEs for working with them:\n\nJupyterLab - my personal favorite, created by the Jupyter project, which also creates the Jupyter notebook format.\nJupyter Notebook Classic - this was the predecessor to JupyterLab, also created by the Jupyter project.\nVSCode - an general purpose IDE created my Microsoft.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#code-cells",
    "href": "chapters/01_jumpstart/jumpstart.html#code-cells",
    "title": "1  Python Jumpstart",
    "section": "1.3 Code Cells",
    "text": "1.3 Code Cells\nA notebook is structured as a sequence of cells. There are three kinds of cells: 1) code cells that contain code; 2) markdown cells that contain markdown or latex; and 3) raw cells that contain raw text. We will work mainly with code cells and markdown cells.\nThe cell below is a code cell - try typing the code and then press shift + enter.\n\nfrom IPython.display import Image\nImage(\"not_ethical.png\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#edit-mode-vs-command-mode",
    "href": "chapters/01_jumpstart/jumpstart.html#edit-mode-vs-command-mode",
    "title": "1  Python Jumpstart",
    "section": "1.4 Edit Mode vs Command Mode",
    "text": "1.4 Edit Mode vs Command Mode\nThere are two modes in a notebook: 1) edit mode; 2) command mode.\nIn edit mode you are inside a cell and you can edit the contents of the cell.\nIn command mode, you are outside the cells and you can navigate between them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#keyboard-shortcuts",
    "href": "chapters/01_jumpstart/jumpstart.html#keyboard-shortcuts",
    "title": "1  Python Jumpstart",
    "section": "1.5 Keyboard Shortcuts",
    "text": "1.5 Keyboard Shortcuts\nHere are some of my favorite JupyterLab keyboard shortcuts:\nedit mode: enter\ncommand mode: esc\nnavigate up: k\nnavigate down: j\ninsert cell above: a\ninsert cell below: b\ndelete cell: d, d (press d twice)\nswitch to code cell: y\nswitch to markup cell: m\nexecute and stay on current cell: ctrl + enter\nexecute and move down a cell: shift + enter",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#drop-down-menus",
    "href": "chapters/01_jumpstart/jumpstart.html#drop-down-menus",
    "title": "1  Python Jumpstart",
    "section": "1.6 Drop Down Menus",
    "text": "1.6 Drop Down Menus\nHere are a few of the drop down menu functions in JupyterLab that I use frequently:\nKernel &gt; Restart Kernel and Clear All Outputs\nKernel &gt; Restart Kearnel and Run All Cells\nRun &gt; Run All Above Selected Cell",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#importing-packages",
    "href": "chapters/01_jumpstart/jumpstart.html#importing-packages",
    "title": "1  Python Jumpstart",
    "section": "1.7 Importing Packages",
    "text": "1.7 Importing Packages\nThe power and convenience of Python as a data analysis language comes from the ecosystem of freely available third party packages.\nHere are the packages that we will be using in this tutorial:\nnumpy - efficient vector and matrix computations\npandas - working with DataFrames\nyfinance - reading in data from Yahoo finance\npandas_datareader - also for reading data from Yahoo Finance\nThe following code imports these packages and assigns them each an alias.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom pandas_datareader import data as pdr",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#reading-in-stock-data-into-a-dataframe",
    "href": "chapters/01_jumpstart/jumpstart.html#reading-in-stock-data-into-a-dataframe",
    "title": "1  Python Jumpstart",
    "section": "1.8 Reading-In Stock Data into a DataFrame",
    "text": "1.8 Reading-In Stock Data into a DataFrame\nLet’s begin by reading in 5 years of SPY price data from Yahoo Finance.\nSPY is an ETF that tracks the performace of the SP500 stock index.\n\ndf_spy = yf.download('SPY', start='2013-12-31', end='2019-01-01', auto_adjust=False, rounding=True)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nTicker\nSPY\nSPY\nSPY\nSPY\nSPY\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2013-12-31\n151.29\n184.69\n184.69\n183.93\n184.07\n86119900\n\n\n2014-01-02\n149.84\n182.92\n184.07\n182.48\n183.98\n119636900\n\n\n2014-01-03\n149.82\n182.89\n183.60\n182.63\n183.23\n81390600\n\n\n2014-01-06\n149.38\n182.36\n183.56\n182.08\n183.49\n108028200\n\n\n2014-01-07\n150.30\n183.48\n183.79\n182.95\n183.09\n86144200\n\n\n\n\n\n\n\nOur stock data now lives in the variable called df_spy, which is a pandas data structure known as a DataFrame. We can see this by using the following code:\n\ntype(df_spy)\n\npandas.core.frame.DataFrame",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#dataframe-index",
    "href": "chapters/01_jumpstart/jumpstart.html#dataframe-index",
    "title": "1  Python Jumpstart",
    "section": "1.9 DataFrame Index",
    "text": "1.9 DataFrame Index\nIn pandas, a DataFrame always has an index. For df_spy the Dates form the index.\n\ndf_spy.index\n\nDatetimeIndex(['2013-12-31', '2014-01-02', '2014-01-03', '2014-01-06',\n               '2014-01-07', '2014-01-08', '2014-01-09', '2014-01-10',\n               '2014-01-13', '2014-01-14',\n               ...\n               '2018-12-17', '2018-12-18', '2018-12-19', '2018-12-20',\n               '2018-12-21', '2018-12-24', '2018-12-26', '2018-12-27',\n               '2018-12-28', '2018-12-31'],\n              dtype='datetime64[ns]', name='Date', length=1259, freq=None)\n\n\nI don’t use indices very much, so let’s make the Date index just a regular column. Notice that we can modify DataFrames inplace.\n\ndf_spy.reset_index(inplace=True)\ndf_spy\n\n\n\n\n\n\n\nPrice\nDate\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nTicker\n\nSPY\nSPY\nSPY\nSPY\nSPY\nSPY\n\n\n\n\n0\n2013-12-31\n151.29\n184.69\n184.69\n183.93\n184.07\n86119900\n\n\n1\n2014-01-02\n149.84\n182.92\n184.07\n182.48\n183.98\n119636900\n\n\n2\n2014-01-03\n149.82\n182.89\n183.60\n182.63\n183.23\n81390600\n\n\n3\n2014-01-06\n149.38\n182.36\n183.56\n182.08\n183.49\n108028200\n\n\n4\n2014-01-07\n150.30\n183.48\n183.79\n182.95\n183.09\n86144200\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1254\n2018-12-24\n211.96\n234.34\n240.84\n234.27\n239.04\n147311600\n\n\n1255\n2018-12-26\n222.67\n246.18\n246.18\n233.76\n235.97\n218485400\n\n\n1256\n2018-12-27\n224.38\n248.07\n248.29\n238.96\n242.57\n186267300\n\n\n1257\n2018-12-28\n224.09\n247.75\n251.40\n246.45\n249.58\n153100200\n\n\n1258\n2018-12-31\n226.05\n249.92\n250.19\n247.47\n249.56\n144299400\n\n\n\n\n1259 rows × 7 columns\n\n\n\nNotice that even though we ran the .reset_index() method of df_spy it still has an index; now its index is just a sequence of integers.\n\ndf_spy.index\n\nRangeIndex(start=0, stop=1259, step=1)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#a-bit-of-cleaning",
    "href": "chapters/01_jumpstart/jumpstart.html#a-bit-of-cleaning",
    "title": "1  Python Jumpstart",
    "section": "1.10 A Bit of Cleaning",
    "text": "1.10 A Bit of Cleaning\nFor an additional bit of clean-up, let’s drop the second level of the column index from df_spy, and also remove the Price label from the row index.\n\ndf_spy = df_spy.droplevel(level=1, axis=1)\ndf_spy = df_spy.rename_axis(None, axis=1)\ndf_spy\n\n\n\n\n\n\n\n\nDate\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\n\n0\n2013-12-31\n151.29\n184.69\n184.69\n183.93\n184.07\n86119900\n\n\n1\n2014-01-02\n149.84\n182.92\n184.07\n182.48\n183.98\n119636900\n\n\n2\n2014-01-03\n149.82\n182.89\n183.60\n182.63\n183.23\n81390600\n\n\n3\n2014-01-06\n149.38\n182.36\n183.56\n182.08\n183.49\n108028200\n\n\n4\n2014-01-07\n150.30\n183.48\n183.79\n182.95\n183.09\n86144200\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1254\n2018-12-24\n211.96\n234.34\n240.84\n234.27\n239.04\n147311600\n\n\n1255\n2018-12-26\n222.67\n246.18\n246.18\n233.76\n235.97\n218485400\n\n\n1256\n2018-12-27\n224.38\n248.07\n248.29\n238.96\n242.57\n186267300\n\n\n1257\n2018-12-28\n224.09\n247.75\n251.40\n246.45\n249.58\n153100200\n\n\n1258\n2018-12-31\n226.05\n249.92\n250.19\n247.47\n249.56\n144299400\n\n\n\n\n1259 rows × 7 columns\n\n\n\nAs a matter of preference, I like my column names to be in snake case.\n\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ','_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2013-12-31\n151.29\n184.69\n184.69\n183.93\n184.07\n86119900\n\n\n1\n2014-01-02\n149.84\n182.92\n184.07\n182.48\n183.98\n119636900\n\n\n2\n2014-01-03\n149.82\n182.89\n183.60\n182.63\n183.23\n81390600\n\n\n3\n2014-01-06\n149.38\n182.36\n183.56\n182.08\n183.49\n108028200\n\n\n4\n2014-01-07\n150.30\n183.48\n183.79\n182.95\n183.09\n86144200\n\n\n\n\n\n\n\nLet’s also remove the columns that we won’t need. We first create a list of the column names that we want to get rid of and then we use the DataFrame.drop() method.\n\nlst_cols = ['high', 'low', 'open', 'adj_close', 'volume',]\ndf_spy.drop(columns=lst_cols, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nclose\n\n\n\n\n0\n2013-12-31\n184.69\n\n\n1\n2014-01-02\n182.92\n\n\n2\n2014-01-03\n182.89\n\n\n3\n2014-01-06\n182.36\n\n\n4\n2014-01-07\n183.48\n\n\n\n\n\n\n\nNotice that trailing commas do not cause errors in Python.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#series",
    "href": "chapters/01_jumpstart/jumpstart.html#series",
    "title": "1  Python Jumpstart",
    "section": "1.11 Series",
    "text": "1.11 Series\nYou can isolate the columns of a DataFrame with square brackets as follows:\n\ndf_spy['close']\n\n0       184.69\n1       182.92\n2       182.89\n3       182.36\n4       183.48\n         ...  \n1254    234.34\n1255    246.18\n1256    248.07\n1257    247.75\n1258    249.92\nName: close, Length: 1259, dtype: float64\n\n\nThe columns of a DataFrame are a pandas data structure called a Series.\n\ntype(df_spy['close'])\n\npandas.core.series.Series",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#numpy-and-ndarrays",
    "href": "chapters/01_jumpstart/jumpstart.html#numpy-and-ndarrays",
    "title": "1  Python Jumpstart",
    "section": "1.12 numpy and ndarrays",
    "text": "1.12 numpy and ndarrays\nPython is a general purpose programming language and was not created for scientific computing in particular. One of the foundational packages that makes Python well suited to scientific computing is numpy, which has a variety of features including a data type called ndarrays. One of the benefits of ndarrays is that they allow for efficient vector and matrix computation.\nThe values of a Series object is a numpy.ndarray. This is one sense in which pandas is built on top of numpy.\n\ndf_spy['close'].values\n\narray([184.69, 182.92, 182.89, ..., 248.07, 247.75, 249.92])\n\n\n\ntype(df_spy['close'].values)\n\nnumpy.ndarray",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#series-built-in-methods",
    "href": "chapters/01_jumpstart/jumpstart.html#series-built-in-methods",
    "title": "1  Python Jumpstart",
    "section": "1.13 Series Built-In Methods",
    "text": "1.13 Series Built-In Methods\nSeries have a variety of built-in methods that provide convenient summarization and modification functionality. For example, you can .sum() all the elements of the Series.\n\ndf_spy['close'].sum()\n\n283892.12\n\n\nNext, we calculate the standard deviation of all the elements of the Series using the .std() method.\n\ndf_spy['close'].std()\n\n31.134513551265016\n\n\nThe .shift() built-in method will be useful for calculating returns in the next section - it has the effect of pushing down the values in a Series.\n\ndf_spy['close'].shift()\n\n0          NaN\n1       184.69\n2       182.92\n3       182.89\n4       182.36\n         ...  \n1254    240.70\n1255    234.34\n1256    246.18\n1257    248.07\n1258    247.75\nName: close, Length: 1259, dtype: float64",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#calculating-daily-returns",
    "href": "chapters/01_jumpstart/jumpstart.html#calculating-daily-returns",
    "title": "1  Python Jumpstart",
    "section": "1.14 Calculating Daily Returns",
    "text": "1.14 Calculating Daily Returns\nOur analysis analysis of the leverage effect will involve daily returns for all the days in df_spy. Let’s calculate those now.\nRecall that the end-of-day day \\(t\\) return of a stock is defined as: \\(r_{t} = \\frac{S_{t}}{S_{t-1}} - 1\\), where \\(S_{t}\\) is the stock price at end-of-day \\(t\\).\nHere is a vectorized approach to calculating all the daily returns in a single line of code.\n\ndf_spy['ret'] = df_spy['close'] / df_spy['close'].shift(1) - 1\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nclose\nret\n\n\n\n\n0\n2013-12-31\n184.69\nNaN\n\n\n1\n2014-01-02\n182.92\n-0.009584\n\n\n2\n2014-01-03\n182.89\n-0.000164\n\n\n3\n2014-01-06\n182.36\n-0.002898\n\n\n4\n2014-01-07\n183.48\n0.006142\n\n\n\n\n\n\n\nNotice that we can create a new column of a DataFrame by using variable assignment syntax.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-adjusted-close-prices",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-adjusted-close-prices",
    "title": "1  Python Jumpstart",
    "section": "1.15 Visualizing Adjusted Close Prices",
    "text": "1.15 Visualizing Adjusted Close Prices\nPython has a variety of packages that can be used for visualization. In this chapter we will focus on built-in plotting capabilities of pandas. These capabilities are built on top of the matplotlib package, which is the foundation of much of Python’s visualization ecosystem.\nDataFrames have a built-in .plot() method that makes creating simple line graphs quite easy.\n\ndf_spy.plot(x='date', y='close');\n\n\n\n\n\n\n\n\nIf we wanted to make this graph more presentable we could do something like:\n\nax = df_spy.\\\n        plot(\n            x = 'date',\n            y = 'close',\n            title = 'SPY: 2014-2018',\n            grid = True,\n            style = 'k',\n            alpha = 0.75,\n            figsize = (9, 4),\n        );\nax.set_xlabel('Trade Date');\nax.set_ylabel('Close Price');\n\n\n\n\n\n\n\n\nNotice that the ax variable created above is a matplotlib object.\n\ntype(ax)\n\nmatplotlib.axes._axes.Axes",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-returns",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-returns",
    "title": "1  Python Jumpstart",
    "section": "1.16 Visualizing Returns",
    "text": "1.16 Visualizing Returns\npandas also gives us the ability to simultaneously plot two different columns of a DataFrame in separate subplots of a single graph. Here is what that code looks like:\n\ndf_spy.plot(x='date', y=['close', 'ret',], subplots=True, style='k', alpha=0.75, figsize=(9, 8), grid=True);\n\n\n\n\n\n\n\n\nThe returns graph above is a bit of a hack, it doesn’t really make sense to create a line graph of consecutive returns. However, because there are so many days jammed into the x-axis, it creates a desirable effect and it used all the time in finance to demonstrate properties of volatility.\nNotice that whenever there is a sharp drop in the adj_close price graph, that the magnitude of the nearby returns becomes large. In contrast, during periods of steady growth (e.g. all of 2017) the magnitude of the returns is small. This is precisely the leverage effect.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#calculating-realized-volatility",
    "href": "chapters/01_jumpstart/jumpstart.html#calculating-realized-volatility",
    "title": "1  Python Jumpstart",
    "section": "1.17 Calculating Realized Volatility",
    "text": "1.17 Calculating Realized Volatility\nRealized volatility is defined as the standard deviation of the daily returns; it indicates how much variability in the stock price there has been. It is a matter of convention to annualize this quantity, so we multiply it by \\(\\sqrt{252}\\).\nThe following vectorized code calculates a rolling 2-month volatility for our SPY price data.\n\ndf_spy['ret'].rolling(42).std() * np.sqrt(252)\n\n0            NaN\n1            NaN\n2            NaN\n3            NaN\n4            NaN\n          ...   \n1254    0.229796\n1255    0.255840\n1256    0.252218\n1257    0.249128\n1258    0.250166\nName: ret, Length: 1259, dtype: float64\n\n\nLet’s add these realized volatility calculations todf_spy this with the following code.\n\ndf_spy['realized_vol'] = df_spy['ret'].rolling(42).std() * np.sqrt(252)\ndf_spy\n\n\n\n\n\n\n\n\ndate\nclose\nret\nrealized_vol\n\n\n\n\n0\n2013-12-31\n184.69\nNaN\nNaN\n\n\n1\n2014-01-02\n182.92\n-0.009584\nNaN\n\n\n2\n2014-01-03\n182.89\n-0.000164\nNaN\n\n\n3\n2014-01-06\n182.36\n-0.002898\nNaN\n\n\n4\n2014-01-07\n183.48\n0.006142\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n1254\n2018-12-24\n234.34\n-0.026423\n0.229796\n\n\n1255\n2018-12-26\n246.18\n0.050525\n0.255840\n\n\n1256\n2018-12-27\n248.07\n0.007677\n0.252218\n\n\n1257\n2018-12-28\n247.75\n-0.001290\n0.249128\n\n\n1258\n2018-12-31\n249.92\n0.008759\n0.250166\n\n\n\n\n1259 rows × 4 columns",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#visualizing-realized-volatility",
    "href": "chapters/01_jumpstart/jumpstart.html#visualizing-realized-volatility",
    "title": "1  Python Jumpstart",
    "section": "1.18 Visualizing Realized Volatility",
    "text": "1.18 Visualizing Realized Volatility\nWe can easily add realized_vol to our graph with the following code.\n\ndf_spy.plot(x = 'date', \n            y = ['close','ret','realized_vol',], \n            subplots=True, style='k', alpha=0.75, \n            figsize=(9, 12), \n            grid=True);\n\n\n\n\n\n\n\n\nThis graph is an excellent illustration of the leverage effect. When SPY suffers losses, there is a spike in realized volatility, which is to say that the magnitude of the nearby returns increases.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/01_jumpstart/jumpstart.html#further-reading",
    "href": "chapters/01_jumpstart/jumpstart.html#further-reading",
    "title": "1  Python Jumpstart",
    "section": "1.19 Further Reading",
    "text": "1.19 Further Reading\nPython Data Science Handbook - Jake VanderPlas\nPython for Finance 2e - Yves Hilpisch\nPython for Data Analysis 3e - Wes McKinney",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Jumpstart</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html",
    "title": "2  DataFrame Basics",
    "section": "",
    "text": "2.1 Importing Packages\nIn this chapter we cover the basics of working with DataFrames in pandas.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport yfinance as yf\npd.set_option('display.max_rows', 10)",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#reading-in-data",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#reading-in-data",
    "title": "2  DataFrame Basics",
    "section": "2.2 Reading-In Data",
    "text": "2.2 Reading-In Data\nNext, let’s use pandas_datareader to read-in SPY prices from March 2020. SPY is an ETF that tracks the S&P500 index.\n\ndf_spy = yf.download('SPY', start='2020-02-28', end='2020-03-31', auto_adjust=False, rounding=True)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nTicker\nSPY\nSPY\nSPY\nSPY\nSPY\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n\n\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n\n\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n\n\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n\n\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n\n\n\n\n\n\n\nAs a bit of clean-up, let’s do the following:\n\ndrop the SPY level of the column index\nmake the Date a regular column instead of an index\nmake the column names snake-case.\n\n\ndf_spy = df_spy.droplevel(level=1, axis=1)\ndf_spy = df_spy.rename_axis(None, axis=1)\ndf_spy.reset_index(drop=False, inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#exploring-a-dataframe",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#exploring-a-dataframe",
    "title": "2  DataFrame Basics",
    "section": "2.3 Exploring a DataFrame",
    "text": "2.3 Exploring a DataFrame\nWe can explore our df_spy DataFrame in a variety of ways.\nFirst, we can first use the type() method to make sure what we have created is in fact a DataFrame.\n\ntype(df_spy)\n\npandas.core.frame.DataFrame\n\n\nNext, we can use the .dtypes attribute of the DataFrame to see the data types of each of the columns.\n\ndf_spy.dtypes\n\ndate         datetime64[ns]\nadj_close           float64\nclose               float64\nhigh                float64\nlow                 float64\nopen                float64\nvolume                int64\ndtype: object\n\n\nWe can also check the number of rows and columns by using the .shape attribute.\n\ndf_spy.shape\n\n(22, 7)\n\n\nAs we can see, our DataFrame df_spy consists of 22 rows and 7 columns.\n\nCode Challenge: Try the DataFrame.info() and DataFrame.describe() methods on df_spy.\n\n\nSolution\ndf_spy.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 22 entries, 0 to 21\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       22 non-null     datetime64[ns]\n 1   adj_close  22 non-null     float64       \n 2   close      22 non-null     float64       \n 3   high       22 non-null     float64       \n 4   low        22 non-null     float64       \n 5   open       22 non-null     float64       \n 6   volume     22 non-null     int64         \ndtypes: datetime64[ns](1), float64(5), int64(1)\nmemory usage: 1.3 KB\n\n\n\n\nSolution\ndf_spy.describe().round(2)\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\ncount\n22\n22.00\n22.00\n22.00\n22.00\n22.00\n2.200000e+01\n\n\nmean\n2020-03-14 12:00:00\n246.07\n266.54\n272.89\n258.81\n265.03\n2.780051e+08\n\n\nmin\n2020-02-28 00:00:00\n206.68\n222.95\n229.68\n218.26\n228.19\n1.713695e+08\n\n\n25%\n2020-03-06 18:00:00\n226.22\n244.06\n256.22\n237.14\n243.12\n2.362968e+08\n\n\n50%\n2020-03-14 12:00:00\n242.35\n261.42\n264.73\n250.05\n255.85\n2.828830e+08\n\n\n75%\n2020-03-22 06:00:00\n271.23\n294.30\n295.55\n282.53\n287.68\n3.218732e+08\n\n\nmax\n2020-03-30 00:00:00\n288.34\n312.86\n313.84\n303.33\n309.50\n3.922207e+08\n\n\nstd\nNaN\n25.07\n27.55\n25.44\n26.98\n26.43\n6.134551e+07",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#dataframe-columns",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#dataframe-columns",
    "title": "2  DataFrame Basics",
    "section": "2.4 DataFrame Columns",
    "text": "2.4 DataFrame Columns\nIn order to isolate a particular column of a DataFrame we can use square brackets ([ ]). The following code isolates the close price column of df_spy.\n\ndf_spy['close']\n\n0     296.26\n1     309.09\n2     300.24\n3     312.86\n4     302.46\n       ...  \n17    243.15\n18    246.79\n19    261.20\n20    253.42\n21    261.65\nName: close, Length: 22, dtype: float64\n\n\n\nCode Challenge: Isolate the date column of df_spy.\n\n\nSolution\ndf_spy['date']\n\n\n0    2020-02-28\n1    2020-03-02\n2    2020-03-03\n3    2020-03-04\n4    2020-03-05\n        ...    \n17   2020-03-24\n18   2020-03-25\n19   2020-03-26\n20   2020-03-27\n21   2020-03-30\nName: date, Length: 22, dtype: datetime64[ns]\n\n\n\nAs we can see from the following code, each column of a DataFrame is actually a different kind of pandas structure called a Series.\n\ntype(df_spy['close'])\n\npandas.core.series.Series\n\n\nHere is a bit of pandas inside baseball:\n\nA DataFrame is collection of columns that are glued together.\nEach column is a Series.\nA Series has two main attributes: 1) .values; 2) .index.\nThe .values component of a Series is a numpy.array.\n\nLet’s look at the .values attribute of the close column of df_spy.\n\ndf_spy['close'].values\n\narray([296.26, 309.09, 300.24, 312.86, 302.46, 297.46, 274.23, 288.42,\n       274.36, 248.11, 269.32, 239.85, 252.8 , 240.  , 240.51, 228.8 ,\n       222.95, 243.15, 246.79, 261.2 , 253.42, 261.65])\n\n\n\nCode Challenge: Verify that the values component of the close column of df_spy is in fact a a numpy.array.\n\n\nSolution\ntype(df_spy['close'].values)\n\n\nnumpy.ndarray",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#component-wise-column-operations",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#component-wise-column-operations",
    "title": "2  DataFrame Basics",
    "section": "2.5 Component-wise Column Operations",
    "text": "2.5 Component-wise Column Operations\nWe can perform component-wise (i.e. vector-like) calculations with DataFrame columns.\nThe following code divides all the close prices by 100.\n\ndf_spy['close'] / 100\n\n0     2.9626\n1     3.0909\n2     3.0024\n3     3.1286\n4     3.0246\n       ...  \n17    2.4315\n18    2.4679\n19    2.6120\n20    2.5342\n21    2.6165\nName: close, Length: 22, dtype: float64\n\n\nWe can also perform component-wise calculations between two colums.\nLet’s say we want to calculate the intraday range of SPY for each of the trade-dates in df_spy; this is the difference between the high and the low of each day. We can do this easily from the columns of our DataFrame.\n\ndf_spy['high'] - df_spy['low']\n\n0     12.35\n1     14.70\n2     16.27\n3      9.77\n4      8.46\n      ...  \n17    10.30\n18    16.60\n19    13.75\n20     9.76\n21     8.90\nLength: 22, dtype: float64\n\n\n\nCode Challenge: Calculate the difference between the close and open columns of df_spy.\n\n\nSolution\ndf_spy['close'] - df_spy['open']\n\n\n0      7.56\n1     10.88\n2     -9.26\n3      6.74\n4     -2.52\n      ...  \n17     8.73\n18     1.92\n19    11.68\n20     0.15\n21     5.95\nLength: 22, dtype: float64",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-variable-assignment",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-variable-assignment",
    "title": "2  DataFrame Basics",
    "section": "2.6 Adding Columns via Variable Assignment",
    "text": "2.6 Adding Columns via Variable Assignment\nLet’s say we want to save our intraday ranges back into df_spy for further analysis later. The most straightforward way to do this is using variable assignment as follows.\n\ndf_spy['intraday_range'] = df_spy['high'] - df_spy['low']\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\nintraday_range\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n12.35\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n14.70\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n16.27\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n9.77\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n8.46\n\n\n\n\n\n\n\n\nCode Challenge: Add a new column to df_spy called open_to_close that consists of the difference between the close and open of each day.\n\n\nSolution\ndf_spy['open_to_close'] = df_spy['close'] - df_spy['open']\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n8.46\n-2.52",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-.assign",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#adding-columns-via-.assign",
    "title": "2  DataFrame Basics",
    "section": "2.7 Adding Columns via .assign()",
    "text": "2.7 Adding Columns via .assign()\nA powerful but less intuitive way of adding a column to a DataFrame uses the .assign() function, which makes use of lambda functions (i.e. anonymous functions).\nThe following code adds another column called intraday_range_assign.\n\ndf_spy.assign(intraday_range_assign = lambda df: df['high'] - df['low'])\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n12.35\n7.56\n12.35\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n14.70\n10.88\n14.70\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n16.27\n-9.26\n16.27\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n9.77\n6.74\n9.77\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n8.46\n-2.52\n8.46\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17\n2020-03-24\n225.41\n243.15\n244.10\n233.80\n234.42\n235494500\n10.30\n8.73\n10.30\n\n\n18\n2020-03-25\n228.78\n246.79\n256.35\n239.75\n244.87\n299430300\n16.60\n1.92\n16.60\n\n\n19\n2020-03-26\n242.14\n261.20\n262.80\n249.05\n249.52\n257632800\n13.75\n11.68\n13.75\n\n\n20\n2020-03-27\n234.93\n253.42\n260.81\n251.05\n253.27\n224341200\n9.76\n0.15\n9.76\n\n\n21\n2020-03-30\n242.56\n261.65\n262.43\n253.53\n255.70\n171369500\n8.90\n5.95\n8.90\n\n\n\n\n22 rows × 10 columns\n\n\n\n\nCode Challenge: Verify that the column intraday_range_assign was not actually added to the df_spy.\n\n\nSolution\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n8.46\n-2.52\n\n\n\n\n\n\n\n\nIn order to add the intraday_range_assign column to df_spy we will need to reassign to it.\n\ndf_spy = df_spy.assign(intraday_range_assign = lambda df: df['high'] - df['low'])\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n12.35\n7.56\n12.35\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n14.70\n10.88\n14.70\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n16.27\n-9.26\n16.27\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n9.77\n6.74\n9.77\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n8.46\n-2.52\n8.46\n\n\n\n\n\n\n\n\nCode Challenge: Use .assign() to create a new column in df_spy, call it open_to_close_assign, that contains the difference between the close and open.\n\n\nSolution\ndf_spy = df_spy.assign(open_to_close_assign = lambda df: df['close'] - df['open'])\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\nintraday_range\nopen_to_close\nintraday_range_assign\nopen_to_close_assign\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n12.35\n7.56\n12.35\n7.56\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n14.70\n10.88\n14.70\n10.88\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n16.27\n-9.26\n16.27\n-9.26\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n9.77\n6.74\n9.77\n6.74\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n8.46\n-2.52\n8.46\n-2.52",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#method-chaining",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#method-chaining",
    "title": "2  DataFrame Basics",
    "section": "2.8 Method Chaining",
    "text": "2.8 Method Chaining\nThe value of .assign() becomes clear when we start chaining methods together.\nIn order to see this let’s first drop the columns that we created.\n\nlst_cols = ['intraday_range', 'open_to_close', 'intraday_range_assign', 'open_to_close_assign']\ndf_spy.drop(columns=lst_cols, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n\n\n\n\n\n\n\nThe following code adds the intraday and and open_to_close columns at the same time.\n\ndf_spy = \\\n    (\n    df_spy\n        .assign(intraday_range = lambda df: df['high'] - df['low'])\n        .assign(open_to_close = lambda df: df['close'] - df['open'])\n    )\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\nintraday_range\nopen_to_close\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n12.35\n7.56\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n14.70\n10.88\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n16.27\n-9.26\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n9.77\n6.74\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n8.46\n-2.52\n\n\n\n\n\n\n\n\nCode Challenge: Use .assign() to add a two new column to df_spy:\n\nthe difference between the close and adj_close\nthe average of the low and open\n\n\n\nSolution\ndf_spy = \\\n    (\n    df_spy\n        .assign(div = lambda df: df['close'] - df['adj_close'])\n        .assign(avg = lambda df: (df['low'] + df['open']) / 2)\n    )\ndf_spy.head()\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\nintraday_range\nopen_to_close\ndiv\navg\n\n\n\n\n0\n2020-02-28\n273.04\n296.26\n297.89\n285.54\n288.70\n384975800\n12.35\n7.56\n23.22\n287.120\n\n\n1\n2020-03-02\n284.86\n309.09\n309.16\n294.46\n298.21\n238703600\n14.70\n10.88\n24.23\n296.335\n\n\n2\n2020-03-03\n276.71\n300.24\n313.84\n297.57\n309.50\n300139100\n16.27\n-9.26\n23.53\n303.535\n\n\n3\n2020-03-04\n288.34\n312.86\n313.10\n303.33\n306.12\n176613400\n9.77\n6.74\n24.52\n304.725\n\n\n4\n2020-03-05\n278.75\n302.46\n308.47\n300.01\n304.98\n186366800\n8.46\n-2.52\n23.71\n302.495",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#aggregating-calulations-on-series",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#aggregating-calulations-on-series",
    "title": "2  DataFrame Basics",
    "section": "2.9 Aggregating Calulations on Series",
    "text": "2.9 Aggregating Calulations on Series\nSeries have a variety of built-in aggregation functions.\nFor example, we can use the following code to calculate the total SPY volume during March 2020.\n\ndf_spy['volume'].sum()\n\n6116112300\n\n\nHere are some summary statistics on the intraday_range column that we added to our DataFrame earlier.\n\nprint(\"Mean:  \", df_spy['intraday_range'].mean()) # average\nprint(\"St Dev: \", df_spy['intraday_range'].std()) # standard deviation\nprint(\"Min:    \" , df_spy['intraday_range'].min()) # minimum\nprint(\"Max:   \" , df_spy['intraday_range'].max()) # maximum\n\nMean:   14.077727272727275\nSt Dev:  4.28352428533215\nMin:     8.460000000000036\nMax:    22.960000000000008\n\n\n\nCode Challenge: Calculate the average daily volume for the trade dates in df_spy.\n\n\nSolution\ndf_spy['volume'].mean()\n\n\n278005104.54545456",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/02_dataframe_basics/dataframe_basics.html#related-reading",
    "href": "chapters/02_dataframe_basics/dataframe_basics.html#related-reading",
    "title": "2  DataFrame Basics",
    "section": "2.10 Related Reading",
    "text": "2.10 Related Reading\nPython Data Science Handbook - Section 3.1 - Introducing Pandas Objects\nPython Data Science Handbook - Section 2.1 - Understanding Data Types in Python\nPython Data Science Handbook - Section 2.2 - The Basics of NumPy Arrays\nPython Data Science Handbook - Section 2.3 - Computation on NumPy Arrays: Universal Functions\nPython Data Science Handbook - Section 2.4 - Aggregations: Min, Max, and Everything In Between",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>`DataFrame` Basics</span>"
    ]
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html",
    "href": "chapters/03_index_slice/index_slice.html",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "",
    "text": "3.1 Importing Packages\nAccessing a specific row of a DataFrame by its location is referred to as indexing. Accessing a sequence of contiguous rows is referred to as slicing.\nThe purpose of this chapter is to survey various methods for indexing and slicing in pandas.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport yfinance as yf\npd.set_option('display.max_rows', 10)",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>`DataFrame` Indexing and Slicing</span>"
    ]
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#reading-in-data",
    "href": "chapters/03_index_slice/index_slice.html#reading-in-data",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.2 Reading-In Data",
    "text": "3.2 Reading-In Data\nNext, lets grab some data from Yahoo finance. In particular, we’ll grab SPY price data from July 2021.\n\ndf_spy = yf.download('SPY', start='2021-06-30', end='2021-07-31', auto_adjust=False, rounding=True)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nTicker\nSPY\nSPY\nSPY\nSPY\nSPY\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\n\n\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\n\n\n\n\n\n\n\nThe following code:\n\nremoves the SPY level of the column index\nresets the index so that Date is a regular column\nputs the column names into snake-case.\n\n\ndf_spy = df_spy.droplevel(level=1, axis=1)\ndf_spy = df_spy.rename_axis(None, axis=1)\ndf_spy.reset_index(inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n2\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\n\n\n3\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n4\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\n\n\n\n\n\n\n\nIt is often useful to look at the data type of each of the columns of a new data set. We can do so with the DataFrame.dtypes attribute.\n\ndf_spy.dtypes\n\ndate         datetime64[ns]\nadj_close           float64\nclose               float64\nhigh                float64\nlow                 float64\nopen                float64\nvolume                int64\ndtype: object",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>`DataFrame` Indexing and Slicing</span>"
    ]
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#row-slicing",
    "href": "chapters/03_index_slice/index_slice.html#row-slicing",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.3 Row Slicing",
    "text": "3.3 Row Slicing\nThe simplest way to slice a DataFrame is to use square brackets: []. The syntax df[i:j] will generate a DataFrame who’s first row is the ith row of df and who’s last row is the (j-1)th row of df. Let’s demonstrate this with a some examples:\nStarting from the 0th row, and ending with the 0th row:\n\ndf_spy[0:1]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n\n\n\n\n\nStarting with the 3rd row, and ending with the 6th row:\n\ndf_spy[3:7]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n3\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n4\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\n\n\n5\n2021-07-08\n407.21\n430.92\n431.73\n427.52\n428.78\n97595200\n\n\n6\n2021-07-09\n411.56\n435.52\n435.84\n430.71\n432.53\n76238600\n\n\n\n\n\n\n\n\nCode Challenge: Retrieve the 15th, 16th, and 17th rows of df_spy.\n\n\nSolution\ndf_spy[15:18]\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n15\n2021-07-22\n411.50\n435.46\n435.72\n433.69\n434.74\n47878500\n\n\n16\n2021-07-23\n415.74\n439.94\n440.30\n436.79\n437.52\n63766600\n\n\n17\n2021-07-26\n416.76\n441.02\n441.03\n439.26\n439.31\n43719200\n\n\n\n\n\n\n\n\nUsing the syntax df[:n] automatically starts the indexing at 0. For example, the following code retrieves all of df_spy (notice that len(df_spy) gives the number of rows of df_spy):\n\ndf_spy[:len(df_spy)]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n2\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\n\n\n3\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n4\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17\n2021-07-26\n416.76\n441.02\n441.03\n439.26\n439.31\n43719200\n\n\n18\n2021-07-27\n414.86\n439.01\n439.94\n435.99\n439.91\n67397100\n\n\n19\n2021-07-28\n414.69\n438.83\n440.30\n437.31\n439.68\n52472400\n\n\n20\n2021-07-29\n416.41\n440.65\n441.80\n439.81\n439.82\n47435300\n\n\n21\n2021-07-30\n414.39\n438.51\n440.06\n437.77\n437.91\n68951200\n\n\n\n\n22 rows × 7 columns\n\n\n\n\nCode Challenge: Retrieve the first five rows of df_spy.\n\n\nSolution\ndf_spy[:5]\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n2\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\n\n\n3\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n4\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\n\n\n\n\n\n\n\n\nThere are a couple of row slicing tricks that involve negative numbers that are worth mentioning.\nThe syntax df[-n:] retrieves the last n rows of df. The following code retrieves the last five rows of df_spy.\n\ndf_spy[-5:]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n17\n2021-07-26\n416.76\n441.02\n441.03\n439.26\n439.31\n43719200\n\n\n18\n2021-07-27\n414.86\n439.01\n439.94\n435.99\n439.91\n67397100\n\n\n19\n2021-07-28\n414.69\n438.83\n440.30\n437.31\n439.68\n52472400\n\n\n20\n2021-07-29\n416.41\n440.65\n441.80\n439.81\n439.82\n47435300\n\n\n21\n2021-07-30\n414.39\n438.51\n440.06\n437.77\n437.91\n68951200\n\n\n\n\n\n\n\nThe syntax df[:-n] retrieves all but the last n rows of df. The following code retrieves all but the last 10 rows of df_spy:\n\ndf_spy[:-10]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n2\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\n\n\n3\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n4\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7\n2021-07-12\n413.03\n437.08\n437.35\n434.97\n435.43\n52889600\n\n\n8\n2021-07-13\n411.63\n435.59\n437.84\n435.31\n436.24\n52911300\n\n\n9\n2021-07-14\n412.24\n436.24\n437.92\n434.91\n437.40\n64130400\n\n\n10\n2021-07-15\n410.83\n434.75\n435.53\n432.72\n434.81\n55126400\n\n\n11\n2021-07-16\n407.61\n431.34\n436.06\n430.92\n436.01\n75874700\n\n\n\n\n12 rows × 7 columns\n\n\n\n\nCode Challenge: Retrieve the first row of df_spy with negative indexing.\n\n\nSolution\ndf_spy[:-(len(df_spy)-1)]\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n\n\n\n\n\n\nCode Challenge: Use simple slicing to select the last three rows of a df_spy: 1) without explicitly using row numbers; 2) with explicitly using row numbers.\n\n\nSolution\ndf_spy[len(df_spy)-3:len(df_spy)]\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n19\n2021-07-28\n414.69\n438.83\n440.30\n437.31\n439.68\n52472400\n\n\n20\n2021-07-29\n416.41\n440.65\n441.80\n439.81\n439.82\n47435300\n\n\n21\n2021-07-30\n414.39\n438.51\n440.06\n437.77\n437.91\n68951200\n\n\n\n\n\n\n\n\n\nSolution\ndf_spy[-3:]\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n19\n2021-07-28\n414.69\n438.83\n440.30\n437.31\n439.68\n52472400\n\n\n20\n2021-07-29\n416.41\n440.65\n441.80\n439.81\n439.82\n47435300\n\n\n21\n2021-07-30\n414.39\n438.51\n440.06\n437.77\n437.91\n68951200",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>`DataFrame` Indexing and Slicing</span>"
    ]
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#dataframe-indexes",
    "href": "chapters/03_index_slice/index_slice.html#dataframe-indexes",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.4 DataFrame Indexes",
    "text": "3.4 DataFrame Indexes\nUnder the hood, a DataFrame has several indexes:\ncolumns - the set of column names is an (explicit) index.\nrow - whenever a DataFrame is created, there is an explicit row index that is created. If one isn’t specified, then a sequence of non-negative integers is used.\nimplicit - each row has an implicit row-number, and each column has an implicit column-number.\nLet’s take a look at the columns index of df_spy.\n\ndf_spy.columns\n\nIndex(['date', 'adj_close', 'close', 'high', 'low', 'open', 'volume'], dtype='object')\n\n\n\ntype(df_spy.columns)\n\npandas.core.indexes.base.Index\n\n\nNext, let’s take a look at the explicit row index attribute of df_spy.\n\ndf_spy.index\n\nRangeIndex(start=0, stop=22, step=1)\n\n\n\ntype(df_spy.index)\n\npandas.core.indexes.range.RangeIndex\n\n\nSince we reset the index for df_spy, a RangeIndex object is used for the explicit row index. You can think of a RangeIndex object as a glorified set of consecutive integers.\nFor the most part, we won’t be too concerned with indexes. A lot of data analysis can be done without worrying about them. However, it’s good to be aware indexes exist becase they can come into play for more advanced topics, such as joining tables together; they also come up in Stack Overflow examples frequently.\nFor the purposes of this chapter, our interest in indexes comes from how they are related to two built-in DataFrame indexers: DataFrame.iloc and DataFrame.loc.",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>`DataFrame` Indexing and Slicing</span>"
    ]
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#indexing-with-dataframe.iloc",
    "href": "chapters/03_index_slice/index_slice.html#indexing-with-dataframe.iloc",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.5 Indexing with DataFrame.iloc",
    "text": "3.5 Indexing with DataFrame.iloc\nThe indexer DataFrame.iloc can be used to access rows and columns using their implicit row and column numbers.\nHere is an example of iloc that retrieves the first two rows of df_spy.\n\ndf_spy.iloc[0:2,]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n\n\n\n\n\nNotice, that because we didn’t specify any column numbers, the code above retrieves all columns.\nThe following code grabs the first three row and the first three columns of df_spy.\n\ndf_spy.iloc[0:3, 0:3]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n\n\n1\n2021-07-01\n406.75\n430.43\n\n\n2\n2021-07-02\n409.86\n433.72\n\n\n\n\n\n\n\nWe can also supply .iloc with lists rather than ranges to specify custom sets of columns and rows:\n\nlst_row = [0, 2] # 0th and 2nd row\nlst_col = [0, 6] # date and adj_close columns\ndf_spy.iloc[lst_row, lst_col]\n\n\n\n\n\n\n\n\ndate\nvolume\n\n\n\n\n0\n2021-06-30\n64827900\n\n\n2\n2021-07-02\n57697700\n\n\n\n\n\n\n\nUsing lists as a means of indexing is sometimes referred to as fancy indexing.\n\nCode Challenge Use fancy indexing to grab the 14th, 0th, and 5th rows of df_spy - in that order.\n\n\nSolution\ndf_spy.iloc[[14, 0, 5]]\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n14\n2021-07-21\n410.64\n434.55\n434.70\n431.01\n432.34\n64724400\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n5\n2021-07-08\n407.21\n430.92\n431.73\n427.52\n428.78\n97595200",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>`DataFrame` Indexing and Slicing</span>"
    ]
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#indexing-with-dataframe.loc",
    "href": "chapters/03_index_slice/index_slice.html#indexing-with-dataframe.loc",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.6 Indexing with DataFrame.loc",
    "text": "3.6 Indexing with DataFrame.loc\nRather than using the implicit row or column numbers, it is often more useful to access data by using the explicit row or column indices.\nLet’s use the DataFrame.set_index() method to set the date column as our new index. The dates will be a more interesting explicit index.\n\ndf_spy.set_index('date', inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\n\n\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\n\n\n\n\n\n\n\nTo see the effect of the above code, we can have a look at the index of df_spy.\n\ndf_spy.index\n\nDatetimeIndex(['2021-06-30', '2021-07-01', '2021-07-02', '2021-07-06',\n               '2021-07-07', '2021-07-08', '2021-07-09', '2021-07-12',\n               '2021-07-13', '2021-07-14', '2021-07-15', '2021-07-16',\n               '2021-07-19', '2021-07-20', '2021-07-21', '2021-07-22',\n               '2021-07-23', '2021-07-26', '2021-07-27', '2021-07-28',\n               '2021-07-29', '2021-07-30'],\n              dtype='datetime64[ns]', name='date', freq=None)\n\n\nAnd notice that date is no longer column of df_spy.\n\ndf_spy.columns\n\nIndex(['adj_close', 'close', 'high', 'low', 'open', 'volume'], dtype='object')\n\n\nNow that we have successfully set the row index of df_spy to be the date, let’s see how we can use this index to access the data via .loc.\nHere is an example of how we can grab a slice of rows, associated with a date-range.\n\ndf_spy.loc['2021-07-23':'2021-07-31']\n\n\n\n\n\n\n\n\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2021-07-23\n415.74\n439.94\n440.30\n436.79\n437.52\n63766600\n\n\n2021-07-26\n416.76\n441.02\n441.03\n439.26\n439.31\n43719200\n\n\n2021-07-27\n414.86\n439.01\n439.94\n435.99\n439.91\n67397100\n\n\n2021-07-28\n414.69\n438.83\n440.30\n437.31\n439.68\n52472400\n\n\n2021-07-29\n416.41\n440.65\n441.80\n439.81\n439.82\n47435300\n\n\n2021-07-30\n414.39\n438.51\n440.06\n437.77\n437.91\n68951200\n\n\n\n\n\n\n\nIf we want to select only the volume and adjusted columns for these dates, we would type the following:\n\ndf_spy.loc['2021-07-23':'2021-07-31', ['volume', 'adj_close']]\n\n\n\n\n\n\n\n\nvolume\nadj_close\n\n\ndate\n\n\n\n\n\n\n2021-07-23\n63766600\n415.74\n\n\n2021-07-26\n43719200\n416.76\n\n\n2021-07-27\n67397100\n414.86\n\n\n2021-07-28\n52472400\n414.69\n\n\n2021-07-29\n47435300\n416.41\n\n\n2021-07-30\n68951200\n414.39\n\n\n\n\n\n\n\n\nCode Challenge: Use .loc to grab the date, volume, and close columns from df_spy.\n\n\nSolution\ndf_spy.loc[:,['volume', 'close']]\n\n\n\n\n\n\n\n\n\nvolume\nclose\n\n\ndate\n\n\n\n\n\n\n2021-06-30\n64827900\n428.06\n\n\n2021-07-01\n53441000\n430.43\n\n\n2021-07-02\n57697700\n433.72\n\n\n2021-07-06\n68710400\n432.93\n\n\n2021-07-07\n63549500\n434.46\n\n\n...\n...\n...\n\n\n2021-07-26\n43719200\n441.02\n\n\n2021-07-27\n67397100\n439.01\n\n\n2021-07-28\n52472400\n438.83\n\n\n2021-07-29\n47435300\n440.65\n\n\n2021-07-30\n68951200\n438.51\n\n\n\n\n22 rows × 2 columns",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>`DataFrame` Indexing and Slicing</span>"
    ]
  },
  {
    "objectID": "chapters/03_index_slice/index_slice.html#related-reading",
    "href": "chapters/03_index_slice/index_slice.html#related-reading",
    "title": "3  DataFrame Indexing and Slicing",
    "section": "3.7 Related Reading",
    "text": "3.7 Related Reading\nPython Data Science Handbook - Section 2.7 - Fancy Indexing\nPython Data Science Handbook - Section 3.2 - Data Indexing and Selection",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>`DataFrame` Indexing and Slicing</span>"
    ]
  },
  {
    "objectID": "chapters/04_query/query.html",
    "href": "chapters/04_query/query.html",
    "title": "4  DataFrame Querying",
    "section": "",
    "text": "4.1 Importing Packages\nIn the previous chapter we discussed accessing rows of a DataFrame by row position. In practice, I don’t find this particularly useful. Rather, I find that I usually access rows by some kind of logical condition, which is referred to as querying the DataFrame.\nIn this chapter we discuss two ways of querying a DataFrame:\nLet’s first import the packages that we will need.\nimport pandas as pd\nimport yfinance as yf\npd.set_option('display.max_rows', 10)",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>`DataFrame` Querying</span>"
    ]
  },
  {
    "objectID": "chapters/04_query/query.html#reading-in-data",
    "href": "chapters/04_query/query.html#reading-in-data",
    "title": "4  DataFrame Querying",
    "section": "4.2 Reading-In Data",
    "text": "4.2 Reading-In Data\nNext, let’s use pandas_datareader to read-in some SPY data from July 2021.\n\ndf_spy = yf.download('SPY', start='2021-06-30', end='2021-07-31', auto_adjust=False, rounding=True)\ndf_spy.head()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nTicker\nSPY\nSPY\nSPY\nSPY\nSPY\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\n\n\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\n\n\n\n\n\n\n\nThe following code:\n\nremoves the SPY level of the column index\nresets the index so that Date is a regular column\nputs the column names into snake-case.\n\n\ndf_spy = df_spy.droplevel(level=1, axis=1)\ndf_spy = df_spy.rename_axis(None, axis=1)\ndf_spy.reset_index(inplace=True)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\n\n\n2\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\n\n\n3\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\n\n\n4\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>`DataFrame` Querying</span>"
    ]
  },
  {
    "objectID": "chapters/04_query/query.html#comparison-and-dataframe-columns",
    "href": "chapters/04_query/query.html#comparison-and-dataframe-columns",
    "title": "4  DataFrame Querying",
    "section": "4.3 Comparison and DataFrame Columns",
    "text": "4.3 Comparison and DataFrame Columns\nAs discussed in a previous chapter, a column of a DataFrame is a Series object, which is a souped up numpy.array (think vector or matrix).\nLet’s separate out the close column of df_spy and assign it to a variable.\n\npd.options.display.max_rows = 6 # this modifies the printing of dataframes\nser_close = df_spy['close']\nser_close\n\n0     428.06\n1     430.43\n2     433.72\n       ...  \n19    438.83\n20    440.65\n21    438.51\nName: close, Length: 22, dtype: float64\n\n\nRecall that a pandas.Series is smart with respect to component-wise arithmetic operations, meaning it behaves like a vector from linear algebra. This means that arithmetic operations are broadcasted as you might expect.\nFor example, division by 100 is broadcasted component-wise.\n\nser_close / 100\n\n0     4.2806\n1     4.3043\n2     4.3372\n       ...  \n19    4.3883\n20    4.4065\n21    4.3851\nName: close, Length: 22, dtype: float64\n\n\n\n4.3.0.1 It is a convenient fact that this broadcasting behavior also occurs with comparison, and produces a Series of booleans.\nThe following code checks which elements of ser_adjusted are greater than 435.\n\nser_test = (ser_close &gt; 435)\nser_test\n\n0     False\n1     False\n2     False\n      ...  \n19     True\n20     True\n21     True\nName: close, Length: 22, dtype: bool\n\n\nLet’s check that the resulting variable ser_test is a pandas.Series.\n\ntype(ser_test)\n\npandas.core.series.Series\n\n\nAnd finally let’s observe the .values elements of ser_test.\n\nprint(ser_test.values)\n\n[False False False False False False  True  True  True  True False False\n False False False  True  True  True  True  True  True  True]\n\n\nA few observation about what just happened:\n\nWhen we compare a Series of numerical values (ser_close) to a single number (435), we get back a Series of booleans (ser_test).\nWe have that ser_test[i] = (ser_adjusted[i] &gt; 435).\nSo the comparison operation was broadcasted as advertised.\n\nThis is easy to see by appending ser_test to df_spy and then reprinting.\n\npd.options.display.max_rows = 25\ndf_spy['test'] = ser_test\ndf_spy\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\ntest\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\nFalse\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\nFalse\n\n\n2\n2021-07-02\n409.86\n433.72\n434.10\n430.52\n431.67\n57697700\nFalse\n\n\n3\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\nFalse\n\n\n4\n2021-07-07\n410.56\n434.46\n434.76\n431.51\n433.66\n63549500\nFalse\n\n\n5\n2021-07-08\n407.21\n430.92\n431.73\n427.52\n428.78\n97595200\nFalse\n\n\n6\n2021-07-09\n411.56\n435.52\n435.84\n430.71\n432.53\n76238600\nTrue\n\n\n7\n2021-07-12\n413.03\n437.08\n437.35\n434.97\n435.43\n52889600\nTrue\n\n\n8\n2021-07-13\n411.63\n435.59\n437.84\n435.31\n436.24\n52911300\nTrue\n\n\n9\n2021-07-14\n412.24\n436.24\n437.92\n434.91\n437.40\n64130400\nTrue\n\n\n10\n2021-07-15\n410.83\n434.75\n435.53\n432.72\n434.81\n55126400\nFalse\n\n\n11\n2021-07-16\n407.61\n431.34\n436.06\n430.92\n436.01\n75874700\nFalse\n\n\n12\n2021-07-19\n401.59\n424.97\n431.41\n421.97\n426.19\n147987000\nFalse\n\n\n13\n2021-07-20\n407.35\n431.06\n432.42\n424.83\n425.68\n99608200\nFalse\n\n\n14\n2021-07-21\n410.64\n434.55\n434.70\n431.01\n432.34\n64724400\nFalse\n\n\n15\n2021-07-22\n411.50\n435.46\n435.72\n433.69\n434.74\n47878500\nTrue\n\n\n16\n2021-07-23\n415.74\n439.94\n440.30\n436.79\n437.52\n63766600\nTrue\n\n\n17\n2021-07-26\n416.76\n441.02\n441.03\n439.26\n439.31\n43719200\nTrue\n\n\n18\n2021-07-27\n414.86\n439.01\n439.94\n435.99\n439.91\n67397100\nTrue\n\n\n19\n2021-07-28\n414.69\n438.83\n440.30\n437.31\n439.68\n52472400\nTrue\n\n\n20\n2021-07-29\n416.41\n440.65\n441.80\n439.81\n439.82\n47435300\nTrue\n\n\n21\n2021-07-30\n414.39\n438.51\n440.06\n437.77\n437.91\n68951200\nTrue\n\n\n\n\n\n\n\nAs we will see in the next two sections, the broadcasting of comparison can be used to query subsets of rows of a DataFrame.",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>`DataFrame` Querying</span>"
    ]
  },
  {
    "objectID": "chapters/04_query/query.html#dataframe-masking",
    "href": "chapters/04_query/query.html#dataframe-masking",
    "title": "4  DataFrame Querying",
    "section": "4.4 DataFrame Masking",
    "text": "4.4 DataFrame Masking\nFrom the code below we know that df_spy has 22 rows.\n\ndf_spy.shape\n\n(22, 8)\n\n\nThe following code creates a list consisting of 22 booleans, all of them False.\n\nlst_bool = [False] * 22\nlst_bool\n\n[False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False]\n\n\nNow, let’s see what happens when we feed this list of False booleans into df_spy using square brackets.\n\ndf_spy[lst_bool]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\ntest\n\n\n\n\n\n\n\n\n\n\nCode Challenge: Verify that df_spy[lst_bool] is an empty DataFrame.\n\n\nSolution\ntype(df_spy[lst_bool])\n\n\npandas.core.frame.DataFrame\n\n\n\n\nSolution\ndf_spy[lst_bool].shape\n\n\n(0, 8)\n\n\n\nNext, let’s modify lst_bool slightly by changing the 0th entry to True. Then lets feed lst_bool into df_spy again.\n\nlst_bool[0] = True\ndf_spy[lst_bool]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\ntest\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\nFalse\n\n\n\n\n\n\n\nSo what happened? Notice that df_spy[lst_bool] returns a DataFrame consisting only of the 0th row of df_spy.\nLet’s modify lst_bool once again, by setting the 1st entry of df_spy to True, and then once again feed it into df_spy.\n\nlst_bool[1] = True\ndf_spy[lst_bool]\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\ntest\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\nFalse\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\nFalse\n\n\n\n\n\n\n\nPunchline: What is returned by the code df_spy[lst_bool] will be a DataFrame consisting of all the rows corresponding to the True entries of lst_bool.\nThis is called DataFrame masking.\n\nCode Challenge: Modify lst_bool and then use DataFrame masking to grab the 0th, 1st and, 3rd rows of df_spy.\n\n\nSolution\nlst_bool[3] = True\ndf_spy[lst_bool]\n\n\n\n\n\n\n\n\n\ndate\nadj_close\nclose\nhigh\nlow\nopen\nvolume\ntest\n\n\n\n\n0\n2021-06-30\n404.51\n428.06\n428.78\n427.18\n427.21\n64827900\nFalse\n\n\n1\n2021-07-01\n406.75\n430.43\n430.60\n428.80\n428.87\n53441000\nFalse\n\n\n3\n2021-07-06\n409.11\n432.93\n434.01\n430.01\n433.78\n68710400\nFalse",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>`DataFrame` Querying</span>"
    ]
  },
  {
    "objectID": "chapters/04_query/query.html#querying-with-dataframe-masking",
    "href": "chapters/04_query/query.html#querying-with-dataframe-masking",
    "title": "4  DataFrame Querying",
    "section": "4.5 Querying with DataFrame Masking",
    "text": "4.5 Querying with DataFrame Masking\nWe often want to query a DataFrame based on some kind of comparison involving its column values.\nWe can achieve this kind of querying by combining the broadcasting of comparison over DataFrame columns with DataFrame masking.\nIn order to consider concrete examples, let’s read-in some data.\nThe following code reads in a data set consisting of end-of-day prices for four different ETFs (SPY, IWM, QQQ, DIA), during the month of July 2021.\n\npd.options.display.max_rows = 25\ndf_etf = yf.download(\n    ['SPY', 'QQQ', 'IWM', 'DIA'], start='2021-06-30', end='2021-07-31',\n    auto_adjust=False, rounding=True\n)\ndf_etf.head()\n\n[*********************100%***********************]  4 of 4 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\n...\nLow\nOpen\nVolume\n\n\nTicker\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\n...\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n320.48\n217.77\n345.63\n404.51\n344.95\n229.37\n354.43\n428.06\n345.51\n230.32\n...\n353.83\n427.18\n342.38\n228.65\n354.83\n427.21\n3778900\n26039000\n32724000\n64827900\n\n\n2021-07-01\n321.79\n219.69\n345.76\n406.75\n346.36\n231.39\n354.57\n430.43\n346.40\n231.85\n...\n352.68\n428.80\n345.78\n230.81\n354.07\n428.87\n3606900\n18089100\n29290000\n53441000\n\n\n2021-07-02\n323.26\n217.60\n349.73\n409.86\n347.94\n229.19\n358.64\n433.72\n348.29\n232.08\n...\n356.28\n430.52\n347.04\n232.00\n356.52\n431.67\n3013500\n21029700\n32727200\n57697700\n\n\n2021-07-06\n321.29\n214.44\n351.24\n409.11\n345.82\n225.86\n360.19\n432.93\n348.11\n229.46\n...\n356.49\n430.01\n347.75\n229.36\n359.26\n433.78\n3910600\n27771300\n38842400\n68710400\n\n\n2021-07-07\n322.31\n212.44\n351.99\n410.56\n346.92\n223.76\n360.95\n434.46\n347.14\n226.67\n...\n358.94\n431.51\n345.65\n225.54\n362.45\n433.66\n3347000\n28521500\n35265200\n63549500\n\n\n\n\n5 rows × 24 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = (\n    df_etf[[('Close','DIA'), ('Close', 'IWM'), ('Close', 'QQQ'), ('Close', 'SPY')]] # grab close prices\n    .stack() # pivot the table\n    .reset_index()\n    .rename_axis(None, axis=1) # remove the name from the row index\n    .rename(columns={'Date':'date', 'Ticker':'symbol', 'Close':'close'}) # columns in snake-case\n    .sort_values(by=['symbol', 'date'])\n)\ndf_etf\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n0\n2021-06-30\nDIA\n344.95\n\n\n4\n2021-07-01\nDIA\n346.36\n\n\n8\n2021-07-02\nDIA\n347.94\n\n\n12\n2021-07-06\nDIA\n345.82\n\n\n16\n2021-07-07\nDIA\n346.92\n\n\n...\n...\n...\n...\n\n\n71\n2021-07-26\nSPY\n441.02\n\n\n75\n2021-07-27\nSPY\n439.01\n\n\n79\n2021-07-28\nSPY\n438.83\n\n\n83\n2021-07-29\nSPY\n440.65\n\n\n87\n2021-07-30\nSPY\n438.51\n\n\n\n\n88 rows × 3 columns\n\n\n\n\n4.5.1 Querying for One Symbol\nWe are now ready to apply DataFrame masking to our ETF data set.\nAs a first example, let’s isolate all the rows of df_etf that correspond to IWM.\n\npd.options.display.max_rows = 6\nser_bool = (df_etf['symbol'] == \"IWM\")\ndf_etf[ser_bool]\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n1\n2021-06-30\nIWM\n229.37\n\n\n5\n2021-07-01\nIWM\n231.39\n\n\n9\n2021-07-02\nIWM\n229.19\n\n\n...\n...\n...\n...\n\n\n77\n2021-07-28\nIWM\n220.82\n\n\n81\n2021-07-29\nIWM\n222.52\n\n\n85\n2021-07-30\nIWM\n221.05\n\n\n\n\n22 rows × 3 columns\n\n\n\nNotice that we did this in two steps:\n\nCalculate the series of booleans called ser_bool using comparison broadcasting.\nPerform the masking by using square brackets [] and ser_bool.\n\nWe can actually perform this masking in a single line of code, without creating the intermediate variable ser_bool.\n\ndf_etf[df_etf['symbol'] == \"IWM\"]\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n1\n2021-06-30\nIWM\n229.37\n\n\n5\n2021-07-01\nIWM\n231.39\n\n\n9\n2021-07-02\nIWM\n229.19\n\n\n...\n...\n...\n...\n\n\n77\n2021-07-28\nIWM\n220.82\n\n\n81\n2021-07-29\nIWM\n222.52\n\n\n85\n2021-07-30\nIWM\n221.05\n\n\n\n\n22 rows × 3 columns\n\n\n\n\nCode Challenge: Select all the rows of df_etf for QQQ.\n\n\nSolution\ndf_etf[df_etf['symbol'] == 'QQQ']\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n2\n2021-06-30\nQQQ\n354.43\n\n\n6\n2021-07-01\nQQQ\n354.57\n\n\n10\n2021-07-02\nQQQ\n358.64\n\n\n...\n...\n...\n...\n\n\n78\n2021-07-28\nQQQ\n365.83\n\n\n82\n2021-07-29\nQQQ\n366.48\n\n\n86\n2021-07-30\nQQQ\n364.57\n\n\n\n\n22 rows × 3 columns\n\n\n\n\n\n\n4.5.2 Querying for Multiple Symbols\nWe can use the .isin() method to query a DataFrame for multiple symbols. The technique is to feed .isin() a list of symbols you want to query for.\nThe following code grabs all the rows of df_etf for both QQQ and DIA.\n\ndf_etf[df_etf['symbol'].isin(['QQQ', 'DIA'])]\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n0\n2021-06-30\nDIA\n344.95\n\n\n4\n2021-07-01\nDIA\n346.36\n\n\n8\n2021-07-02\nDIA\n347.94\n\n\n...\n...\n...\n...\n\n\n78\n2021-07-28\nQQQ\n365.83\n\n\n82\n2021-07-29\nQQQ\n366.48\n\n\n86\n2021-07-30\nQQQ\n364.57\n\n\n\n\n44 rows × 3 columns\n\n\n\n\nCode Challenge: Grab all rows of df_etf corresponding to SPY, IWM, and QQQ.\n\n\nSolution\ndf_etf[df_etf['symbol'].isin(['SPY', 'IWM', 'QQQ'])]\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n1\n2021-06-30\nIWM\n229.37\n\n\n5\n2021-07-01\nIWM\n231.39\n\n\n9\n2021-07-02\nIWM\n229.19\n\n\n...\n...\n...\n...\n\n\n79\n2021-07-28\nSPY\n438.83\n\n\n83\n2021-07-29\nSPY\n440.65\n\n\n87\n2021-07-30\nSPY\n438.51\n\n\n\n\n66 rows × 3 columns\n\n\n\n\n\n\n4.5.3 Querying for Dates\nThe following code grabs all the rows of df_etf that come after the middle of the month.\n\ndf_etf[df_etf['date'] &gt; '2021-07-15']\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n44\n2021-07-16\nDIA\n346.74\n\n\n48\n2021-07-19\nDIA\n339.88\n\n\n52\n2021-07-20\nDIA\n345.08\n\n\n...\n...\n...\n...\n\n\n79\n2021-07-28\nSPY\n438.83\n\n\n83\n2021-07-29\nSPY\n440.65\n\n\n87\n2021-07-30\nSPY\n438.51\n\n\n\n\n44 rows × 3 columns\n\n\n\n\nCode Challenge: Grab all the rows of df_etf for the last trade date of the month.\n\n\nSolution\ndf_etf[df_etf['date'] == '2021-07-30']\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n84\n2021-07-30\nDIA\n349.48\n\n\n85\n2021-07-30\nIWM\n221.05\n\n\n86\n2021-07-30\nQQQ\n364.57\n\n\n87\n2021-07-30\nSPY\n438.51\n\n\n\n\n\n\n\n\n\n\n4.5.4 Querying on Multiple Criteria\nWe can filter on muliple criteria by using the & operator, which is the vectorized version of and.\nSuppose that we want all rows for SPY that come before July fourth.\n\nbln_ticker = (df_etf['symbol'] == 'SPY')\nbln_date = (df_etf['date'] &lt; '2021-07-04')\nbln_combined = bln_ticker & bln_date\n\ndf_etf[bln_combined]\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n3\n2021-06-30\nSPY\n428.06\n\n\n7\n2021-07-01\nSPY\n430.43\n\n\n11\n2021-07-02\nSPY\n433.72\n\n\n\n\n\n\n\n\nCode Challenge: Isolate the rows for QQQ and IWM on the last trading day before July 4th - try to not use intermediate variables.\n\ndf_etf[(df_etf['symbol'].isin([\"QQQ\", \"IWM\"])) & (df_etf['date']=='2021-07-02')]\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n9\n2021-07-02\nIWM\n229.19\n\n\n10\n2021-07-02\nQQQ\n358.64",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>`DataFrame` Querying</span>"
    ]
  },
  {
    "objectID": "chapters/04_query/query.html#querying-with-.query",
    "href": "chapters/04_query/query.html#querying-with-.query",
    "title": "4  DataFrame Querying",
    "section": "4.6 Querying with .query()",
    "text": "4.6 Querying with .query()\nI find querying a DataFrame via masking to be rather cumbersome.\nI greatly prefer the use of the DataFrame.query() method which uses SQL-like strings to define queries.\nFor example, the following code grabs all the rows corresponding to IWM.\n\ndf_etf.query('symbol == \"IWM\"')\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n1\n2021-06-30\nIWM\n229.37\n\n\n5\n2021-07-01\nIWM\n231.39\n\n\n9\n2021-07-02\nIWM\n229.19\n\n\n...\n...\n...\n...\n\n\n77\n2021-07-28\nIWM\n220.82\n\n\n81\n2021-07-29\nIWM\n222.52\n\n\n85\n2021-07-30\nIWM\n221.05\n\n\n\n\n22 rows × 3 columns\n\n\n\nThis code queries all rows corresponding to QQQ and DIA.\n\ndf_etf.query('symbol in (\"QQQ\", \"DIA\")')\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n0\n2021-06-30\nDIA\n344.95\n\n\n4\n2021-07-01\nDIA\n346.36\n\n\n8\n2021-07-02\nDIA\n347.94\n\n\n...\n...\n...\n...\n\n\n78\n2021-07-28\nQQQ\n365.83\n\n\n82\n2021-07-29\nQQQ\n366.48\n\n\n86\n2021-07-30\nQQQ\n364.57\n\n\n\n\n44 rows × 3 columns\n\n\n\nHere we grab the rows corresponding to the first half of July.\n\ndf_etf.query('date &lt; \"2021-07-15\"')\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n0\n2021-06-30\nDIA\n344.95\n\n\n4\n2021-07-01\nDIA\n346.36\n\n\n8\n2021-07-02\nDIA\n347.94\n\n\n...\n...\n...\n...\n\n\n31\n2021-07-12\nSPY\n437.08\n\n\n35\n2021-07-13\nSPY\n435.59\n\n\n39\n2021-07-14\nSPY\n436.24\n\n\n\n\n40 rows × 3 columns\n\n\n\nAnd we can filter on multiple criteria via method chaining. Here we grab all the rows for SPY and IWM from the second half of the month.\n\n(\ndf_etf\n    .query('symbol in (\"SPY\", \"IWM\")')\n    .query('date &gt; \"2021-07-15\"')\n)\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n45\n2021-07-16\nIWM\n214.95\n\n\n49\n2021-07-19\nIWM\n211.73\n\n\n53\n2021-07-20\nIWM\n218.30\n\n\n...\n...\n...\n...\n\n\n79\n2021-07-28\nSPY\n438.83\n\n\n83\n2021-07-29\nSPY\n440.65\n\n\n87\n2021-07-30\nSPY\n438.51\n\n\n\n\n22 rows × 3 columns\n\n\n\n\nCode Challenge: Grab all the rows of df_etf that correspond to the following criteria: 1. SPY 2. first half of month 3. close less than 435\n\n(\ndf_etf\n    .query('symbol == \"SPY\"')\n    .query('date &lt; \"2021-07-15\"')\n    .query('close &lt; 435')\n)\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n3\n2021-06-30\nSPY\n428.06\n\n\n7\n2021-07-01\nSPY\n430.43\n\n\n11\n2021-07-02\nSPY\n433.72\n\n\n15\n2021-07-06\nSPY\n432.93\n\n\n19\n2021-07-07\nSPY\n434.46\n\n\n23\n2021-07-08\nSPY\n430.92",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>`DataFrame` Querying</span>"
    ]
  },
  {
    "objectID": "chapters/04_query/query.html#related-reading",
    "href": "chapters/04_query/query.html#related-reading",
    "title": "4  DataFrame Querying",
    "section": "4.7 Related Reading",
    "text": "4.7 Related Reading\nPython Data Science Handbook - Section 2.6 - Comparisons, Masks, and Boolean Logic\nPython Data Science Handbook - Section 2.7 - Fancy Indexing\nPython Data Science Handbook - Section 3.2 - Data Indexing and Selection\nPython Data Science Handbook - Section 3.12 - High Performance Pandas",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>`DataFrame` Querying</span>"
    ]
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html",
    "href": "chapters/05_functions_apply/functions_apply.html",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "",
    "text": "5.1 Defining Functions\nWe have already discussed how to add a new column to a DataFrame that is a simple function of existing columns.\nSuppose the situation is a little more complicated, and that the column we want to add is some kind of custom (user defined) function of existing columns.\nIn this tutorial we discuss two ways of doing this:\nWe will use a finance task to motivate these two techniques: calculating the payoffs of expiring options.\nDefining functions in Python is straightforward.\nThey syntax is simply def function_name(arguments):.\nThe following function squares two numbers.\ndef square(x):\n    sq = x ** 2\n    return(sq)\nLet’s verify that our function works.\nprint(square(2))\nprint(square(5))\n\n4\n25\nCode Challenge: Write a cube() function that cubes a number, and along the way, verify that indentation is required after the def statement.\nSolution\ndef cube(x):\n    cb = x ** 3\n    return(cb)\n\nprint(cube(3))\n\n\n27",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions and the  `DataFrame.apply()` Method</span>"
    ]
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#option-payoff-function",
    "href": "chapters/05_functions_apply/functions_apply.html#option-payoff-function",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.2 Option Payoff Function",
    "text": "5.2 Option Payoff Function\nLet’s now write a more financially interesting function.\nOptions are insurance contracts that are written on top of an underlying stock, much like car insurance is written on top of your car. There are two types of options: puts and calls. Put options protect you from the stock price going too low, while call options protect you from the stock price going too high. Both types have a feature called a strike price, which acts much like the deductible of your car insurance. Options expire sometime in the future, and the payoff (payout) of the option at the time of the expiration is as follows:\nLet \\(K\\) be the strike price of an option, and let \\(S_{T}\\) price of its underlying at the time of expiration. Then the payoff of each type of option is as follows:\n\ncall: \\(\\max(S_T - K, 0)\\)\nput: \\(\\max(K - S_T, 0)\\)\n\nWe can codify this as follows.\n\ndef option_payoff(cp, strike, upx):\n    if cp == 'call':\n        payoff = max(upx - strike, 0)\n    elif cp == 'put':\n        payoff = max(strike - upx, 0)\n    \n    return payoff\n\nLet’s verify that our function works.\n\nprint(option_payoff(\"call\", 100, 110))\nprint(option_payoff(\"put\", 100, 110))\nprint(option_payoff(\"call\", 100, 90))\nprint(option_payoff(\"put\", 100, 90))\n\n10\n0\n0\n10",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions and the  `DataFrame.apply()` Method</span>"
    ]
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#loading-packages",
    "href": "chapters/05_functions_apply/functions_apply.html#loading-packages",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.3 Loading Packages",
    "text": "5.3 Loading Packages\nLet’s now load the packages that we will need.\n\nimport numpy as np\nimport pandas as pd",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions and the  `DataFrame.apply()` Method</span>"
    ]
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#reading-in-data",
    "href": "chapters/05_functions_apply/functions_apply.html#reading-in-data",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.4 Reading-In Data",
    "text": "5.4 Reading-In Data\nNext, let’s read in a data file called spy_expiring_options.csv.\nThis data set consists of 21 different options on SPY that expired on November 16, 2018.\nThe upx column is the settle price of SPY from that day, and it will be used to calculate the payoff of each of these options.\n\ndf_opt = pd.read_csv(\"spy_expiring_options.csv\")\ndf_opt = df_opt.round(2)\ndf_opt.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions and the  `DataFrame.apply()` Method</span>"
    ]
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#initializing-payoff-columns",
    "href": "chapters/05_functions_apply/functions_apply.html#initializing-payoff-columns",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.5 Initializing Payoff Columns",
    "text": "5.5 Initializing Payoff Columns\nOur ultimate objective is to add a column of option payoffs to df_opt. We are going to accomplish this task using two different methods: (1) a for loop; (2) the DataFrame.apply() method.\nAs a first step, let’s add two columns to df_opt, one for each method, and then initialize them both with np.nan, which is a special data type that represents missing numerical data.\n\ndf_opt['payoff_loop'] = np.nan\ndf_opt['payoff_apply'] = np.nan\ndf_opt.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\npayoff_loop\npayoff_apply\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\nNaN\nNaN\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\nNaN\nNaN\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\nNaN\nNaN\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\nNaN\nNaN\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0\nNaN\nNaN",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions and the  `DataFrame.apply()` Method</span>"
    ]
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#calculate-payoff_loop-via-for-loop",
    "href": "chapters/05_functions_apply/functions_apply.html#calculate-payoff_loop-via-for-loop",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.6 Calculate payoff_loop via for loop",
    "text": "5.6 Calculate payoff_loop via for loop\nLet’s iterate through df_opt with a for loop and calculate the payoffs one by one. Notice that we are useing the .at indexer which is specifically designed to grab a single value from a column.\n\nfor ix in df_opt.index:\n    \n    # grabbing data from dataframe\n    opt_type = df_opt.at[ix, 'type']\n    strike = df_opt.at[ix, 'strike']\n    upx = df_opt.at[ix, 'upx']\n    \n    # calculating payoff\n    payoff = option_payoff(opt_type, strike, upx)\n    \n    # writing payoff to dataframe\n    df_opt.at[ix, 'payoff_loop'] = payoff\n      \ndf_opt\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\npayoff_loop\npayoff_apply\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\n0.00\nNaN\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\n0.00\nNaN\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\n0.00\nNaN\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\n0.00\nNaN\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0\n0.00\nNaN\n\n\n5\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.5\n0.00\nNaN\n\n\n6\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.0\n0.00\nNaN\n\n\n7\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.5\n0.00\nNaN\n\n\n8\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.0\n0.27\nNaN\n\n\n9\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.5\n0.77\nNaN\n\n\n10\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.0\n1.27\nNaN\n\n\n11\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.5\n1.77\nNaN\n\n\n12\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.0\n2.27\nNaN\n\n\n13\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.5\n2.77\nNaN\n\n\n14\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.0\n3.27\nNaN\n\n\n15\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.5\n3.77\nNaN\n\n\n16\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.0\n0.00\nNaN\n\n\n17\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.5\n0.00\nNaN\n\n\n18\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.0\n0.00\nNaN\n\n\n19\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.5\n0.00\nNaN\n\n\n20\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n280.0\n0.00\nNaN",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions and the  `DataFrame.apply()` Method</span>"
    ]
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#calculate-payoff_apply-via-.apply",
    "href": "chapters/05_functions_apply/functions_apply.html#calculate-payoff_apply-via-.apply",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.7 Calculate payoff_apply via .apply()",
    "text": "5.7 Calculate payoff_apply via .apply()\nThe DataFrame.apply() method allows us to perform these calculations without explicitly iterating through df_opt with a for loop. It is a way to vectorize user defined functions.\nIn order to make use of .apply(), we will have to construct our custom payoff function slightly differently. The following opt_pay() function expects as its argument the row of a DataFrame.\n\ndef opt_pay(row):\n    # reading function inputs from DataFrame row\n    cp = row['type']\n    strike = row['strike']\n    upx = row['upx']\n    \n    # option payoff logic\n    if cp == 'call':\n        payoff = max(upx - strike, 0)\n    elif cp == 'put':\n        payoff = max(strike - upx, 0)\n    \n    return payoff\n\nWe can use .apply() to calculate the payoffs in a single line of code.\n\ndf_opt['payoff_apply'] = df_opt.apply(opt_pay, axis = 1)\ndf_opt\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\npayoff_loop\npayoff_apply\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\n0.00\n0.00\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\n0.00\n0.00\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\n0.00\n0.00\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\n0.00\n0.00\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0\n0.00\n0.00\n\n\n5\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.5\n0.00\n0.00\n\n\n6\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.0\n0.00\n0.00\n\n\n7\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.5\n0.00\n0.00\n\n\n8\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.0\n0.27\n0.27\n\n\n9\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.5\n0.77\n0.77\n\n\n10\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.0\n1.27\n1.27\n\n\n11\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.5\n1.77\n1.77\n\n\n12\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.0\n2.27\n2.27\n\n\n13\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.5\n2.77\n2.77\n\n\n14\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.0\n3.27\n3.27\n\n\n15\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.5\n3.77\n3.77\n\n\n16\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.0\n0.00\n0.00\n\n\n17\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.5\n0.00\n0.00\n\n\n18\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.0\n0.00\n0.00\n\n\n19\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.5\n0.00\n0.00\n\n\n20\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n280.0\n0.00\n0.00\n\n\n\n\n\n\n\n\nCode Challenge: Add a column to df_opt that identifies if the upx is bigger or smaller than strike. Do this by writing a custom function and then using DataFrame.apply().\n\n\nSolution\ndef big_small(row):\n    upx = row['upx']\n    strike = row['strike']\n    \n    if upx &gt;= strike:\n        return('bigger')\n    else:\n        return('smaller')\n\n\n\n\nSolution\ndf_opt['big_small'] = df_opt.apply(big_small, axis = 1)\ndf_opt\n\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\npayoff_loop\npayoff_apply\nbig_small\n\n\n\n\n0\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.0\n0.00\n0.00\nbigger\n\n\n1\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n270.5\n0.00\n0.00\nbigger\n\n\n2\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.0\n0.00\n0.00\nbigger\n\n\n3\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n271.5\n0.00\n0.00\nbigger\n\n\n4\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.0\n0.00\n0.00\nbigger\n\n\n5\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n272.5\n0.00\n0.00\nbigger\n\n\n6\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.0\n0.00\n0.00\nbigger\n\n\n7\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n273.5\n0.00\n0.00\nbigger\n\n\n8\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.0\n0.27\n0.27\nsmaller\n\n\n9\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n274.5\n0.77\n0.77\nsmaller\n\n\n10\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.0\n1.27\n1.27\nsmaller\n\n\n11\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n275.5\n1.77\n1.77\nsmaller\n\n\n12\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.0\n2.27\n2.27\nsmaller\n\n\n13\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n276.5\n2.77\n2.77\nsmaller\n\n\n14\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.0\n3.27\n3.27\nsmaller\n\n\n15\nSPY\n273.73\nput\n2018-11-16\n2018-11-16\n277.5\n3.77\n3.77\nsmaller\n\n\n16\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.0\n0.00\n0.00\nsmaller\n\n\n17\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n278.5\n0.00\n0.00\nsmaller\n\n\n18\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.0\n0.00\n0.00\nsmaller\n\n\n19\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n279.5\n0.00\n0.00\nsmaller\n\n\n20\nSPY\n273.73\ncall\n2018-11-16\n2018-11-16\n280.0\n0.00\n0.00\nsmaller",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions and the  `DataFrame.apply()` Method</span>"
    ]
  },
  {
    "objectID": "chapters/05_functions_apply/functions_apply.html#related-reading",
    "href": "chapters/05_functions_apply/functions_apply.html#related-reading",
    "title": "5  Functions and the DataFrame.apply() Method",
    "section": "5.8 Related Reading",
    "text": "5.8 Related Reading\nA Whirlwind Tour of Python - Chapter 8 - Control Flow\nA Whirlwind Tour of Python - Chapter 9 - Defining Functions",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions and the  `DataFrame.apply()` Method</span>"
    ]
  },
  {
    "objectID": "chapters/06_merge/merge.html",
    "href": "chapters/06_merge/merge.html",
    "title": "6  Merging DataFrames",
    "section": "",
    "text": "6.1 Loading Packages\nIn this chapter we will learn how to merge together two DataFrames. In SQL this is referred to as joining. The effect of merging is similar to the lookup functions in Excel.\nLet’s load the packages we will need for this tutorial.\nimport numpy as np\nimport pandas as pd",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merging `DataFrames`</span>"
    ]
  },
  {
    "objectID": "chapters/06_merge/merge.html#reading-in-data",
    "href": "chapters/06_merge/merge.html#reading-in-data",
    "title": "6  Merging DataFrames",
    "section": "6.2 Reading-In Data",
    "text": "6.2 Reading-In Data\nThe data set we are going to use is a list of ETFs that have weekly expiring options. What does that mean? Most stocks or ETFs have exchange traded options that expire every month, and at any given time the monthly expiring options go out about a year. The most liquid underlyings have options that expire every week; these weekly expiring options go out about 6-8 weeks.\nThis is a list that is published by the CBOE and it consists of all the ETFs that have weekly options trading.\n\ndf_weekly = pd.read_csv('weekly_etf.csv')\ndf_weekly.head()\n\n\n\n\n\n\n\n\nticker\nname\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\n\n\n1\nAMLP\nAlerian MLP ETF\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\n\n\n\n\n\n\n\nThe next data set that we are going to load is a comprehensive list of all ETFs that are trading in the market.\n\ndf_etf = pd.read_csv(\"etf.csv\")\ndf_etf.head()\n\n\n\n\n\n\n\n\nsymbol\nname\nissuer\nexpense_ratio\naum\nspread\nsegment\n\n\n\n\n0\nSPY\nSPDR S&P 500 ETF Trust\nState Street Global Advisors\n0.09%\n$275.42B\n0.00%\nEquity: U.S. - Large Cap\n\n\n1\nIVV\niShares Core S&P 500 ETF\nBlackRock\n0.04%\n$155.86B\n0.01%\nEquity: U.S. - Large Cap\n\n\n2\nVTI\nVanguard Total Stock Market ETF\nVanguard\n0.04%\n$103.58B\n0.01%\nEquity: U.S. - Total Market\n\n\n3\nVOO\nVanguard S&P 500 ETF\nVanguard\n0.04%\n$96.91B\n0.01%\nEquity: U.S. - Large Cap\n\n\n4\nEFA\niShares MSCI EAFE ETF\nBlackRock\n0.32%\n$72.12B\n0.01%\nEquity: Developed Markets Ex-U.S. - Total Market\n\n\n\n\n\n\n\nMotivation: Notice that df_etf has a segment column which df_weekly does not. This segment column contains asset-class information that could be useful for categorizing the weekly ETFs.\nObjective: we want to get the segment column into df_weekly.\nThere are a couple of ways of accomplishing this in pandas and both of them involve the pd.merge() method:\n\ninner-merge\nleft/right-merge (sometimes called outer merge)",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merging `DataFrames`</span>"
    ]
  },
  {
    "objectID": "chapters/06_merge/merge.html#inner",
    "href": "chapters/06_merge/merge.html#inner",
    "title": "6  Merging DataFrames",
    "section": "6.3 Inner",
    "text": "6.3 Inner\nAs with many of the basic operations in data analysis, it’s easiest to understand inner-merges by digging into an example.\nHere is the line of code that accomplishes most of the work that we want done:\n\npd.merge(df_weekly, df_etf, how='inner', left_on='ticker', right_on='symbol')\n\n\n\n\n\n\n\n\nticker\nname_x\nsymbol\nname_y\nissuer\nexpense_ratio\naum\nspread\nsegment\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\nAMJ\nJ.P. Morgan Alerian MLP Index ETN\nJPMorgan\n0.85%\n$3.45B\n0.04%\nEquity: U.S. MLPs\n\n\n1\nAMLP\nAlerian MLP ETF\nAMLP\nAlerian MLP ETF\nALPS\n0.85%\n$10.64B\n0.10%\nEquity: U.S. MLPs\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nDeutsche Bank\n0.65%\n$630.14M\n0.04%\nEquity: China - Total Market\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\nDIA\nSPDR Dow Jones Industrial Average ETF Trust\nState Street Global Advisors\n0.17%\n$21.70B\n0.01%\nEquity: U.S. - Large Cap\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\nDUST\nDirexion Daily Gold Miners Index Bear 3x Shares\nDirexion\n1.08%\n$122.21M\n0.06%\nInverse Equity: Global Gold Miners\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n61\nXLV\nHEALTH CARE SELECT SECTOR SPDR\nXLV\nHealth Care Select Sector SPDR Fund\nState Street Global Advisors\n0.13%\n$17.49B\n0.01%\nEquity: U.S. Health Care\n\n\n62\nXLY\nConsumer Discretionary Select Sector SPDR\nXLY\nConsumer Discretionary Select Sector SPDR Fund\nState Street Global Advisors\n0.13%\n$14.35B\n0.01%\nEquity: U.S. Consumer Cyclicals\n\n\n63\nXME\nSPDR S&P Metals & Mining ETF\nXME\nSPDR S&P Metals & Mining ETF\nState Street Global Advisors\n0.35%\n$879.10M\n0.03%\nEquity: U.S. Metals & Mining\n\n\n64\nXOP\nP Oil & Gas Exploration & Production ETF\nXOP\nSPDR S&P Oil & Gas Exploration & Production ETF\nState Street Global Advisors\n0.35%\n$3.06B\n0.02%\nEquity: U.S. Oil & Gas Exploration & Production\n\n\n65\nXRT\nSPDR S&P Oil & Gas Exploration & Production ETF\nXRT\nSPDR S&P Retail ETF\nState Street Global Advisors\n0.35%\n$704.67M\n0.02%\nEquity: U.S. Retail\n\n\n\n\n66 rows × 9 columns\n\n\n\nObservations on the syntax:\n\nThe first two arguments of pd.merge() are the two DataFrames we want to merge together. The first DataFrame is the left DataFrame and the second one is the right DataFrame.\nThe how argument defines the type of merge.\nleft_on is the column in the left table that will be used for matching, right_on is the column in the right table that will be used for matching.\n\nObservations on output:\n\nThe output is basically each of the two tables smashed together, however only the rows with matching ticker/symbol are retained in the output. All columns of both tables are included.\ndf_weekly had 67 rows in it, and df_etf had 2,160 row in it. The DataFrame that results from pd.merge() has 66 rows in it.\nNotice that both df_weekly and df_etf have a column called name. In the merged DataFrame, suffixes of _x and _y have been added to the column names to make them unique.\n\nLet’s do a little clean up of our DataFrame so that it’s just the information that we wanted: df_weekly with the segment column added to it. Notice that .merge() is also a DataFrame method, and we use this form to invoke method chaining.\n\ndf_inner = \\\n    (\n    df_weekly\n        .merge(df_etf, how='inner', left_on='ticker', right_on='symbol')\n        [['ticker', 'name_x', 'segment']]\n        .rename(columns={'name_x':'name'})\n    )\ndf_inner\n\n\n\n\n\n\n\n\nticker\nname\nsegment\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\nEquity: U.S. MLPs\n\n\n1\nAMLP\nAlerian MLP ETF\nEquity: U.S. MLPs\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nEquity: China - Total Market\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\nEquity: U.S. - Large Cap\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\nInverse Equity: Global Gold Miners\n\n\n...\n...\n...\n...\n\n\n61\nXLV\nHEALTH CARE SELECT SECTOR SPDR\nEquity: U.S. Health Care\n\n\n62\nXLY\nConsumer Discretionary Select Sector SPDR\nEquity: U.S. Consumer Cyclicals\n\n\n63\nXME\nSPDR S&P Metals & Mining ETF\nEquity: U.S. Metals & Mining\n\n\n64\nXOP\nP Oil & Gas Exploration & Production ETF\nEquity: U.S. Oil & Gas Exploration & Production\n\n\n65\nXRT\nSPDR S&P Oil & Gas Exploration & Production ETF\nEquity: U.S. Retail\n\n\n\n\n66 rows × 3 columns",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merging `DataFrames`</span>"
    ]
  },
  {
    "objectID": "chapters/06_merge/merge.html#left",
    "href": "chapters/06_merge/merge.html#left",
    "title": "6  Merging DataFrames",
    "section": "6.4 Left",
    "text": "6.4 Left\nNotice that in the inner-join example from the previous section, the original DataFrame of ETFs with weekly options (df_weekly) had 67 rows, but the merged DataFrame with the segment column added (df_inner) only has 66 rows.\n\nprint(df_weekly.shape)\nprint(df_inner.shape)\n\n(67, 2)\n(66, 3)\n\n\nSo what happened? This means that one of the tickers from df_weekly had no matching symbol in df_etf.\nInner-merges, by design, are only intended to retain rows that have matches in both tables. This may or may not be the desired behavior you are looking for.\nLet’s say that instead we wanted to keep ALL the rows in the left DataFrame, df_weekly, irrespective of whether there is a match in the right DataFrame.\nThis is precisely what a left-merge is. The syntax is the exact same as before except for the how argument is set to 'left'.\n\npd.merge(df_weekly, df_etf, how='left', left_on='ticker', right_on='symbol')\n\n\n\n\n\n\n\n\nticker\nname_x\nsymbol\nname_y\nissuer\nexpense_ratio\naum\nspread\nsegment\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\nAMJ\nJ.P. Morgan Alerian MLP Index ETN\nJPMorgan\n0.85%\n$3.45B\n0.04%\nEquity: U.S. MLPs\n\n\n1\nAMLP\nAlerian MLP ETF\nAMLP\nAlerian MLP ETF\nALPS\n0.85%\n$10.64B\n0.10%\nEquity: U.S. MLPs\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nDeutsche Bank\n0.65%\n$630.14M\n0.04%\nEquity: China - Total Market\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\nDIA\nSPDR Dow Jones Industrial Average ETF Trust\nState Street Global Advisors\n0.17%\n$21.70B\n0.01%\nEquity: U.S. - Large Cap\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\nDUST\nDirexion Daily Gold Miners Index Bear 3x Shares\nDirexion\n1.08%\n$122.21M\n0.06%\nInverse Equity: Global Gold Miners\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n62\nXLV\nHEALTH CARE SELECT SECTOR SPDR\nXLV\nHealth Care Select Sector SPDR Fund\nState Street Global Advisors\n0.13%\n$17.49B\n0.01%\nEquity: U.S. Health Care\n\n\n63\nXLY\nConsumer Discretionary Select Sector SPDR\nXLY\nConsumer Discretionary Select Sector SPDR Fund\nState Street Global Advisors\n0.13%\n$14.35B\n0.01%\nEquity: U.S. Consumer Cyclicals\n\n\n64\nXME\nSPDR S&P Metals & Mining ETF\nXME\nSPDR S&P Metals & Mining ETF\nState Street Global Advisors\n0.35%\n$879.10M\n0.03%\nEquity: U.S. Metals & Mining\n\n\n65\nXOP\nP Oil & Gas Exploration & Production ETF\nXOP\nSPDR S&P Oil & Gas Exploration & Production ETF\nState Street Global Advisors\n0.35%\n$3.06B\n0.02%\nEquity: U.S. Oil & Gas Exploration & Production\n\n\n66\nXRT\nSPDR S&P Oil & Gas Exploration & Production ETF\nXRT\nSPDR S&P Retail ETF\nState Street Global Advisors\n0.35%\n$704.67M\n0.02%\nEquity: U.S. Retail\n\n\n\n\n67 rows × 9 columns\n\n\n\nLet’s put this left-merged table into a DataFrame called df_left, and perform a bit of data munging.\n\ndf_left = \\\n    (\n    df_weekly\n        .merge(df_etf, how='left', left_on='ticker', right_on='symbol')\n        [['ticker', 'name_x', 'segment']]\n        .rename(columns={'name_x':'name'})\n    )\ndf_left\n\n\n\n\n\n\n\n\nticker\nname\nsegment\n\n\n\n\n0\nAMJ\nJP Morgan Alerian MLP Index ETN\nEquity: U.S. MLPs\n\n\n1\nAMLP\nAlerian MLP ETF\nEquity: U.S. MLPs\n\n\n2\nASHR\nXtrackers Harvest CSI 300 China A-Shares ETF\nEquity: China - Total Market\n\n\n3\nDIA\nSPDR Dow Jones Ind Av ETF Trust\nEquity: U.S. - Large Cap\n\n\n4\nDUST\nDirexion Daily Gold Miners Index Bear 3X Shares\nInverse Equity: Global Gold Miners\n\n\n...\n...\n...\n...\n\n\n62\nXLV\nHEALTH CARE SELECT SECTOR SPDR\nEquity: U.S. Health Care\n\n\n63\nXLY\nConsumer Discretionary Select Sector SPDR\nEquity: U.S. Consumer Cyclicals\n\n\n64\nXME\nSPDR S&P Metals & Mining ETF\nEquity: U.S. Metals & Mining\n\n\n65\nXOP\nP Oil & Gas Exploration & Production ETF\nEquity: U.S. Oil & Gas Exploration & Production\n\n\n66\nXRT\nSPDR S&P Oil & Gas Exploration & Production ETF\nEquity: U.S. Retail\n\n\n\n\n67 rows × 3 columns\n\n\n\n\nCode Challenge: Use .query() on df_left to verify that ticker FTK has NaNs for all the columns from df_etf. Do this in two separate ways:\n\nquerying on ticker\nquerying on segment\n\n\n\nSolution\ndf_left.query('ticker == \"FTK\"')\n\n\n\n\n\n\n\n\n\nticker\nname\nsegment\n\n\n\n\n17\nFTK\nFLOTEK INDUSTRIES INC\nNaN\n\n\n\n\n\n\n\n\n\nSolution\ndf_left.query('segment.isnull()')\n\n\n\n\n\n\n\n\n\nticker\nname\nsegment\n\n\n\n\n17\nFTK\nFLOTEK INDUSTRIES INC\nNaN\n\n\n\n\n\n\n\n\nResearch Challenge: Google FTK and figure out why it’s not in df_etf.\n\n\nSolution\n# FTK is a stock not and ETF.",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merging `DataFrames`</span>"
    ]
  },
  {
    "objectID": "chapters/06_merge/merge.html#related-reading",
    "href": "chapters/06_merge/merge.html#related-reading",
    "title": "6  Merging DataFrames",
    "section": "6.5 Related Reading",
    "text": "6.5 Related Reading\nPython Data Science Handboook - Section 3.7 - Combining Datasets: Merging and Joining",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Merging `DataFrames`</span>"
    ]
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html",
    "title": "7  .groupby() and .agg() - 1",
    "section": "",
    "text": "7.1 Loading Packages\nThe real power of data analysis with DataFrames comes into focus when we start utilizing the .groupby() and .agg() methods. This is known as grouping and aggregating.\nTalking about grouping in the abstract can be confusing; I think it’s best to see grouping in action by doing meaningful calculations.\nThe purpose of this chapter is to introduce grouping and aggregation by way of the following finance task: calculating monthly returns and monthly volatilities for several ETFs.\nLet’s begin by loading the packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 1</span>"
    ]
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#reading-in-data",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#reading-in-data",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.2 Reading-In Data",
    "text": "7.2 Reading-In Data\nOur analysis will be on the set of of July 2021 prices for SPY, IWM, QQQ, DIA.\nLet’s read-in that data with pandas_datareader.\n\npd.options.display.max_rows = 25\ndf_etf = yf.download(\n    ['SPY', 'QQQ', 'IWM', 'DIA'], start='2021-06-30', end='2021-07-31',\n    auto_adjust=False, rounding=True\n)\ndf_etf.head()\n\n[*********************100%***********************]  4 of 4 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\n...\nLow\nOpen\nVolume\n\n\nTicker\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\n...\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\nDIA\nIWM\nQQQ\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021-06-30\n320.48\n217.77\n345.63\n404.51\n344.95\n229.37\n354.43\n428.06\n345.51\n230.32\n...\n353.83\n427.18\n342.38\n228.65\n354.83\n427.21\n3778900\n26039000\n32724000\n64827900\n\n\n2021-07-01\n321.79\n219.69\n345.76\n406.75\n346.36\n231.39\n354.57\n430.43\n346.40\n231.85\n...\n352.68\n428.80\n345.78\n230.81\n354.07\n428.87\n3606900\n18089100\n29290000\n53441000\n\n\n2021-07-02\n323.26\n217.60\n349.73\n409.86\n347.94\n229.19\n358.64\n433.72\n348.29\n232.08\n...\n356.28\n430.52\n347.04\n232.00\n356.52\n431.67\n3013500\n21029700\n32727200\n57697700\n\n\n2021-07-06\n321.29\n214.44\n351.24\n409.11\n345.82\n225.86\n360.19\n432.93\n348.11\n229.46\n...\n356.49\n430.01\n347.75\n229.36\n359.26\n433.78\n3910600\n27771300\n38842400\n68710400\n\n\n2021-07-07\n322.32\n212.44\n351.99\n410.56\n346.92\n223.76\n360.95\n434.46\n347.14\n226.67\n...\n358.94\n431.51\n345.65\n225.54\n362.45\n433.66\n3347000\n28521500\n35265200\n63549500\n\n\n\n\n5 rows × 24 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = (\n    df_etf[[('Close','DIA'), ('Close', 'IWM'), ('Close', 'QQQ'), ('Close', 'SPY')]] # grab close prices\n    .stack() # pivot the table\n    .reset_index()\n    .rename_axis(None, axis=1) # remove the name from the row index\n    .rename(columns={'Date':'date', 'Ticker':'symbol', 'Close':'close'}) # columns in snake-case\n    .sort_values(by=['symbol', 'date'])\n)\ndf_etf\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\n\n\n\n\n0\n2021-06-30\nDIA\n344.95\n\n\n4\n2021-07-01\nDIA\n346.36\n\n\n8\n2021-07-02\nDIA\n347.94\n\n\n12\n2021-07-06\nDIA\n345.82\n\n\n16\n2021-07-07\nDIA\n346.92\n\n\n...\n...\n...\n...\n\n\n71\n2021-07-26\nSPY\n441.02\n\n\n75\n2021-07-27\nSPY\n439.01\n\n\n79\n2021-07-28\nSPY\n438.83\n\n\n83\n2021-07-29\nSPY\n440.65\n\n\n87\n2021-07-30\nSPY\n438.51\n\n\n\n\n88 rows × 3 columns",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 1</span>"
    ]
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#daily-returns-with-groupby",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#daily-returns-with-groupby",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.3 Daily Returns with groupby()",
    "text": "7.3 Daily Returns with groupby()\nOur ultimate goal is to calculate monthly returns and monthly volatilities for each ETF in df_etf. These quantities are both functions of daily returns. So our first order of business is to calculate daily returns.\nIn a previous tutorial we calculated daily returns in a simple vectorized fashion. Unfortunately, we can’t use the exact same approach here because there are multiple ETFs in the data set.\nTo overcome this challenge we will use our first application of .groupby().\nHere is the .groupby() code that calculates daily returns for each ETF.\n\n# sorting values to get everything in the right order\ndf_etf.sort_values(['symbol', 'date'], inplace=True)\n\n# vectorized return calculation\ndf_etf['ret'] = \\\n    df_etf['close'].groupby(df_etf['symbol']).pct_change()\ndf_etf\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\nret\n\n\n\n\n0\n2021-06-30\nDIA\n344.95\nNaN\n\n\n4\n2021-07-01\nDIA\n346.36\n0.004088\n\n\n8\n2021-07-02\nDIA\n347.94\n0.004562\n\n\n12\n2021-07-06\nDIA\n345.82\n-0.006093\n\n\n16\n2021-07-07\nDIA\n346.92\n0.003181\n\n\n...\n...\n...\n...\n...\n\n\n71\n2021-07-26\nSPY\n441.02\n0.002455\n\n\n75\n2021-07-27\nSPY\n439.01\n-0.004558\n\n\n79\n2021-07-28\nSPY\n438.83\n-0.000410\n\n\n83\n2021-07-29\nSPY\n440.65\n0.004147\n\n\n87\n2021-07-30\nSPY\n438.51\n-0.004856\n\n\n\n\n88 rows × 4 columns\n\n\n\n\nCode Challenge: If the .group_by() worked correctly, we should see a NaN value in the ret column for the first trade-date of each ETF. Use DataFrame.query() to confirm this.\n\n\nSolution\ndf_etf.query('ret.isnull()')\n\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\nret\n\n\n\n\n0\n2021-06-30\nDIA\n344.95\nNaN\n\n\n1\n2021-06-30\nIWM\n229.37\nNaN\n\n\n2\n2021-06-30\nQQQ\n354.43\nNaN\n\n\n3\n2021-06-30\nSPY\n428.06\nNaN",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 1</span>"
    ]
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#monthly-return-for-each-symbol",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#monthly-return-for-each-symbol",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.4 Monthly Return for Each symbol",
    "text": "7.4 Monthly Return for Each symbol\nWe’ll now proceed to calculate monthly returns and monthly volatilities for each of the ETFs in our data set. This amounts to first grouping by symbol, and then performing an aggregation calculation on ret (daily returns).\nLet’s start with monthly returns. As a preliminary step we’ll calculate the daily growth factor in a separate column.\n\ndf_etf['daily_factor'] = 1 + df_etf['ret']\ndf_etf.head()\n\n\n\n\n\n\n\n\ndate\nsymbol\nclose\nret\ndaily_factor\n\n\n\n\n0\n2021-06-30\nDIA\n344.95\nNaN\nNaN\n\n\n4\n2021-07-01\nDIA\n346.36\n0.004088\n1.004088\n\n\n8\n2021-07-02\nDIA\n347.94\n0.004562\n1.004562\n\n\n12\n2021-07-06\nDIA\n345.82\n-0.006093\n0.993907\n\n\n16\n2021-07-07\nDIA\n346.92\n0.003181\n1.003181\n\n\n\n\n\n\n\nRecall that the monthly growth factor is the product of the daily growth factors. Here is a way to write all that logic in a single line using .groupby() and .agg():\n\ndf_grouped_factor = \\\n    df_etf.groupby(['symbol'])['daily_factor'].agg([np.prod]).reset_index()\ndf_grouped_factor\n\n\n\n\n\n\n\n\nsymbol\nprod\n\n\n\n\n0\nDIA\n1.013132\n\n\n1\nIWM\n0.963727\n\n\n2\nQQQ\n1.028609\n\n\n3\nSPY\n1.024412\n\n\n\n\n\n\n\nNotice that pandas isn’t very sophisticated about the name that it gives to the column that stores the aggregation calculation. It just gave it the name prod, which is the name of the function that was used in the aggregation calculation. Let’s make df_grouped_factor a bit more readable by renaming that column.\n\ndf_grouped_factor.rename(columns={'prod': 'monthly_factor'}, inplace=True)\ndf_grouped_factor\n\n\n\n\n\n\n\n\nsymbol\nmonthly_factor\n\n\n\n\n0\nDIA\n1.013132\n\n\n1\nIWM\n0.963727\n\n\n2\nQQQ\n1.028609\n\n\n3\nSPY\n1.024412\n\n\n\n\n\n\n\nAnd finally, recall that the monthly return is calculated by subtracting one from the monthly growth factor.\n\ndf_grouped_factor['monthly_return'] = df_grouped_factor['monthly_factor'] - 1\ndf_grouped_factor[['symbol', 'monthly_return']]\n\n\n\n\n\n\n\n\nsymbol\nmonthly_return\n\n\n\n\n0\nDIA\n0.013132\n\n\n1\nIWM\n-0.036273\n\n\n2\nQQQ\n0.028609\n\n\n3\nSPY\n0.024412",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 1</span>"
    ]
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#monthly-volatility-for-each-symbol",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#monthly-volatility-for-each-symbol",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.5 Monthly Volatility for Each symbol",
    "text": "7.5 Monthly Volatility for Each symbol\nNow let’s calculate the (realized/historical) volatility for each of the ETFs.\nWe once again use .groupby() and .agg() to do most of the work in a single line of code.\n\ndf_grouped_vol = \\\n    df_etf.groupby(['symbol'])['ret'].agg([np.std]).reset_index()\n\ndf_grouped_vol\n\n\n\n\n\n\n\n\nsymbol\nstd\n\n\n\n\n0\nDIA\n0.007733\n\n\n1\nIWM\n0.014032\n\n\n2\nQQQ\n0.006832\n\n\n3\nSPY\n0.007152\n\n\n\n\n\n\n\nAgain, let’s rename our aggregation column to something more descriptive.\n\ndf_grouped_vol.rename(columns={'std':'daily_vol'}, inplace=True)\ndf_grouped_vol\n\n\n\n\n\n\n\n\nsymbol\ndaily_vol\n\n\n\n\n0\nDIA\n0.007733\n\n\n1\nIWM\n0.014032\n\n\n2\nQQQ\n0.006832\n\n\n3\nSPY\n0.007152\n\n\n\n\n\n\n\nWhat we have calculated is a daily volatility, but when practitioners talk about volatility, they typically annualize it. A daily volatility is annualized by multiplying by \\(\\sqrt{252}\\).\n\ndf_grouped_vol['ann_vol'] = df_grouped_vol['daily_vol'] * np.sqrt(252)\ndf_grouped_vol\n\n\n\n\n\n\n\n\nsymbol\ndaily_vol\nann_vol\n\n\n\n\n0\nDIA\n0.007733\n0.122752\n\n\n1\nIWM\n0.014032\n0.222744\n\n\n2\nQQQ\n0.006832\n0.108455\n\n\n3\nSPY\n0.007152\n0.113542\n\n\n\n\n\n\n\n\nCode Challenge Use .groupby() and .agg() to calculate the average daily return for each of the ETFs.\n\n(\ndf_etf\n    .groupby(['symbol'])[['ret']].agg(np.mean)\n    .reset_index()\n    .rename(columns={'ret':'daily_avg_ret'})\n)\n\n\n\n\n\n\n\n\nsymbol\ndaily_avg_ret\n\n\n\n\n0\nDIA\n0.000650\n\n\n1\nIWM\n-0.001665\n\n\n2\nQQQ\n0.001366\n\n\n3\nSPY\n0.001174",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 1</span>"
    ]
  },
  {
    "objectID": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#related-reading",
    "href": "chapters/07_groupby_aggregation_part_1/groupby_aggregation_part_1.html#related-reading",
    "title": "7  .groupby() and .agg() - 1",
    "section": "7.6 Related Reading",
    "text": "7.6 Related Reading\nPython Data Science Handbook (VanderPlas) - Section 3.8 - Aggregation and Grouping\nPython for Data Analysis (McKinney) - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations\nOptions, Futures, and Other Derivatives (Hull) - Chapter 15 (pp 325-329) The Black-Scholes-Merton Model",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 1</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html",
    "title": "8  .groupby() and .agg() - 2",
    "section": "",
    "text": "8.1 Loading Packages\nThe purpose of this chapter is to demonstrate how to use .groupby() and .agg() with user defined functions.\nIn service of this objective our analysis goal will be to calculate three monthly statistics for several ETFs during the year of 2020. The three statistics we will calculate are:\nLet’s load the packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#reading-in-data",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#reading-in-data",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.2 Reading-In Data",
    "text": "8.2 Reading-In Data\nIn this chapter we will be working with price data for the Select Sector SPDR ETFs. Each of these funds tracks a particular subset (sector) of the SP&500 Index. For example, XLF tracks the financial sector and has major holdings in JP Morgan, Wells Fargo, and Bank of America.\nLet’s use pandas_datareader to grab the data from Yahoo Finance.\n\npd.options.display.max_rows = 25\nlst_symbols = ['XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLI', 'XLB', 'XLRE', 'XLK', 'XLU',]\ndf_etf = yf.download(\n    lst_symbols, start='2020-01-01', end='2020-12-31',\n    auto_adjust=False, rounding=True,\n)\ndf_etf.head()\n\n[*********************100%***********************]  10 of 10 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\n...\nVolume\n\n\nTicker\nXLB\nXLE\nXLF\nXLI\nXLK\nXLP\nXLRE\nXLU\nXLV\nXLY\n...\nXLB\nXLE\nXLF\nXLI\nXLK\nXLP\nXLRE\nXLU\nXLV\nXLY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n54.30\n47.82\n27.97\n75.93\n88.95\n54.08\n31.81\n53.65\n93.51\n121.00\n...\n7357400\n11944700\n28843300\n16121300\n13283500\n14460700\n4380100\n19107700\n6277400\n6295500\n\n\n2020-01-03\n53.43\n47.68\n27.68\n75.79\n87.95\n54.00\n32.04\n53.76\n92.69\n119.97\n...\n12423200\n29502900\n51363600\n17571300\n15011800\n26388900\n3499000\n17989300\n8247500\n5596400\n\n\n2020-01-06\n53.19\n48.05\n27.66\n75.81\n88.16\n54.11\n32.05\n53.81\n93.27\n120.31\n...\n15764400\n22458100\n27956100\n16153100\n7815000\n22541700\n3097200\n10444500\n6441800\n6411600\n\n\n2020-01-07\n53.13\n47.93\n27.48\n75.66\n88.12\n53.69\n31.70\n53.74\n93.08\n120.12\n...\n20266900\n11462500\n39627500\n16675400\n7681800\n15607600\n3550600\n13070300\n6335300\n9150800\n\n\n2020-01-08\n53.32\n47.14\n27.66\n75.91\n89.06\n53.89\n31.86\n53.71\n93.69\n120.48\n...\n8079600\n19021400\n47966600\n10677700\n11627200\n11451400\n5089000\n12741400\n7494700\n4725900\n\n\n\n\n5 rows × 60 columns\n\n\n\nThis data is not as tidy as we would like. Let’s use method chaining to perform a series of data munging operations.\n\ndf_etf = (\n    df_etf\n    .stack()\n    .reset_index()\n    .rename_axis(None, axis=1)\n    .rename(columns={'Date':'date', 'Ticker':'symbol', 'Adj Close':'adj_close','Close':'close', \n                    'High':'high', 'Low':'low', 'Open':'open', 'Volume':'volume'}) #renaming columns\n    .sort_values(['symbol', 'date'])\n)\ndf_etf\n\n\n\n\n\n\n\n\ndate\nsymbol\nadj_close\nclose\nhigh\nlow\nopen\nvolume\n\n\n\n\n0\n2020-01-02\nXLB\n54.30\n60.70\n61.94\n60.63\n61.83\n7357400\n\n\n10\n2020-01-03\nXLB\n53.43\n59.72\n60.44\n59.70\n60.08\n12423200\n\n\n20\n2020-01-06\nXLB\n53.19\n59.46\n59.83\n59.41\n59.55\n15764400\n\n\n30\n2020-01-07\nXLB\n53.13\n59.39\n59.80\n59.20\n59.36\n20266900\n\n\n40\n2020-01-08\nXLB\n53.32\n59.60\n59.84\n59.20\n59.40\n8079600\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2479\n2020-12-23\nXLY\n151.90\n157.55\n158.05\n156.57\n157.24\n2173000\n\n\n2489\n2020-12-24\nXLY\n152.22\n157.88\n158.12\n157.21\n157.70\n1048800\n\n\n2499\n2020-12-28\nXLY\n153.96\n159.68\n160.32\n158.60\n159.42\n2912400\n\n\n2509\n2020-12-29\nXLY\n154.00\n159.73\n160.53\n158.98\n160.24\n2431200\n\n\n2519\n2020-12-30\nXLY\n154.93\n160.69\n160.93\n160.13\n160.30\n2440700\n\n\n\n\n2520 rows × 8 columns\n\n\n\n\nCoding Challenge: Use a DataFrame attribute to determine the number of rows and columns in df_etf.\n\n\nSolution\ndf_etf.shape\n\n\n(2520, 8)",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#exploring-and-cleaning-the-data",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#exploring-and-cleaning-the-data",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.3 Exploring and Cleaning the Data",
    "text": "8.3 Exploring and Cleaning the Data\nAs we can see from the coding challenge, this data set is large (by our standards). Whenever I encounter a new data set that I can’t look at in its entirety, I like to do a bit of exploration via the built-in pandas methods.\nWe know we have a variety of ETFs in our data, but it would be useful to know how many (especially if we were expecting a certain number).\n\nprint(df_etf['symbol'].unique())\nprint(df_etf['symbol'].unique().size)\n\n['XLB' 'XLE' 'XLF' 'XLI' 'XLK' 'XLP' 'XLRE' 'XLU' 'XLV' 'XLY']\n10\n\n\n\nCoding Challenge: What DataFrame attribute could we use to check the data types of the columns of df_etf?\n\n\nSolution\ndf_etf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2520 entries, 0 to 2519\nData columns (total 8 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   date       2520 non-null   datetime64[ns]\n 1   symbol     2520 non-null   object        \n 2   adj_close  2520 non-null   float64       \n 3   close      2520 non-null   float64       \n 4   high       2520 non-null   float64       \n 5   low        2520 non-null   float64       \n 6   open       2520 non-null   float64       \n 7   volume     2520 non-null   int64         \ndtypes: datetime64[ns](1), float64(5), int64(1), object(1)\nmemory usage: 177.2+ KB\n\n\n\nWhen I work with a time series of daily prices that I expect to come from a certain date range, I like to check the first and last trade dates that are represented in the data.\n\nprint(df_etf['date'].min())\nprint(df_etf['date'].max())\n\n2020-01-02 00:00:00\n2020-12-30 00:00:00\n\n\nHere is what we know about our data set thus far:\n\n10 different ETFs are represented.\nPrices are coming from the entirety of 2020.\n\nHere are some things that we aren’t necessarily sure of that would be worth checking in a high-stakes situation:\n\nIs there a row/price for each symbol on each trade date?\nIs there ever more than one row/price for a given symbol on a given trade date?\n\nWe won’t bother answering these questions for the purposes of this chapter, but these are the types of data-integrity questions I will often try to answer when encountering a new data set.",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#calculating-daily-returns-with-groupby",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#calculating-daily-returns-with-groupby",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.4 Calculating Daily Returns with groupby()",
    "text": "8.4 Calculating Daily Returns with groupby()\nOur ultimate goal is to calculate monthly returns and monthly volatilities for each ETF in df_etf. These quantities are both functions of daily returns. So our first order of business is to calculate daily returns.\nIn a previous tutorial we calculated daily returns in a simple vectorized fashion. Unfortunately, we can’t use the exact same approach here because there are multiple ETFs in the data set.\nTo overcome this challenge we will use our first application of .groupby().\nHere is the .groupby() code that calculates daily returns for each ETF.\n\n# sorting values to get everything in the right order\ndf_etf.sort_values(['symbol', 'date'], inplace=True)\n\n# vectorized return calculation\ndf_etf['dly_ret'] = \\\n    df_etf['close'].groupby(df_etf['symbol']).pct_change()\ndf_etf.head()\n\n\n\n\n\n\n\n\ndate\nsymbol\nadj_close\nclose\nhigh\nlow\nopen\nvolume\ndly_ret\n\n\n\n\n0\n2020-01-02\nXLB\n54.30\n60.70\n61.94\n60.63\n61.83\n7357400\nNaN\n\n\n10\n2020-01-03\nXLB\n53.43\n59.72\n60.44\n59.70\n60.08\n12423200\n-0.016145\n\n\n20\n2020-01-06\nXLB\n53.19\n59.46\n59.83\n59.41\n59.55\n15764400\n-0.004354\n\n\n30\n2020-01-07\nXLB\n53.13\n59.39\n59.80\n59.20\n59.36\n20266900\n-0.001177\n\n\n40\n2020-01-08\nXLB\n53.32\n59.60\n59.84\n59.20\n59.40\n8079600\n0.003536",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#adding-year-and-month-columns",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#adding-year-and-month-columns",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.5 Adding year and month Columns",
    "text": "8.5 Adding year and month Columns\nThe ultimate goal is to calculate monthly statistics for each of the ETFs in our data set.\nAs a preliminary step let’s add a month and year column to the df_etf by utilizing the .dt attribute that pandas provides for date columns.\n\ndf_etf['year'] = df_etf['date'].dt.year\ndf_etf['month'] = df_etf['date'].dt.month\ndf_etf[['date', 'year', 'month']].head()\n\n\n\n\n\n\n\n\ndate\nyear\nmonth\n\n\n\n\n0\n2020-01-02\n2020\n1\n\n\n10\n2020-01-03\n2020\n1\n\n\n20\n2020-01-06\n2020\n1\n\n\n30\n2020-01-07\n2020\n1\n\n\n40\n2020-01-08\n2020\n1\n\n\n\n\n\n\n\nLet’s do a quick data-integrity check: There are 10 ETFs in our data set and there are 12 months in a year, so the number of symbol-year-month combinations should be 120.\nThe following code counts the number of rows associated with each symbol-year-month combination and puts that data into a DataFrame.\n\ndf_num_rows = \\\n    df_etf.groupby(['symbol', 'year', 'month']).size().reset_index()\ndf_num_rows.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nmonth\n0\n\n\n\n\n0\nXLB\n2020\n1\n21\n\n\n1\nXLB\n2020\n2\n19\n\n\n2\nXLB\n2020\n3\n22\n\n\n3\nXLB\n2020\n4\n21\n\n\n4\nXLB\n2020\n5\n20\n\n\n\n\n\n\n\n\nCoding Challenge: Confirm that there are the correct number of symbol-year-month combinations in df_num_rows.\n\n\nSolution\ndf_num_rows.shape\n\n\n(120, 4)\n\n\n\nNow that we’ve added the year and month columns we can proceed to calculating our monthly statistics.",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#average-daily-volume",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#average-daily-volume",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.6 Average Daily Volume",
    "text": "8.6 Average Daily Volume\nLet’s start with the most straight-forward calculation: average daily volume, over each month, for all 10 of the ETFs in our data set.\nThis amounts to:\n\ngrouping by symbol, month, and year\napplying the built-in np.mean() function to the volume column\n\n\ndf_volume = \\\n    df_etf.groupby(['symbol', 'year', 'month'])['volume'].agg([np.mean]).reset_index()\ndf_volume.rename(columns={'mean':'avg_volume'}, inplace=True)\ndf_volume.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nmonth\navg_volume\n\n\n\n\n0\nXLB\n2020\n1\n7.235429e+06\n\n\n1\nXLB\n2020\n2\n1.058022e+07\n\n\n2\nXLB\n2020\n3\n1.432920e+07\n\n\n3\nXLB\n2020\n4\n9.000557e+06\n\n\n4\nXLB\n2020\n5\n4.829185e+06\n\n\n\n\n\n\n\n\nCoding Challenge: Calculate the maximum daily volume for each symbol, over the entire year.\n\n\nSolution\ndf_etf.groupby(['symbol', 'year'])['volume'].agg([np.max]).reset_index()\n\n\n\n\n\n\n\n\n\nsymbol\nyear\namax\n\n\n\n\n0\nXLB\n2020\n30741700\n\n\n1\nXLE\n2020\n99356700\n\n\n2\nXLF\n2020\n256525000\n\n\n3\nXLI\n2020\n79118200\n\n\n4\nXLK\n2020\n61727100\n\n\n5\nXLP\n2020\n50978800\n\n\n6\nXLRE\n2020\n49899800\n\n\n7\nXLU\n2020\n90263100\n\n\n8\nXLV\n2020\n39561900\n\n\n9\nXLY\n2020\n20616100",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#monthly-returns",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#monthly-returns",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.7 Monthly Returns",
    "text": "8.7 Monthly Returns\nNext, let’s calculate monthly returns for each of the ETFs in our data set. This amounts to:\n\ngrouping by symbol, month, and year\napplying an aggregation function to the daily_returns column\n\nThese are the same two steps that we have done in our previous aggregation examples. However, there is one additional wrinkle that we are going to have to contend with.\nIn the previous section, we used simple built-in aggregation funtions available through numpy, such as np.max and np.mean. Calculating monthly returns from daily returns is a little more complicated.\nThus, we are going to have to first create a custom function for calculating monthly returns from daily returns, and then use this custom function in .agg().\nThe following code defines our monthly returns function in terms of daily returns:\n\ndef monthly_ret(dly_ret):\n    return np.prod(1 + dly_ret) - 1\n\nNow we can apply our monthly_ret() function for all of our ETFs using the following code.\n\ndf_ret = \\\n    df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_ret]).reset_index()\ndf_ret.head()\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nmonthly_ret\n\n\n\n\n0\nXLB\n1\n2020\n-0.050577\n\n\n1\nXLB\n2\n2020\n-0.085199\n\n\n2\nXLB\n3\n2020\n-0.145675\n\n\n3\nXLB\n4\n2020\n0.151865\n\n\n4\nXLB\n5\n2020\n0.068813\n\n\n\n\n\n\n\nWe can see from our calculation that in March of 2020 XLB had a monthly return of -14.6%.\n\nCoding Challenge: Which ETF had the highest single monthly return in all of 2020? What was the month?\n\n\nSolution\ndf_ret.sort_values(by=['monthly_ret'], ascending=False)\n\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nmonthly_ret\n\n\n\n\n15\nXLE\n4\n2020\n0.307639\n\n\n22\nXLE\n11\n2020\n0.279944\n\n\n111\nXLY\n4\n2020\n0.188825\n\n\n34\nXLF\n11\n2020\n0.168483\n\n\n46\nXLI\n11\n2020\n0.160274\n\n\n...\n...\n...\n...\n...\n\n\n74\nXLRE\n3\n2020\n-0.157380\n\n\n20\nXLE\n9\n2020\n-0.159888\n\n\n38\nXLI\n3\n2020\n-0.192529\n\n\n26\nXLF\n3\n2020\n-0.216999\n\n\n14\nXLE\n3\n2020\n-0.358074\n\n\n\n\n120 rows × 4 columns",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#monthly-volatility",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#monthly-volatility",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.8 Monthly Volatility",
    "text": "8.8 Monthly Volatility\nLet’s use a similar process to calculate the monthly volatility for each of the ETFs.\nWe begin by defining a custom function that calculates the monthly volatility from daily returns. Recall that industry convention is to state these volatilities in annualized terms.\n\ndef monthly_vol(dly_ret):\n    return np.std(dly_ret) * np.sqrt(252)\n\nWe can now use our monthly_vol() function in to perform an aggregating calculation.\n\ndf_vol = \\\n    df_etf.groupby(['symbol', 'month', 'year'])['dly_ret'].agg([monthly_vol]).reset_index()\ndf_vol.head()\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nmonthly_vol\n\n\n\n\n0\nXLB\n1\n2020\n0.150336\n\n\n1\nXLB\n2\n2020\n0.282201\n\n\n2\nXLB\n3\n2020\n0.932265\n\n\n3\nXLB\n4\n2020\n0.503394\n\n\n4\nXLB\n5\n2020\n0.277311\n\n\n\n\n\n\n\n\nCoding Challenge: What was the volatility for XLF in December 2018?\n\n\nSolution\ndf_vol.query('symbol == \"XLF\" & month == 12')\n\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nmonthly_vol\n\n\n\n\n35\nXLF\n12\n2020\n0.137471",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#combining-metrics---inner-.merge",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#combining-metrics---inner-.merge",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.9 Combining Metrics - inner .merge()",
    "text": "8.9 Combining Metrics - inner .merge()\nNow, suppose that we want to combine our three metrics into one report - meaning that we want them organized into one DataFrame in an easy to read fashion.\nOne way to do this is to use the pandas.merge() method that we learned in the previous tutorial to join together df_volume (average daily volume), df_ret (monthly returns), and df_vol (monthly volatility).\n\ndf_joined = \\\n    (\n    df_volume\n        .merge(df_ret, on=['symbol', 'year', 'month'])\n        .merge(df_vol, on=['symbol', 'year', 'month'])\n    )\ndf_joined.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nmonth\navg_volume\nmonthly_ret\nmonthly_vol\n\n\n\n\n0\nXLB\n2020\n1\n7.235429e+06\n-0.050577\n0.150336\n\n\n1\nXLB\n2020\n2\n1.058022e+07\n-0.085199\n0.282201\n\n\n2\nXLB\n2020\n3\n1.432920e+07\n-0.145675\n0.932265\n\n\n3\nXLB\n2020\n4\n9.000557e+06\n0.151865\n0.503394\n\n\n4\nXLB\n2020\n5\n4.829185e+06\n0.068813\n0.277311",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#combining-metrics---multiple-aggregation",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#combining-metrics---multiple-aggregation",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.10 Combining Metrics - multiple aggregation",
    "text": "8.10 Combining Metrics - multiple aggregation\nAnother way to combine all our statistics into a single DataFrame is to supply all of our custom aggregation functions as arguments to the .agg() function at the same time.\nHere is what that looks like:\n\n# defining aggregations\nagg_funcs = \\\n    {'volume':[np.mean], 'dly_ret':[monthly_ret, monthly_vol]}\n\n# performing all aggregations all three aggregations at once\ndf_joined = \\\n    df_etf.groupby(['symbol', 'month', 'year']).agg(agg_funcs).reset_index()\n\n# looking at the data frame\ndf_joined.head()\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nvolume\ndly_ret\n\n\n\n\n\n\nmean\nmonthly_ret\nmonthly_vol\n\n\n\n\n0\nXLB\n1\n2020\n7.235429e+06\n-0.050577\n0.150336\n\n\n1\nXLB\n2\n2020\n1.058022e+07\n-0.085199\n0.282201\n\n\n2\nXLB\n3\n2020\n1.432920e+07\n-0.145675\n0.932265\n\n\n3\nXLB\n4\n2020\n9.000557e+06\n0.151865\n0.503394\n\n\n4\nXLB\n5\n2020\n4.829185e+06\n0.068813\n0.277311\n\n\n\n\n\n\n\nNotice that the input into the .agg() method is a dict whose elements are pairs that look like:\n'column_name':[list_of_aggregating_functions].\n\nCode Challenge: Modify the code above to add maximum daily volume to the report.\n\n\nSolution\n# defining aggregations\nagg_funcs = \\\n    {'volume':[np.mean, np.max], 'dly_ret':[monthly_ret, monthly_vol]}\n\n# performing all aggregations all three aggregations at once\ndf_joined = \\\n    df_etf.groupby(['symbol', 'month', 'year']).agg(agg_funcs).reset_index()\n\n# looking at the data frame\ndf_joined.head()\n\n\n\n\n\n\n\n\n\nsymbol\nmonth\nyear\nvolume\ndly_ret\n\n\n\n\n\n\nmean\namax\nmonthly_ret\nmonthly_vol\n\n\n\n\n0\nXLB\n1\n2020\n7.235429e+06\n20266900\n-0.050577\n0.150336\n\n\n1\nXLB\n2\n2020\n1.058022e+07\n30741700\n-0.085199\n0.282201\n\n\n2\nXLB\n3\n2020\n1.432920e+07\n28390200\n-0.145675\n0.932265\n\n\n3\nXLB\n4\n2020\n9.000557e+06\n30738000\n0.151865\n0.503394\n\n\n4\nXLB\n5\n2020\n4.829185e+06\n7386300\n0.068813\n0.277311",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#related-reading",
    "href": "chapters/08_groupby_aggregation_part_2/groupby_aggregation_part_2.html#related-reading",
    "title": "8  .groupby() and .agg() - 2",
    "section": "8.11 Related Reading",
    "text": "8.11 Related Reading\nPython Data Science Handbook (VanderPlas) - Section 3.7 - Combining Datasets: Merging and Joining\nPython Data Science Handbook (VanderPlas) - Section 3.8 - Aggregation and Grouping\nPython for Data Analysis (McKinney) - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations",
    "crumbs": [
      "Basic Data Wrangling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`.groupby()` and `.agg()` - 2</span>"
    ]
  },
  {
    "objectID": "parts/02_part_visualization/part_visualization.html",
    "href": "parts/02_part_visualization/part_visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Unlike R, in which a single package (ggplot2) has become the de facto standard for visualization, in Python there are a variety of packages that form a visualization ecosystem. The matplotlib package is the foundation of much of this ecosystem, and many elements of this ecosystem can be thought of as wrappers around matplotlib. Such packages include seaborn and also the visualization tools in pandas which we discuss here.",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html",
    "title": "9  Line Graphs with pandas",
    "section": "",
    "text": "9.1 Load Packages\nThe purpose of this chapter is to demonstrate the use of line graphs in finance. In particular, we use line graphs to plot various volatility-related time series. Our analysis objective is to observe the leverage effect: when stock markets suffer losses there is greater volatility. In order to do this we utilize the built-in visualization capabilities of pandas; these capabilities are built on top of the matplotlib package.\nThe API for matplotlib is fairly low level, which means you have to specify a lot of different things even for simply graphs. In practice this can be cumbersome, especially for quick and dirty plotting. Many constituents of the Python visualization ecosystem seek to simplify the creation of standard graphs, and this is true of the pandas visualization functionality.\nLet’s begin by loading the packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#ipython-magic-commands",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#ipython-magic-commands",
    "title": "9  Line Graphs with pandas",
    "section": "9.2 IPython Magic Commands",
    "text": "9.2 IPython Magic Commands\nNow that we are (implictly) using matplotlib, we will have occasion to use our first IPython magic command. These magic commands are often referred to as simply magics.\nThe following line of code tells Jupyter to print graphs as output just below the code cell - in the same way that other ouput is printed to the screen.\n\n%matplotlib inline\n\nMagics are convenience functions that IPython adds to base Python to make a variety of analysis tasks easier.\nAnother example is %timeit that performs time tests on code.\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n202 µs ± 351 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nYou can read more about magics in Section 1.3 of Python Data Science Handbook. The primary magic command that we will utilize is %matplotlib inline.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#reading-in-data",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#reading-in-data",
    "title": "9  Line Graphs with pandas",
    "section": "9.3 Reading In Data",
    "text": "9.3 Reading In Data\nLet’s now use pandas_datareader to read-in SPY and VIX data for 2016Q1 through 2021Q2.\n\ndf_spy = yf.download(\n    ['SPY', '^VIX'], start='2016-01-01', end='2021-06-30',\n    auto_adjust=False, rounding=True\n)\ndf_spy.head()\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nTicker\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n171.35\n20.70\n201.02\n20.70\n201.03\n23.36\n198.59\n20.67\n200.49\n22.48\n222353500\n0\n\n\n2016-01-05\n171.64\n19.34\n201.36\n19.34\n201.90\n21.06\n200.05\n19.25\n201.40\n20.75\n110845800\n0\n\n\n2016-01-06\n169.47\n20.59\n198.82\n20.59\n200.06\n21.86\n197.60\n19.80\n198.34\n21.67\n152112600\n0\n\n\n2016-01-07\n165.41\n24.99\n194.05\n24.99\n197.44\n25.86\n193.59\n22.40\n195.33\n23.22\n213436100\n0\n\n\n2016-01-08\n163.59\n27.01\n191.92\n27.01\n195.85\n27.08\n191.58\n22.48\n195.19\n22.96\n209817200\n0\n\n\n\n\n\n\n\nThe following code cleans up the data by isolating the the Close prices, resetting the index, and then changing the column names.\n\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename_axis(None, axis=1, inplace=True)\ndf_spy.rename(columns={'Date':'date','SPY':'spy','^VIX':'vix'}, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\n\n\n1\n2016-01-05\n201.36\n19.34\n\n\n2\n2016-01-06\n198.82\n20.59\n\n\n3\n2016-01-07\n194.05\n24.99\n\n\n4\n2016-01-08\n191.92\n27.01",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#creating-a-basic-price-plot",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#creating-a-basic-price-plot",
    "title": "9  Line Graphs with pandas",
    "section": "9.4 Creating a Basic Price Plot",
    "text": "9.4 Creating a Basic Price Plot\npandas was created by Wes McKinney when he was a quantitative analyst at the hedge fund called AQR. One of McKinney’s goals for pandas was to facilitate the analysis of financial time series. For example, plotting stock prices and returns over time can be done very easily.\nThe following single line of code produces a line graph consisting of the close prices of SPY over time.\n\ndf_spy.plot(x='date', y='spy');\n\n\n\n\n\n\n\n\nNote that .plot() is a DataFrame method.\nNow, for the purposes of exploratory data analysis (EDA), this plot may be all that we need. However, if we needed to share this graph in a publication or presentation, there are a variety of shortcomings that we would need address by utilizing various arguments of the .plot() method.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#improving-our-graph",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#improving-our-graph",
    "title": "9  Line Graphs with pandas",
    "section": "9.5 Improving Our Graph",
    "text": "9.5 Improving Our Graph\nAs a first improvement, let’s add a title to our graph, and add some grid lines to make it a little easier to read.\n\ndf_spy.\\\n    plot(\n        x = 'date',\n        y = 'spy',\n        title = 'SPY: 2016Q1-2021Q2',\n        grid = True,\n    );\n\n\n\n\n\n\n\n\nIn order to add custom labels to the x-axis and y-axis we will have to work with the matplotlib API. Don’t worry about the details too much right now, just copy this code if you need to relabel your axes.\n\nax = df_spy.\\\n        plot(\n            x = 'date',\n            y = 'spy',\n            title = 'SPY: 2016Q1-2021Q2',\n            grid = True,\n        );\nax.set_xlabel('Trade Date');\nax.set_ylabel('Close Price');\n\n\n\n\n\n\n\n\n\nCoding Challenge: Copy the code above and then see what the effect is of adding these arguments to .plot():\n1. figsize = (10, 5)\n2. style = 'k--'\n3. alpha = 0.5\n\n\nSolution\nax = df_spy.\\\n        plot(\n            x = 'date',\n            y = 'spy',\n            title = 'SPY: 2016Q1-2021Q2',\n            grid = True,\n            figsize = (10, 5),\n            style = 'k--',\n            alpha = 0.5,\n        );\nax.set_xlabel('Trade Date');\nax.set_ylabel('Close Price');\n\n\n\n\n\n\n\n\n\n\nFor the remainder of the chapter we will utilize graphs as we would for EDA, so we won’t concern ourselves with titles and labels.\nHowever, whenever graphs are being used to communicate results with a broader audience, they should be properly labeled.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#subplot-of-price-and-returns",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#subplot-of-price-and-returns",
    "title": "9  Line Graphs with pandas",
    "section": "9.6 Subplot of Price and Returns",
    "text": "9.6 Subplot of Price and Returns\nIn this section, we will plot prices and returns as subplots on the same x-axis. This dual plot will be our first observation of the leverage effect: when the stock market suffers losses there is greater volatility.\nLet’s begin by adding a return column to df_spy.\n\ndf_spy['return'] = df_spy['spy'] / df_spy['spy'].shift(1) - 1\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\n\n\n\n\n\n\n\nNow we can use the subplots argument of .plot() to plot both the prices and returns simulatneously.\n\ndf_spy.plot(x='date', y=['spy', 'return'], subplots=True, style='k', grid=True, alpha=0.75, figsize=(8, 8),);\n\n\n\n\n\n\n\n\nIt is easy to confirm visually that when the market goes down, the magnitude of the proximate returns is large, i.e. there is greater volatility. Similarly, during bull markets, returns tend to be smaller in magnitude.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#realized-volatility",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#realized-volatility",
    "title": "9  Line Graphs with pandas",
    "section": "9.7 Realized Volatility",
    "text": "9.7 Realized Volatility\nPlotting the realized volatility - i.e. the rolling standard deviation - is another way to observe the leverage effect.\nIn order to execute rolling calculations in pandas we will use the DataFrame.rolling() method.\n\npd.options.display.max_rows = 6\ndf_spy['return'].rolling(42).std() * np.sqrt(252)\n\n0            NaN\n1            NaN\n2            NaN\n          ...   \n1379    0.118643\n1380    0.118678\n1381    0.117790\nName: return, Length: 1382, dtype: float64\n\n\nNote that the argument of .rolling() is the window size, which we have set to two months.\nLet’s add this rolling realized volatility calculation to df_spy.\n\ndf_spy['realized_vol'] = df_spy['return'].rolling(42).std() * np.sqrt(252)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nrealized_vol\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\nNaN\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\nNaN\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\nNaN\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\nNaN\n\n\n\n\n\n\n\nAgain, we can use the subplot argument of .plot() to plot all three time series.\n\ndf_spy.plot(x='date', y=['spy', 'return', 'realized_vol',], subplots=True, style='k', grid=True, alpha=0.75, figsize=(8, 12));\n\n\n\n\n\n\n\n\nNotice that when there is a market downturn, there is a spike in the realized volatility graph.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#implied-volatility---the-vix-index",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#implied-volatility---the-vix-index",
    "title": "9  Line Graphs with pandas",
    "section": "9.8 Implied Volatility - The VIX Index",
    "text": "9.8 Implied Volatility - The VIX Index\nDuring times of market stress, options all become more expensive. One measure of the relative cheapness or expensiveness of options is implied volatility. When options become more expensive, implied volatility rises.\nOne of the complexities of implied volatility measurements is that even for a single underlying they differ depending on strike and expiration. However, all implied volatility measurements tend to rise and fall together.\nThe VIX index is a single number that summarizes the general level of option implied volatility for options on the S&P500. The S&P500 represents a large number of the most important stocks in America. Moreover, S&P500 options are the most actively traded options in the world. For these reasons, the VIX is a good barometer for overall implied volatility level in the stock market.\nLet’s plot vix along side the other volatility measures in the same graph.\n\ndf_spy.plot(x='date', y=['spy', 'return', 'realized_vol', 'vix',], subplots=True, style='k', grid=True, alpha=0.75, figsize=(8, 16));\n\n\n\n\n\n\n\n\nThis plot demonstrates typical behavior of stock market returns and volatility: when there is a market downturn, there is a spike in both implied volatility and realized volatility.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#further-reading",
    "href": "chapters/09_pandas_line_graphs/pandas_line_graphs.html#further-reading",
    "title": "9  Line Graphs with pandas",
    "section": "9.9 Further Reading",
    "text": "9.9 Further Reading\nPython Data Science Handbook (VanderPlas) - Section 1.3 - Python Magic Commands\nPython for Finance (Hilpisch) - Section 6.2 - Financial Data\nPython for Data Analysis (McKinney) - Section 8.1 - A Brief matplotlib API Primer\nPython for Data Analysis (McKinney) - Section 8.2 - Plotting Functions in pandas",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Line Graphs with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html",
    "title": "10  Bar Charts with pandas",
    "section": "",
    "text": "10.1 Load Packages\nThe purpose of this chapter is to demonstrate how to easily generate bar charts with the pandas built-in .plot() function.\nWe apply this technique to the task of visualizing monthly pnls for the data set in the spy_2018_call_pnl.csv data file.\nLet’s begin by loading the packages we need:\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nKnowledge Challenge: What is the purpose of this line of code in the above cell: %matplotlib inline?\nSolution\n# plotting graphs below code cells",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bar Charts with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#reading-in-data",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#reading-in-data",
    "title": "10  Bar Charts with pandas",
    "section": "10.2 Reading-In Data",
    "text": "10.2 Reading-In Data\nNext, let’s read in the data from the CSV file.\n\ndf_pnl = pd.read_csv('spy_2018_call_pnl.csv')\ndf_pnl.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\nbid\nask\nimplied_vol\ndelta\ndly_opt_pnl\ndly_dh_pnl\n\n\n\n\n0\nSPY\n266.529999\ncall\n2018-01-19\n2017-12-15\n270\n1.14\n1.16\n0.068257\n0.328344\n-0.02\n0.000000\n\n\n1\nSPY\n268.230011\ncall\n2018-01-19\n2017-12-18\n270\n1.68\n1.69\n0.071450\n0.421353\n-0.53\n0.558189\n\n\n2\nSPY\n267.250000\ncall\n2018-01-19\n2017-12-19\n270\n1.39\n1.41\n0.074841\n0.365808\n0.28\n-0.412931\n\n\n3\nSPY\n267.100006\ncall\n2018-01-19\n2017-12-20\n270\n1.10\n1.11\n0.070911\n0.327058\n0.30\n-0.054869\n\n\n4\nSPY\n267.540009\ncall\n2018-01-19\n2017-12-21\n270\n1.31\n1.32\n0.072183\n0.372113\n-0.21\n0.143906\n\n\n\n\n\n\n\nThis data consists of daily PNLs from 12 different SPY short call trades throughout 2018.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bar Charts with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#wrangling",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#wrangling",
    "title": "10  Bar Charts with pandas",
    "section": "10.3 Wrangling",
    "text": "10.3 Wrangling\nFirst, we will refactor the expiration and data_date columns to datetime using the pd.to_datetime() method.\n\ndf_pnl['expiration'] = pd.to_datetime(df_pnl['expiration'])\ndf_pnl['data_date'] = pd.to_datetime(df_pnl['data_date'])\ndf_pnl.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\nbid\nask\nimplied_vol\ndelta\ndly_opt_pnl\ndly_dh_pnl\n\n\n\n\n0\nSPY\n266.529999\ncall\n2018-01-19\n2017-12-15\n270\n1.14\n1.16\n0.068257\n0.328344\n-0.02\n0.000000\n\n\n1\nSPY\n268.230011\ncall\n2018-01-19\n2017-12-18\n270\n1.68\n1.69\n0.071450\n0.421353\n-0.53\n0.558189\n\n\n2\nSPY\n267.250000\ncall\n2018-01-19\n2017-12-19\n270\n1.39\n1.41\n0.074841\n0.365808\n0.28\n-0.412931\n\n\n3\nSPY\n267.100006\ncall\n2018-01-19\n2017-12-20\n270\n1.10\n1.11\n0.070911\n0.327058\n0.30\n-0.054869\n\n\n4\nSPY\n267.540009\ncall\n2018-01-19\n2017-12-21\n270\n1.31\n1.32\n0.072183\n0.372113\n-0.21\n0.143906\n\n\n\n\n\n\n\nWe are interested in total pnl, which is the sum of the option pnl and the delta-hedge PNL. Let’s add a column called dly_tot_pnl which captures this logic.\n\ndf_pnl['dly_tot_pnl'] = df_pnl['dly_opt_pnl'] + df_pnl['dly_dh_pnl']\ndf_pnl.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\nbid\nask\nimplied_vol\ndelta\ndly_opt_pnl\ndly_dh_pnl\ndly_tot_pnl\n\n\n\n\n0\nSPY\n266.529999\ncall\n2018-01-19\n2017-12-15\n270\n1.14\n1.16\n0.068257\n0.328344\n-0.02\n0.000000\n-0.020000\n\n\n1\nSPY\n268.230011\ncall\n2018-01-19\n2017-12-18\n270\n1.68\n1.69\n0.071450\n0.421353\n-0.53\n0.558189\n0.028189\n\n\n2\nSPY\n267.250000\ncall\n2018-01-19\n2017-12-19\n270\n1.39\n1.41\n0.074841\n0.365808\n0.28\n-0.412931\n-0.132931\n\n\n3\nSPY\n267.100006\ncall\n2018-01-19\n2017-12-20\n270\n1.10\n1.11\n0.070911\n0.327058\n0.30\n-0.054869\n0.245131\n\n\n4\nSPY\n267.540009\ncall\n2018-01-19\n2017-12-21\n270\n1.31\n1.32\n0.072183\n0.372113\n-0.21\n0.143906\n-0.066094\n\n\n\n\n\n\n\nAs the final step of our wrangling, let’s extract the year and month of the expiration, as this is what we will use for grouping.\n\ndf_pnl['year'] = df_pnl['expiration'].dt.year\ndf_pnl['month'] = df_pnl['expiration'].dt.month\ndf_pnl.head()\n\n\n\n\n\n\n\n\nunderlying\nupx\ntype\nexpiration\ndata_date\nstrike\nbid\nask\nimplied_vol\ndelta\ndly_opt_pnl\ndly_dh_pnl\ndly_tot_pnl\nyear\nmonth\n\n\n\n\n0\nSPY\n266.529999\ncall\n2018-01-19\n2017-12-15\n270\n1.14\n1.16\n0.068257\n0.328344\n-0.02\n0.000000\n-0.020000\n2018\n1\n\n\n1\nSPY\n268.230011\ncall\n2018-01-19\n2017-12-18\n270\n1.68\n1.69\n0.071450\n0.421353\n-0.53\n0.558189\n0.028189\n2018\n1\n\n\n2\nSPY\n267.250000\ncall\n2018-01-19\n2017-12-19\n270\n1.39\n1.41\n0.074841\n0.365808\n0.28\n-0.412931\n-0.132931\n2018\n1\n\n\n3\nSPY\n267.100006\ncall\n2018-01-19\n2017-12-20\n270\n1.10\n1.11\n0.070911\n0.327058\n0.30\n-0.054869\n0.245131\n2018\n1\n\n\n4\nSPY\n267.540009\ncall\n2018-01-19\n2017-12-21\n270\n1.31\n1.32\n0.072183\n0.372113\n-0.21\n0.143906\n-0.066094\n2018\n1",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bar Charts with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#groupby-and-agg",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#groupby-and-agg",
    "title": "10  Bar Charts with pandas",
    "section": "10.4 groupby() and agg()",
    "text": "10.4 groupby() and agg()\nWe are interested in graphing the PNLs by expiration, so let’s sum up the dly_tot_pnl by the year and month of the expiration.\n\ndf_monthly = \\\n    df_pnl.groupby(['year', 'month'])['dly_tot_pnl'].agg([np.sum]).reset_index()\ndf_monthly.head()\n\n\n\n\n\n\n\n\nyear\nmonth\nsum\n\n\n\n\n0\n2018\n1\n0.091963\n\n\n1\n2018\n2\n-2.759090\n\n\n2\n2018\n3\n-0.340270\n\n\n3\n2018\n4\n-1.174222\n\n\n4\n2018\n5\n1.487206\n\n\n\n\n\n\n\nBefore we proceed to graphing, let’s change the name of the aggregated pnl column to something more meaningful.\n\ndf_monthly.rename(columns={'sum':'monthly_pnl'}, inplace=True)",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bar Charts with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#visualizing-the-data",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#visualizing-the-data",
    "title": "10  Bar Charts with pandas",
    "section": "10.5 Visualizing the Data",
    "text": "10.5 Visualizing the Data\nCreating a simple bar graph of the monthly_pnls in df_monthly can be done easily with a single line of code.\n\ndf_monthly.plot(x='month', y='monthly_pnl', kind='bar');\n\n\n\n\n\n\n\n\nWhile the above graph may be fine for EDA purposes, it still leaves much to be desired, especially if our intention is to share it with a broader audience.\nThe following code makes several of modifications to improve its appearance.\n\nax = \\\n    df_monthly.\\\n        plot(\n            x = 'month',\n            y = 'monthly_pnl',\n            kind='bar',\n            color='k', # color is grey\n            grid=True , # adding a grid\n            alpha=0.75, # translucence\n            width=0.8, # increasing the width of the bars\n            title='Monthly PNL for SPY Calls',\n            figsize=(8, 4), # modifying the figure size\n        );\n\nax.set_xlabel(\"Month\"); # x axis label\nax.set_ylabel(\"PNL\");   # y axis label\n\n\n\n\n\n\n\n\n\nCode Challenge: Google and try to find how you create a horizontal bar graph using pandas.\n\n\nSolution\nax = \\\n    df_monthly.\\\n        plot(\n            x = 'month',\n            y = 'monthly_pnl',\n            kind='barh', # changed to barh\n            color='k', # color is grey\n            grid=True , # adding a grid\n            alpha=0.75, # translucence\n            width=0.8, # increasing the width of the bars\n            title='Monthly PNL for SPY Calls',\n            figsize=(8, 4), # modifying the figure size\n        );\n\nax.set_xlabel(\"Month\"); # x axis label\nax.set_ylabel(\"PNL\");",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bar Charts with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#a-few-words-about-visualization",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#a-few-words-about-visualization",
    "title": "10  Bar Charts with pandas",
    "section": "10.6 A Few Words About Visualization",
    "text": "10.6 A Few Words About Visualization\nVisualizing data can be an effective way of communicating results to others, or exploring data on your own. The benefit of visualization comes into focus when we can convey a particular result more quickly and more viscerally with a graph rather than a table of numbers.\nThis is nicely illustrated by comparing our bar graph to the original DataFrame of data. Consider the following question:\nWhat were the two worst PNL months for these SPY calls?\nDo you find it easier to answer the question using the bar graph or the table? Explain why.\n\nax = \\\n    df_monthly.\\\n        plot(\n            x = 'month',\n            y = 'monthly_pnl',\n            kind = 'bar', \n            color='k', # color is grey\n            grid=True, # adding a grid\n            alpha=0.75, # translucence\n            width=0.8, # increasing the width of the bars\n            title='Monthly PNL for SPY Calls',\n            figsize=(8, 4), # modifying the figure size\n        );\n\nax.set_xlabel(\"Month\"); # x axis label\nax.set_ylabel(\"PNL\");   # y axis label\n\n\n\n\n\n\n\n\n\ndf_monthly\n\n\n\n\n\n\n\n\nyear\nmonth\nmonthly_pnl\n\n\n\n\n0\n2018\n1\n0.091963\n\n\n1\n2018\n2\n-2.759090\n\n\n2\n2018\n3\n-0.340270\n\n\n3\n2018\n4\n-1.174222\n\n\n4\n2018\n5\n1.487206\n\n\n5\n2018\n6\n0.644469\n\n\n6\n2018\n7\n0.516556\n\n\n7\n2018\n8\n0.195526\n\n\n8\n2018\n9\n0.753701\n\n\n9\n2018\n10\n-0.133537\n\n\n10\n2018\n11\n-0.979537\n\n\n11\n2018\n12\n-2.085526",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bar Charts with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#related-reading",
    "href": "chapters/10_pandas_bar_charts/pandas_bar_charts.html#related-reading",
    "title": "10  Bar Charts with pandas",
    "section": "10.7 Related Reading",
    "text": "10.7 Related Reading\nPython for Data Analysis (McKinney) - Section 8.2 - Plotting Functions in pandas",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bar Charts with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html",
    "href": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html",
    "title": "11  Scatter Plots with pandas",
    "section": "",
    "text": "11.1 Loading Packages\nThe purpose of this chapter is to demonstrate the pandas built-in functionality for creating scatter plots.\nThe financial task we will accomplish is demonstrating SPY’s implied leverage effect: when the market suffers losses, implied volatility increases; when the market experiences gains, implied volatility decreases.\nOur measure of SPY implied volatility will be the VIX index. To verify the above relationship, we will plot SPY daily returns against daily changes in the VIX for 2016Q1-2021Q2.\nLet’s begin by loading the packages we will need.\nimport pandas as pd\nimport yfinance as yf\n%matplotlib inline",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scatter Plots with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#reading-in-data",
    "href": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#reading-in-data",
    "title": "11  Scatter Plots with pandas",
    "section": "11.2 Reading-In Data",
    "text": "11.2 Reading-In Data\nNext, let’s use pandas_datareader to read in the SPY and VIX data.\n\ndf_spy = yf.download(\n    ['SPY', '^VIX'], start='2016-01-01', end='2021-06-30',\n    auto_adjust=False, rounding=True\n)\ndf_spy.head()\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nTicker\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\nSPY\n^VIX\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n171.35\n20.70\n201.02\n20.70\n201.03\n23.36\n198.59\n20.67\n200.49\n22.48\n222353500\n0\n\n\n2016-01-05\n171.64\n19.34\n201.36\n19.34\n201.90\n21.06\n200.05\n19.25\n201.40\n20.75\n110845800\n0\n\n\n2016-01-06\n169.47\n20.59\n198.82\n20.59\n200.06\n21.86\n197.60\n19.80\n198.34\n21.67\n152112600\n0\n\n\n2016-01-07\n165.41\n24.99\n194.05\n24.99\n197.44\n25.86\n193.59\n22.40\n195.33\n23.22\n213436100\n0\n\n\n2016-01-08\n163.59\n27.01\n191.92\n27.01\n195.85\n27.08\n191.58\n22.48\n195.19\n22.96\n209817200\n0\n\n\n\n\n\n\n\nThe following code cleans up the data by isolating the the Close prices, resetting the index, and then changing the column names\n\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename_axis(None, axis=1, inplace=True)\ndf_spy.rename(columns={'Date':'date','SPY':'spy','^VIX':'vix'}, inplace=True)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\n\n\n1\n2016-01-05\n201.36\n19.34\n\n\n2\n2016-01-06\n198.82\n20.59\n\n\n3\n2016-01-07\n194.05\n24.99\n\n\n4\n2016-01-08\n191.92\n27.01",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scatter Plots with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#adding-returns-and-vix-changes-to-df_spy",
    "href": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#adding-returns-and-vix-changes-to-df_spy",
    "title": "11  Scatter Plots with pandas",
    "section": "11.3 Adding Returns and VIX Changes to df_spy",
    "text": "11.3 Adding Returns and VIX Changes to df_spy\nLet’s add a return column to df_spy.\n\ndf_spy['return'] = df_spy['spy'] / df_spy['spy'].shift(1) - 1\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\n\n\n\n\n\n\n\nNext, let’s calculate the daily change in the VIX, and put it in a new column called vix_chg.\n\ndf_spy['vix_chng'] = df_spy['vix'].diff()\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nvix_chng\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\n-1.36\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\n1.25\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\n4.40\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\n2.02\n\n\n\n\n\n\n\nThe return column in df_spy is expressed as a decimal, so let’s change the vix and vix_chng columns of df_vix to also be expressed as decimals.\n\ndf_spy['vix'] = df_spy['vix'] / 100\ndf_spy['vix_chng'] = df_spy['vix_chng'] / 100\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nvix_chng\n\n\n\n\n0\n2016-01-04\n201.02\n0.2070\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n0.1934\n0.001691\n-0.0136\n\n\n2\n2016-01-06\n198.82\n0.2059\n-0.012614\n0.0125\n\n\n3\n2016-01-07\n194.05\n0.2499\n-0.023992\n0.0440\n\n\n4\n2016-01-08\n191.92\n0.2701\n-0.010977\n0.0202",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scatter Plots with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#scatter-plot",
    "href": "chapters/11_pandas_scatter_plots/pandas_scatter_plots.html#scatter-plot",
    "title": "11  Scatter Plots with pandas",
    "section": "11.4 Scatter Plot",
    "text": "11.4 Scatter Plot\nNow that we have our data wrangled, we are in position to use the DataFrame.plot.scatter() method to plot daily SPY return against daily changes in the VIX.\n\ndf_spy.plot.scatter('return', 'vix_chng');\n\n\n\n\n\n\n\n\nThe following code improves the aesthetics of our plot:\n\ndf_spy.plot.scatter(\n    x = 'return',\n    y = 'vix_chng',\n    grid=True ,  \n    c='k',\n    alpha=0.75,\n    s=10,  # changing the size of the dots\n    figsize=(8, 6),\n    title='SPY Return vs VIX Changes (2016Q1-2021Q2: daily)',\n);",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scatter Plots with **pandas**</span>"
    ]
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html",
    "href": "chapters/12_seaborn/seaborn.html",
    "title": "12  Visualization with seaborn",
    "section": "",
    "text": "12.1 Loading Packages\nIn the previous visualization we used the built-in plotting capabilities of pandas to create some useful financial graphs. In this tutorial, we recreate those plots using the seaborn visualization package.\nLike the pandas visualization functionality, seaborn is built on top of the matplotlib package. As previously discussed, matplotlib allows for low level control over visualizations, which makes it very flexible. However, this flexibility comes at the cost of complexity. In contrast, seaborn provides a high level interface that allows for easy implementations of attractive graphs.\nThe focus of seaborn is general statistical data visualizations, whereas pandas is more tailor-made for financial time series.\nThis tutorial is not meant to be a comprehensive introduction to seaborn. Rather, my intention is to simply show you the code for a few graphs types that I have found useful in finance. For a thorough introduction to seaborn, I recommend working through the official tutorials that are linked in the Further Reading section - they are extremely well done, but quite long.\nLet’s load the packages that we will be using.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualization with **seaborn**</span>"
    ]
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#line-graph---prices-returns-realized-vol-vix",
    "href": "chapters/12_seaborn/seaborn.html#line-graph---prices-returns-realized-vol-vix",
    "title": "12  Visualization with seaborn",
    "section": "12.2 Line Graph - Prices, Returns, Realized Vol, VIX",
    "text": "12.2 Line Graph - Prices, Returns, Realized Vol, VIX\nLet’s read-in and wrangle some data for SPY and VIX during 2016Q1-2021Q2.\n\ndf_spy = yf.download(\n    ['SPY', '^VIX'], start='2016-01-01', end='2021-06-30',\n    auto_adjust=False, rounding=True,\n)\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename_axis(None, axis=1, inplace=True)\ndf_spy.rename(columns={'Date':'date','SPY':'spy','^VIX':'vix'}, inplace=True)\ndf_spy.head()\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\ndate\nspy\nvix\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\n\n\n1\n2016-01-05\n201.36\n19.34\n\n\n2\n2016-01-06\n198.82\n20.59\n\n\n3\n2016-01-07\n194.05\n24.99\n\n\n4\n2016-01-08\n191.92\n27.01\n\n\n\n\n\n\n\nNext, let’s add returns and realized_vol to the df_spy.\n\ndf_spy['return'] = df_spy['spy'] / df_spy['spy'].shift(1) - 1\ndf_spy['realized_vol'] = df_spy['return'].rolling(42).std() * np.sqrt(252)\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nrealized_vol\n\n\n\n\n0\n2016-01-04\n201.02\n20.70\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n19.34\n0.001691\nNaN\n\n\n2\n2016-01-06\n198.82\n20.59\n-0.012614\nNaN\n\n\n3\n2016-01-07\n194.05\n24.99\n-0.023992\nNaN\n\n\n4\n2016-01-08\n191.92\n27.01\n-0.010977\nNaN\n\n\n\n\n\n\n\n\n12.2.1 Graphing with pandas\nRecall that pandas allows us to quickly graph these four time-series in a single figure.\n\ndf_spy. \\\n    plot(\n        x = 'date', \n        y = ['spy', 'return', 'realized_vol', 'vix',],\n        subplots = True,\n        figsize=(8, 12),\n        title='SPY 2016Q1-2021Q2',\n    );\nplt.subplots_adjust(top=0.96); # this adjusts the location of the title\n\n\n\n\n\n\n\n\n\n\n12.2.2 Graphing with seaborn\nLet’s create similar graphs with seaborn. Creating all four plots in a single graph is not as easy with seaborn and the code is a little confusing. To keep things simple we will recreate two of the plots separately.\nHere is the code that generates the graph of the daily prices.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(x='date', y='spy', kind='line', data=df_spy, aspect=1.5)\n    g.fig.autofmt_xdate()\n    # creating and tweaking the title\n    g.fig.suptitle('SPY Close Price: 2016Q1-2021Q2')\n    plt.subplots_adjust(top=0.93);\n\n\n\n\n\n\n\n\nAnd here is the code produces the graph of the daily returns.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(x='date', y='return', kind='line', data=df_spy, aspect=1.5)\n    g.fig.autofmt_xdate()\n    # creating and tweaking the title\n    g.fig.suptitle('SPY Close Price: 2016Q1-2021Q2');\n    plt.subplots_adjust(top=0.93);",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualization with **seaborn**</span>"
    ]
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#bar-graph---monthly-spy-call-pnls",
    "href": "chapters/12_seaborn/seaborn.html#bar-graph---monthly-spy-call-pnls",
    "title": "12  Visualization with seaborn",
    "section": "12.3 Bar Graph - Monthly SPY Call PNLs",
    "text": "12.3 Bar Graph - Monthly SPY Call PNLs\nOur next data set consists of monthly pnls from the call trades detailed in seaborn_monthly_pnl_bar.csv.\n\ndf_monthly_bar = pd.read_csv('seaborn_monthly_pnl_bar.csv')\ndf_monthly_bar.head()\n\n\n\n\n\n\n\n\nmonth\nyear\nmonthly_pnl\n\n\n\n\n0\n1\n2018\n0.091963\n\n\n1\n2\n2018\n-2.759090\n\n\n2\n3\n2018\n-0.340270\n\n\n3\n4\n2018\n-1.174222\n\n\n4\n5\n2018\n1.487206\n\n\n\n\n\n\n\n\n12.3.1 Graphing with pandas\nRecall that this code creates the barplot of the pnls by month using pandas.\n\nax = \\\n    (df_monthly_bar\n        .plot(\n            x = 'month',\n            y = ['monthly_pnl'],\n            kind ='bar',\n            color='k', # color is grey\n            grid=True, # adding a grid\n            alpha=0.75, # translucence\n            width=0.8, # increasing the width of the bars\n            title='Monthly PNL for SPY Calls',\n            figsize=(8, 5), # modifying the figure size\n        ));\n\nax.set_xlabel(\"Month\"); # x-axis label\nax.set_ylabel(\"PNL\");   # y-axis label\n\n\n\n\n\n\n\n\n\n\n12.3.2 Graphing with seaborn\nHere is the code that produces a similar graph in using seaborn.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.catplot(\n        x='month'\n        , y='monthly_pnl'\n        , kind='bar'\n        , color='black'\n        , alpha=0.75\n        , height=5\n        , aspect = 1.5\n        , data=df_monthly_bar\n    );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('Monthly PNL for SPY Calls');",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualization with **seaborn**</span>"
    ]
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#scatter-plot---spy-returns-vs-vix-change-implied-leverage",
    "href": "chapters/12_seaborn/seaborn.html#scatter-plot---spy-returns-vs-vix-change-implied-leverage",
    "title": "12  Visualization with seaborn",
    "section": "12.4 Scatter Plot - SPY Returns vs VIX Change (implied leverage)",
    "text": "12.4 Scatter Plot - SPY Returns vs VIX Change (implied leverage)\nLet’s add vix_chng to df_spy and change the units to decimals.\n\ndf_spy['vix_chng'] = df_spy['vix'].diff()\ndf_spy['vix'] = df_spy['vix'] / 100\ndf_spy['vix_chng'] = df_spy['vix_chng'] / 100\ndf_spy.head()\n\n\n\n\n\n\n\n\ndate\nspy\nvix\nreturn\nrealized_vol\nvix_chng\n\n\n\n\n0\n2016-01-04\n201.02\n0.2070\nNaN\nNaN\nNaN\n\n\n1\n2016-01-05\n201.36\n0.1934\n0.001691\nNaN\n-0.0136\n\n\n2\n2016-01-06\n198.82\n0.2059\n-0.012614\nNaN\n0.0125\n\n\n3\n2016-01-07\n194.05\n0.2499\n-0.023992\nNaN\n0.0440\n\n\n4\n2016-01-08\n191.92\n0.2701\n-0.010977\nNaN\n0.0202\n\n\n\n\n\n\n\n\n12.4.1 Graphing with pandas\nHere is the pandas code that creates scatter plot of returns vs VIX changes.\n\ndf_spy.plot.scatter(\n    x = 'return',\n    y = 'vix_chng',\n    grid=True ,  \n    c='k',\n    alpha=0.75,\n    s=10, # changing the size of the dots\n    figsize=(7, 5),\n    title='SPY Return vs VIX Changes: 2016Q1-2021Q2',\n);\n\n\n\n\n\n\n\n\n\n\n12.4.2 Graphing with seaborn\nHere is the code for a similar graph using seaborn.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(\n            x = 'return',\n            y = 'vix_chng',\n            data = df_spy,\n            color = 'black',\n            alpha = 0.75,\n            height = 5.5,\n            aspect = 1.3,\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('SPY Return vs VIX Changes: 2016Q1-2021Q2)');",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualization with **seaborn**</span>"
    ]
  },
  {
    "objectID": "chapters/12_seaborn/seaborn.html#further-reading",
    "href": "chapters/12_seaborn/seaborn.html#further-reading",
    "title": "12  Visualization with seaborn",
    "section": "12.5 Further Reading",
    "text": "12.5 Further Reading\nPython Data Science Handbook - 4.14 - Visualization with Seaborn\nSeaborn Official Tutorials - https://seaborn.pydata.org/tutorial.html (very good, but long)",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Visualization with **seaborn**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html",
    "title": "13  Basic Plotting with bokeh",
    "section": "",
    "text": "13.1 Data\nIn this notebook we create some basic interactive plots with the bokeh package. These plots come with a fair amount of interactivity out of the box. In a subsequent chapter we will add more advanced interactivity.\nLet’s begin by loading the data that we will need for our visualizations.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\n\n\n# SPY Close, Returns, Realized Volatility, and VIX\ndf_spy = yf.download(\n    ['SPY', '^VIX'], start='2016-01-01', end='2021-06-30',\n    auto_adjust=False, rounding=True,\n)\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename_axis(None, axis=1, inplace=True)\ndf_spy.rename(columns={'Date':'date','SPY':'close','^VIX':'vix'}, inplace=True)\ndf_spy['return'] = np.log(df_spy['close'] / df_spy['close'].shift(1))\ndf_spy['realized_vol'] = df_spy['return'].rolling(42).std() * np.sqrt(252)\n\n# SPY Monthly Returns\ndf_spy['year'] = df_spy['date'].dt.year\ndf_spy['month'] = df_spy['date'].dt.month\ndf_monthly = df_spy.groupby(['year', 'month'], as_index=False)[['return']].sum()\ndf_monthly['year_month'] = (df_monthly['year'] * 100) + df_monthly['month']\ndf_monthly['year_month'] = df_monthly['year_month'].astype(str)\n\n# Implied Leverage Effect\ndf_spy['vix_change'] = df_spy['vix'].diff()\ndf_spy['vix'] = df_spy['vix'] / 100\ndf_spy['vix_change'] = df_spy['vix_change'] / 100\n\n# Asset Allocation - hypothetical allocation through time\ndf_asset_allocation = pd.read_csv('asset_allocation.csv', parse_dates=['trade_date'])\n\n[*********************100%***********************]  2 of 2 completed",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#two-interfaces",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#two-interfaces",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.2 Two Interfaces",
    "text": "13.2 Two Interfaces\nThere are two interfaces for working with bokeh:\nbokeh.models: a low level interface that gives you complete control over how bokeh creates all elements of your visualization.\nbokeh.plotting: a high level, general-purpose interface that is similar to plotting interfaces of libraries such as Matplotlib or Matlab. It automatically assembles plots with default elements such as axes, grids, and tools for you.\nOur focus will be on the bokeh.plotting interface.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#blank-graph",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#blank-graph",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.3 Blank Graph",
    "text": "13.3 Blank Graph\nLet’s begin by creating a blank graph, and examine the elements of the code:\nbokeh.io.output_notebook() - output the graph inline in the notebook.\nbokeh.plotting.figure() - creates the basic plot object, called a Figure model.\nbokeh.plotting.show() - outputs the Figure model.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, show\n\n# output inline\noutput_notebook()\n\n# create figure() object with a title\nfig = figure(title='Blank Figure')\n\n# output to notebook\nshow(fig)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\nWARNING:bokeh.core.validation.check:W-1000 (MISSING_RENDERERS): Plot has no renderers: figure(id='p1001', ...)\n\n\n\n  \n\n\n\n\n\nNotice the toolbar on the right of the figure above which has a variety of interactive tools. These aren’t that interesting at the moment because our graph is blank.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#basic-line-plot",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#basic-line-plot",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.4 Basic Line Plot",
    "text": "13.4 Basic Line Plot\nLet’s create our first proper graph by adding a line glyph that plots the SPY close prices.\nNotice that we also modify our Figure object by adding arguments to the figure() function.\nNow the interactive tools are a bit more interesting.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\n# output inline\noutput_notebook()\n\n# create figure() object with a title\np = figure(width=600, height=400, x_axis_type='datetime', title='SPY Close Price',\n           x_axis_label='date', y_axis_label='close price')\n\n# adding a line glyph to display close prices by passing data Series directly\np.line(x=df_spy['date'], y=df_spy['close'])\n\n# output to notebook\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#columndatasource",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#columndatasource",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.5 ColumnDataSource",
    "text": "13.5 ColumnDataSource\nIn the example above, we passed our data to be graphed directly into the p.line() glyph by specifying particular columns of df_spy, which is a pandas.DataFrame. If you recall, the columns of a DataFrame are pandas.Series objects. We can also pass lists and numpy.arrays directly into a glyph.\nHowever, it is more fruitful to use a ColumnDataSource object. We can easily create one from a DataFrame called df with the following syntax: ColumnDataSource(df).\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\n\n# output inline\noutput_notebook()\n\n# creating ColumnDataSource from DataFrame\ncds = ColumnDataSource(df_spy)\n\n# creating figure() object\np = figure(width=600, height=400, x_axis_type='datetime', title='SPY Close Price',\n           x_axis_label='date', y_axis_label='close price')\n\n# adding a line glyph to display close prices, passing data via ColumnDataSource\np.line(x='date', y='close',source=cds)\n\n# output to notebook\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#basic-scatter",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#basic-scatter",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.6 Basic Scatter",
    "text": "13.6 Basic Scatter\nNext, let’s create a basic scatter plot using the .circle() glyph. Notice that we have reformatted the x-axis and y-axis to be expressed as percents.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\n\n# output inline\noutput_notebook()\n\n# creating a ColumnDataSource from DataFrame\ncds = ColumnDataSource(df_spy)\n\n# creating figure() object with a title\n# p = figure(plot_width=500, plot_height=500, title='SPY Implied Leverage Effect',\n#            x_axis_label='return', y_axis_label='vix change')\np = figure(width=500, height=500, title='SPY Implied Leverage Effect',\n           x_axis_label='return', y_axis_label='vix change')\n\n# adding circle glyph, passing data via ColumnDataSource\np.circle('return', 'vix_change', source=cds)\n\n# formatting the x-axis and y-axis to percents\np.xaxis.formatter = NumeralTickFormatter(format='0%') \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# output to notebook           \nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#other-glyphs",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#other-glyphs",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.7 Other Glyphs",
    "text": "13.7 Other Glyphs\nWe can recreate this scatter plot with a larger hexes instead of circles. We also modified the outline color and fill color, both of which were affected by arguments to p.hex().\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\n\n# outpute inline\noutput_notebook()\n\n# creating ColumnDataSource from DataFrame\ncds = ColumnDataSource(df_spy)\n\n# creating figure() object\np = figure(width=600, height=400, title='SPY Implied Leverage Effect',\n           x_axis_label='return', y_axis_label='vix change')\n\n# adding hex glyph, while changing some visual paramenters\np.hex('return', 'vix_change', line_color=\"navy\", fill_color=\"orange\", size=15, source=cds)\n\n# formatting the x-axis and y-axis to percents\np.xaxis.formatter = NumeralTickFormatter(format='0%') \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n  \n\n\n\n\n\n\n13.7.1 Glyphs Gallery\nThere are a wide variety of glyphs that can be used for scatter plots. They are demonstrated below in a visual that I borrowed from the bokeh documentation.\n\nfrom numpy.random import random\nfrom bokeh.io import output_notebook\nfrom bokeh.core.enums import MarkerType\nfrom bokeh.plotting import figure, show\n\n\noutput_notebook()\n\np = figure(title=\"Bokeh Markers\", toolbar_location=None)\np.grid.grid_line_color = None\np.background_fill_color = \"#eeeeee\"\np.axis.visible = False\np.y_range.flipped = True\n\nN = 10\n\nfor i, marker in enumerate(MarkerType):\n    x = i % 4\n    y = (i // 4) * 4 + 1\n\n    p.scatter(random(N)+2*x, random(N)+y, marker=marker, size=14,\n              line_color=\"navy\", fill_color=\"orange\", alpha=0.5)\n\n    p.text(2*x+0.5, y+2.5, text=[marker],\n           text_color=\"firebrick\", text_align=\"center\", text_font_size=\"13px\")\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#barchart-with-monthly-returns",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#barchart-with-monthly-returns",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.8 Barchart with Monthly Returns",
    "text": "13.8 Barchart with Monthly Returns\nHere is a barchart of SPY monthly returns using the p.vbar() renderer.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\nfrom bokeh.plotting import figure, show\nfrom bokeh.plotting import reset_output\nimport math\n\n# output inline\noutput_notebook()\n\n# creating ColumnDataSource from DataFrame\nsource = ColumnDataSource(df_monthly)\n\n# initializing the figure\np = figure(width=1000, height=400, x_range=df_monthly['year_month'], title='SPY Monthly Returns 2016Q1 to 2021Q3',\n           x_axis_label='month', y_axis_label='monthly return')\n\n# adding vbar glyphs, passing data with ColumnDataSource\np.vbar(x='year_month', bottom=0, top='return', color='blue', width=0.75, source=source)\n\n# formatting the y-axis to percents\np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# rotating x-axis labels\np.xaxis.major_label_orientation = math.pi/4\n\n# output graph\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#stacked-areas-with-varying-asset-allocations",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#stacked-areas-with-varying-asset-allocations",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.9 Stacked Areas with Varying Asset Allocations",
    "text": "13.9 Stacked Areas with Varying Asset Allocations\nNext, we create a stacked area chart to visualize our hypotheticl asset allocation that varies through time. In order to get a useful set of colors, we need to use a palette from the bokeh.palettes module.\n\n# import bokeh functions\nimport bokeh\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.io import output_notebook\nimport pandas as pd\n\n# output inline\noutput_notebook()\n\n# putting data into ColumnDataSource object\nsource = ColumnDataSource(df_asset_allocation)\n\n# initializing the figure\np = figure(width=900, height=500, x_axis_type='datetime', title='Asset Allocation 2019-2020',\n           x_axis_label='month', y_axis_label='allocation')\n\n# choosing assets and colors to graph\nassets = df_asset_allocation.drop(columns='trade_date').columns\nnum_assets = len(assets)\ncolors = bokeh.palettes.magma(num_assets) # choosing color palette\n\n# adding glyph with legend\np.varea_stack(assets, x='trade_date', color=colors, source=source, legend_label=assets.to_list())\n\n# reversing order of legend to match order of graph\np.legend[0].items.reverse()\n\n# reformatting y-axis as percent \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# moving legend off of the graph\np.add_layout(p.legend[0], 'right')\n\n# output graph\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/13_bokeh_basic/bokeh_basic.html#layouts",
    "href": "chapters/13_bokeh_basic/bokeh_basic.html#layouts",
    "title": "13  Basic Plotting with bokeh",
    "section": "13.10 Layouts",
    "text": "13.10 Layouts\nLayout functions let you build a grid of plots and widgets. You can have as many rows, columns, or grids of plots in one layout as you like.\nWe will use the column() function to vertically stack our leverage effect line graphs. Notice that each graph has its own set of interactive tools because they are not linked together. We will remedy this in the next notebook.\n\n# import bokeh functions\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\n\n# output inline\noutput_notebook()\n\n# defining multiple plots\np1 = figure(width=600, height=200, x_axis_type='datetime', title='SPY Leverage Effecct')\np1.line(x=df_spy['date'], y=df_spy['close'])\n\np2 = figure(width=600, height=200, x_axis_type='datetime')\np2.line(x=df_spy['date'], y=df_spy['return'])\np2.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np3 = figure(width=600, height=200, x_axis_type='datetime')\np3.line(x=df_spy['date'], y=df_spy['realized_vol'])\np3.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np4 = figure(width=600, height=200, x_axis_type='datetime')\np4.line(x=df_spy['date'], y=df_spy['vix'])\np4.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# putting them together using column()\nshow(column(p1, p2, p3, p4))\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html",
    "title": "14  Advanced Plotting with bokeh",
    "section": "",
    "text": "14.1 Data\nIn this chapter we will build on our work from a previous chapter and add a variety of interactions to the plots that we have created with the bokeh package.\nLet’s begin by loading the data that we will need for our visualizations.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\n\n\n# SPY Close, Returns, Realized Volatility, and VIX\ndf_spy = yf.download(\n    ['SPY', '^VIX'], start='2016-01-01', end='2021-06-30',\n    auto_adjust=False, rounding=True,\n)\ndf_spy = df_spy['Close'].reset_index()\ndf_spy.rename_axis(None, axis=1, inplace=True)\ndf_spy.rename(columns={'Date':'date','SPY':'close','^VIX':'vix'}, inplace=True)\ndf_spy['return'] = np.log(df_spy['close'] / df_spy['close'].shift(1))\ndf_spy['realized_vol'] = df_spy['return'].rolling(42).std() * np.sqrt(252)\n\n# SPY Monthly Returns\ndf_spy['year'] = df_spy['date'].dt.year\ndf_spy['month'] = df_spy['date'].dt.month\ndf_monthly = df_spy.groupby(['year', 'month'], as_index=False)[['return']].sum()\ndf_monthly['year_month'] = (df_monthly['year'] * 100) + df_monthly['month']\ndf_monthly['year_month'] = df_monthly['year_month'].astype(str)\n\n# Implied Leverage Effect\ndf_spy['vix_change'] = df_spy['vix'].diff()\ndf_spy['vix'] = df_spy['vix'] / 100\ndf_spy['vix_change'] = df_spy['vix_change'] / 100\n\n# Asset Allocation - hypothetical allocation through time\ndf_asset_allocation = pd.read_csv('asset_allocation.csv', parse_dates=['trade_date'])\n\n[*********************100%***********************]  2 of 2 completed",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Advanced Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-panning-with-gridplot",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-panning-with-gridplot",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.2 Linked Panning with gridplot()",
    "text": "14.2 Linked Panning with gridplot()\nIn a previous chapter, we created a column() of plots to visualize the leverage effect. However, the plots were all independent of one another, which may not be desireable.\nWe remedy that here using the gridplot() function. We also modify the x_range inputs of all of our figures so that they link properly.\nNotice that we now have a single toolbar for all four plots.\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import NumeralTickFormatter\n\noutput_notebook()\n\n# defining plot options all at once\nplot_options = dict(width=600, height=200, x_axis_type='datetime')\n\n# defining multiple plots - notice the change in the x_range for p2, p3, p3\np1 = figure(**plot_options, title='SPY Leverage Effect')\np1.line(x=df_spy['date'], y=df_spy['close'], legend_label='close')\np1.legend.location = 'top_left'\n\np2 = figure(x_range=p1.x_range,  **plot_options)\np2.line(x=df_spy['date'], y=df_spy['return'], legend_label='return')\np2.yaxis.formatter = NumeralTickFormatter(format='0%')\np2.legend.location = 'top_left'\n\np3 = figure(x_range=p1.x_range, **plot_options)\np3.line(x=df_spy['date'], y=df_spy['realized_vol'], legend_label='realized volatility')\np3.yaxis.formatter = NumeralTickFormatter(format='0%')\np3.legend.location = 'top_left'\n\np4 = figure(x_range=p1.x_range, **plot_options)\np4.line(x=df_spy['date'], y=df_spy['vix'], legend_label='vix')\np4.yaxis.formatter = NumeralTickFormatter(format='0%')\np4.legend.location = 'top_left'\n\n# putting all plots into a grid plot\np = gridplot([[p1], \n              [p2], \n              [p3],\n              [p4]]\n            )\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Advanced Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#specifying-tools",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#specifying-tools",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.3 Specifying Tools",
    "text": "14.3 Specifying Tools\nWe can also specify the interactive tools that we want in our graph.\nThe easiest way to do this is the tools argument of the figure() function which takes values that are comma delimited strings such as 'reset,hover,save'.\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\n\noutput_notebook()\n\ncds = ColumnDataSource(df_spy)\n\n# creating custom tool list\ntools = 'box_select,box_zoom,lasso_select,pan,wheel_zoom,reset,hover,save'\n\n# adding custom tool list to figurw\np = figure(width=600, height=400, tools=tools, title='SPY Implied Leverage Effect',\n           x_axis_label='return', y_axis_label='vix change')\n\n# adding glyph\np.hex('return', 'vix_change', line_color=\"navy\", fill_color=\"orange\", size=15, source=cds)\n\n# formatting the x-axis and y-axis to percents\np.xaxis.formatter = NumeralTickFormatter(format='0%') \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Advanced Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-properties",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-properties",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.4 Linked Properties",
    "text": "14.4 Linked Properties\nWe can also link properities of the graph to widgets.\nThe example below allows us to modify the size of the .hex glyphs so we can dial in the appearance of our graph.\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import Slider\nfrom bokeh.layouts import column\nfrom bokeh.models import NumeralTickFormatter\n\noutput_notebook()\n\ncds = ColumnDataSource(df_spy)\n\np = figure(width=600, height=400, title='SPY Implied Leverage Effect',\n           x_axis_label='return', y_axis_label='vix change')\n\n# adding glyph\nr = p.hex('return', 'vix_change', line_color=\"navy\", fill_color=\"orange\", size=10, source=cds)\n\n# linking slider to  the size of the hex glyph \nslider = Slider(start=1, end=20, step=1, value=10)\nslider.js_link('value', r.glyph, 'size')\n\n# formatting the x-axis and y-axis to percents\np.xaxis.formatter = NumeralTickFormatter(format='0%') \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\nshow(column(p, slider))\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Advanced Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-brushing",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#linked-brushing",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.5 Linked Brushing",
    "text": "14.5 Linked Brushing\nLinked brushing allows us to highlight related data between two graphs.\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import NumeralTickFormatter\n\noutput_notebook()\n\ncds = ColumnDataSource(df_spy)\n\n# defining plot options all at once, passing custom tool list directly\nplot_options = dict(width=500, height=350, tools='box_zoom,lasso_select,reset, box_select')\n\np1 = figure(**plot_options, title='SPY Leverage Effect', x_axis_label='return', y_axis_label='vix change')\np1.hex('return', 'vix_change', line_color=\"navy\", fill_color=\"orange\", size=10, source=cds)\n# formatting the x-axis and y-axis to percents\np1.xaxis.formatter = NumeralTickFormatter(format='0%') \np1.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np2 = figure(**plot_options, x_range=p1.x_range, title='Return vs Realized Vol',\n            x_axis_label='return', y_axis_label='realize vol')\np2.hex('return', 'realized_vol', line_color=\"navy\", fill_color=\"red\", size=10, source=cds)\n# formatting the x-axis and y-axis to percents\np2.xaxis.formatter = NumeralTickFormatter(format='0%') \np2.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np = gridplot([[p1,p2]])\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Advanced Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#hover-tool",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#hover-tool",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.6 Hover Tool",
    "text": "14.6 Hover Tool\nHover tools can be helpful for making large visualizations more readable. Here we add one to our monthly returns bar chart.\n\nfrom bokeh.io import output_notebook\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show\nfrom bokeh.plotting import reset_output\nfrom bokeh.models import NumeralTickFormatter\nimport math\nreset_output()\n\noutput_notebook()\n\nsource = ColumnDataSource(df_monthly)\n\n# defining custom tool list\ntools = 'box_zoom, reset'\n\n# defining tool tip\ntooltips = [\n    ('month', '@year_month'),\n    ('return', '@return{0.0%}'),\n]\n\np = figure(width=1000, height=400, x_range=df_monthly['year_month'], tools=[tools], tooltips=tooltips,\n           title='SPY Monthly Returns 2016Q1 to 2021Q3', x_axis_label='month', y_axis_label='monthly return')\np.vbar(x='year_month', bottom=0, top='return', color='blue', width=0.75, source=source)\n\n# formatting the y-axis to percents\np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\np.xaxis.major_label_orientation = math.pi/4\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Advanced Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/14_bokeh_advanced/bokeh_advanced.html#hover-tool-interactive-legend",
    "href": "chapters/14_bokeh_advanced/bokeh_advanced.html#hover-tool-interactive-legend",
    "title": "14  Advanced Plotting with bokeh",
    "section": "14.7 Hover Tool + Interactive Legend",
    "text": "14.7 Hover Tool + Interactive Legend\nHere we add a hover tool to our asset allocation stacked area plot.\nWe also make the legend interactive to be able to focus in on specific allocation buckets.\n\nimport bokeh\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models import NumeralTickFormatter\nfrom bokeh.models import HoverTool\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.io import output_notebook\n\noutput_notebook()\n\nsource = ColumnDataSource(df_asset_allocation)\n\np = figure(width=900, height=500, x_axis_type='datetime', title='Asset Allocation 2019-2020',\n           x_axis_label='month', y_axis_label='allocation')\n\n# choosing assets and colors to graph\nassets = df_asset_allocation.drop(columns='trade_date').columns\nnum_assets = len(assets)\ncolors = bokeh.palettes.magma(num_assets) # choosing color palette\n\n# adding glyphs\np.varea_stack(assets, x='trade_date', color=colors, source=source, legend_label=assets.to_list())\np.vline_stack(assets.to_list(), x='trade_date', color=colors, source=source,)\n\n# defining tool tip\np.add_tools(HoverTool(\n    tooltips = [\n        (\"Trade Date\", \"@trade_date{%F}\"),\n        (\"VXX\", \"@VXX{0%}\"),\n        (\"DBA\", \"@DBA{0%}\"),\n        (\"USO\", \"@USO{0%}\"),\n        (\"HYG\", \"@HYG{0%}\"),\n        (\"TLT\", \"@TLT{0%}\"),\n        (\"IWM\", \"@IWM{0%}\"),\n        (\"SPY\", \"@SPY{0%}\"),\n    ],\n    formatters={\n        '@trade_date':'datetime', # use 'datetime' formatter for 'date' field\n    },\n\n))\n\n# interactive legend\np.legend.click_policy='hide'\n\n# reformatting \np.yaxis.formatter = NumeralTickFormatter(format='0%')\n\n# reversing ordering of legend to make it consistent with stacking order\np.legend[0].items.reverse()\n\n# moving legend off of the graph\np.add_layout(p.legend[0], 'right')\n\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Advanced Plotting with **bokeh**</span>"
    ]
  },
  {
    "objectID": "chapters/15a_option_replication/option_replication.html",
    "href": "chapters/15a_option_replication/option_replication.html",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "",
    "text": "15.1 Loading Packages\nThe Black-Scholes-Merton (BSM) model is one of the foundations of quantitative finance. Not only does it provide a theoretical pricing framework for options, but it also gives a practical engineering result: the manufacturing process a derivatives desk should employ to construct options for its customers. This manufacturing process gave birth to the entire derivatives industry.\nThe purpose of this case study is to explore the BSM option manufacturing process by performing data analysis on simulated data.\nLet’s begin by loading the packages we will need.\nimport numpy as np\nimport pandas as pd\npd.options.display.max_rows = 10\nAdditionally, we will also require several functions from the py_vollib package for calculating option greeks.\nfrom py_vollib.black_scholes_merton import black_scholes_merton\nfrom py_vollib.black_scholes_merton.greeks.analytical import delta\nfrom py_vollib.black_scholes_merton.greeks.analytical import vega\nfrom py_vollib.black_scholes_merton.implied_volatility import implied_volatility",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Black-Scholes-Merton Option Replication</span>"
    ]
  },
  {
    "objectID": "chapters/15a_option_replication/option_replication.html#converting-py_vollib-functions",
    "href": "chapters/15a_option_replication/option_replication.html#converting-py_vollib-functions",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.2 Converting py_vollib Functions",
    "text": "15.2 Converting py_vollib Functions\nIn this section, we convert several of the py_vollib functions we imported above so that they can accept a row of a DataFrame as their argument. This will allow us to use these function with the DataFrame.apply() method, which we will use to write compact vectorized code. Notice that we assume a zero risk-free rate, and zero dividend yield throughout this tutorial, and thus those values are hardcoded into these functions..\n\ndef bsm_px(row):\n    cp = row['cp']\n    upx = row['upx']\n    strike = row['strike']\n    t2x = row['t2x']\n    rf = 0\n    volatility = row['volatility']\n    q = 0\n    px = black_scholes_merton(cp, upx, strike, t2x, rf, volatility, q)\n    px = np.round(px, 2)\n    return(px)\n\n\ndef bsm_delta(row):\n    cp = row['cp']\n    upx = row['upx']\n    strike = row['strike']\n    t2x = row['t2x']\n    rf = 0\n    volatility = row['volatility']\n    q = 0\n    if t2x == 0:\n        return(0)\n    diff = delta(cp, upx, strike, t2x, rf, volatility, q)\n    diff = np.round(diff, 3)\n    return(diff)\n\n\ndef bsm_vega(row):\n    cp = row['cp']\n    upx = row['upx']\n    strike = row['strike']\n    t2x = row['t2x']\n    rf = 0\n    volatility = row['volatility']\n    q = 0\n    if t2x == 0:\n        return(0)\n    vga = vega(cp, upx, strike, t2x, rf, volatility, q)\n    vga = np.round(vga, 3)\n    return(vga)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Black-Scholes-Merton Option Replication</span>"
    ]
  },
  {
    "objectID": "chapters/15a_option_replication/option_replication.html#geometric-brownian-motion",
    "href": "chapters/15a_option_replication/option_replication.html#geometric-brownian-motion",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.3 Geometric Brownian Motion",
    "text": "15.3 Geometric Brownian Motion\nThe series of trade prices for a stock is often modeled as a series of random variables, which is also referred to as a stochastic process. There many types of stochastic processes; some of them resemble actual stock price movements better than others.\nThe Black-Scholes-Merton option pricing framework assumes that the price process of the underlying asset follows a geometric brownian motion (GBM). This means that:\n\nThe price process is continuous.\nThe log return over any period of time is normally distributed.\nThe returns during any two disjoint periods are independent.\n\nGBMs are one of the simplest types of processes that reasonably model asset price dynamics, so it’s often a good place to start when learning about simulating stock price data.\nThe price process of a geometric brownian motion is determined by the current risk-free rate \\(r\\) and the annualized volatility of the underlying \\(\\sigma\\). Prices that are separated by \\(\\Delta t\\) units of time are related by following equation:\n\\[S_{t} =  S_{t - \\Delta t} \\cdot \\exp\\bigg(\\bigg(r - \\frac{1}{2}\\sigma^2\\bigg)\\Delta t + \\sigma \\sqrt{\\Delta t} z_{t}\\bigg)\\]\nwhere \\(z_{t}\\) is a standard normal random variable.\nThis is called the Euler discretization of a GBM. It will serve as the recipe for our price-path simulation algorithm. Note that the expression in the parentheses is the log-return of the stock between time \\(t - \\Delta t\\) and \\(t\\).\nAlthough the GBM assumptions are often violated in actual prices, there is still enough truth in them that the Black-Scholes-Merton manufacturing process is practically useful. It prescribes a process that derivative dealers can use to construct the contracts that their customers are interested in. This manufacturing process is referred to as constructing a replicating portfolio, which in the case of vanilla option is accomplished via a dynamic trading strategy of the underlying asset. This dynamic strategy is called delta-hedging.\n\nDiscussion Question: Put-call parity is a pricing identity that emanates from a static manufacturing (replication) strategy of a certain type of derivative. Describe the strategy: what kind of derivative does this replication strategy manufacture, and what is the resulting pricing identity.\n\n\nSolution\n##&gt; A forward contract can be manufactured/replicated with a \n##&gt; portfolio consisting of long call position plus a short put \n##&gt; position, both with the same strike, call it K.  Thus, the \n##&gt; put-call parity identity is: c(K, T) - p(K, T) = S - Ke^(-rT).",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Black-Scholes-Merton Option Replication</span>"
    ]
  },
  {
    "objectID": "chapters/15a_option_replication/option_replication.html#the-option-we-will-analyze",
    "href": "chapters/15a_option_replication/option_replication.html#the-option-we-will-analyze",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.4 The Option We Will Analyze",
    "text": "15.4 The Option We Will Analyze\nWe want to analyze what it means for a derivatives dealer to trade an option and then delta-hedge the position. Let’s consider an option that actually traded in the market place:\n\nunderlying: QQQ\ncurrent date: 11/16/2018\nexpiration: 12/21/2018\ntype: put\nstrike: 160\nupx: 168\ndays-to-expiration (d2x): 24\nprice: 2.25\n\nFrom this trade price, we can calculate an implied volatility, which we will also refer to as the pricing volatility. (Note that this is the typical flow of events, the price of an option is observed and from that observed price, an implied volatility is calculated.)\n\npricing_vol = implied_volatility(price = 2.25, S = 168, K = 160, t = 24/252, r = 0, q = 0, flag = 'p')\npricing_vol = np.round(pricing_vol, 4)\npricing_vol\n\n0.2636",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Black-Scholes-Merton Option Replication</span>"
    ]
  },
  {
    "objectID": "chapters/15a_option_replication/option_replication.html#delta-hedging-a-single-simulated-underlying-price-path",
    "href": "chapters/15a_option_replication/option_replication.html#delta-hedging-a-single-simulated-underlying-price-path",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.5 Delta-Hedging: A Single Simulated Underlying Price Path",
    "text": "15.5 Delta-Hedging: A Single Simulated Underlying Price Path\nIt is typical that a derivatives dealer will trade the above option with a customer, and then hold that option option until expiration and delta hedge it on a daily basis.\nThe BSM manufacturing framework states that the dealer will break even if:\n\nthe underlying price follows a geometric brownian motion\nthe realized volatility during the life of the option is equal to the implied volatility used to price the option\nthe dealer delta-hedges with frequent rebalancing (in order for the result to be deterministic the delta-hedging must be continuous)\n\nIn this section we are explore what this manufacturing process looks like for a particular price path of the underlying. In order to do this, let’s simulate a single geometric brownian motion path whose realized volatility is equal to the pricing volatility of our QQQ option. This price path will consist of a series of daily prices that starts with 168, the spot price at the time of the trade. We will rebalance the delta-hedge daily.\nThe following code generates the price path.\n\n# setting the random seed\nnp.random.seed(1)\n\n# parameters of simulation\nr = 0\npath_vol = pricing_vol\ndt = 1./252\n\n# initializing paths\nsingle_path = np.zeros(25)\nsingle_path[0] = 168\n\n# looping through days and generating steps in the paths\nfor t in range(1, 25):\n    z = np.random.standard_normal(1)\n    single_path[t] = single_path[t - 1] * np.exp((r - 0.5 * path_vol ** 2) * dt + path_vol * np.sqrt(dt) * z) # memorize this line\n    single_path[t] = np.round(single_path[t], 2)\n\nLet’s take a look at the path we generated. (Obviously, in a real-world situation this price path would be realized over the course of the life of the option.)\n\nsingle_path\n\narray([168.  , 172.57, 170.8 , 169.29, 166.28, 168.66, 162.31, 167.06,\n       164.94, 165.79, 165.08, 169.11, 163.4 , 162.51, 161.45, 164.5 ,\n       161.5 , 161.02, 158.67, 158.76, 160.28, 157.36, 160.36, 162.76,\n       164.1 ])\n\n\nNext, let’s create a DataFrame that will track the PNL from delta-hedging the option; this DataFrame will contain all the information needed to calculate the price and greeks of the option on a daily basis.\n\ndf_path = \\\n    (\n    pd.DataFrame(\n        {'underlying':'QQQ',\n         'cp':'p',\n         'strike':160,\n         'volatility':0.2636,\n         'upx':single_path, \n         'd2x':list(range(24, -1, -1)),\n         'buy_sell':1,\n        }       \n    )\n    .assign(t2x = lambda df: df.d2x / 252)\n    )\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n\n\n\n\n25 rows × 8 columns\n\n\n\nWe can now use the bsm_px() function to calculate the prices of the option for each day in the simulation. At a derivatives dealer, the option value and greeks would be monitored in real-time in a position management system.\n\ndf_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n\n\n\n\n25 rows × 9 columns\n\n\n\nLet’s calculate the deltas through time. Notice that as the price of the underlying goes down the (absolute) delta of the put increases, and as the price of the underlying goes up the delta decreases.\n\ndf_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\ndelta\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n-0.261\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n-0.161\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n-0.190\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n-0.218\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n-0.289\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n-0.472\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n-0.714\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n-0.457\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n-0.150\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n0.000\n\n\n\n\n25 rows × 10 columns\n\n\n\nNext, we calculate the option PNL.\n\ndf_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\ndelta\noption_pnl\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n-0.261\nNaN\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n-0.161\n-1.04\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n-0.190\n0.23\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n-0.218\n0.23\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n-0.289\n0.66\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n-0.472\n-1.05\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n-0.714\n1.46\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n-0.457\n-2.11\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n-0.150\n-1.12\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n0.000\n-0.21\n\n\n\n\n25 rows × 11 columns\n\n\n\nDelta-hedging with daily rebalancing means at the end of each day we hold a position in the underlying whose size is equal to the negative of the delta of the option position. Thus, the daily delta-hedging PNL is calculated as follows:\n\ndf_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff() \ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\ndelta\noption_pnl\ndelta_hedge_pnl\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n-0.261\nNaN\nNaN\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n-0.161\n-1.04\n1.19277\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n-0.190\n0.23\n-0.28497\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n-0.218\n0.23\n-0.28690\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n-0.289\n0.66\n-0.65618\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n-0.472\n-1.05\n0.87552\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n-0.714\n1.46\n-1.37824\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n-0.457\n-2.11\n2.14200\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n-0.150\n-1.12\n1.09680\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n0.000\n-0.21\n0.20100\n\n\n\n\n25 rows × 12 columns\n\n\n\n\nDiscussion Question: What is the delta-hedging position held at the end of d2x = 21. What is the trade executed at that time?\n\n\nSolution\n##&gt; The end-of-day delta hedge is long .714 contracts of the QQQ.  \n##&gt; In order to get to that position we would have to buy 0.242 contracts. \n\n\n\nThe total_pnl of the delta-hedged option position is the combination of the option_pnl and the delta_hedge_pnl.\n\ndf_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\ndf_path\n\n\n\n\n\n\n\n\nunderlying\ncp\nstrike\nvolatility\nupx\nd2x\nbuy_sell\nt2x\noption_price\ndelta\noption_pnl\ndelta_hedge_pnl\ntotal_pnl\n\n\n\n\n0\nQQQ\np\n160\n0.2636\n168.00\n24\n1\n0.095238\n2.25\n-0.261\nNaN\nNaN\nNaN\n\n\n1\nQQQ\np\n160\n0.2636\n172.57\n23\n1\n0.091270\n1.21\n-0.161\n-1.04\n1.19277\n0.15277\n\n\n2\nQQQ\np\n160\n0.2636\n170.80\n22\n1\n0.087302\n1.44\n-0.190\n0.23\n-0.28497\n-0.05497\n\n\n3\nQQQ\np\n160\n0.2636\n169.29\n21\n1\n0.083333\n1.67\n-0.218\n0.23\n-0.28690\n-0.05690\n\n\n4\nQQQ\np\n160\n0.2636\n166.28\n20\n1\n0.079365\n2.33\n-0.289\n0.66\n-0.65618\n0.00382\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\nQQQ\np\n160\n0.2636\n160.28\n4\n1\n0.015873\n1.98\n-0.472\n-1.05\n0.87552\n-0.17448\n\n\n21\nQQQ\np\n160\n0.2636\n157.36\n3\n1\n0.011905\n3.44\n-0.714\n1.46\n-1.37824\n0.08176\n\n\n22\nQQQ\np\n160\n0.2636\n160.36\n2\n1\n0.007937\n1.33\n-0.457\n-2.11\n2.14200\n0.03200\n\n\n23\nQQQ\np\n160\n0.2636\n162.76\n1\n1\n0.003968\n0.21\n-0.150\n-1.12\n1.09680\n-0.02320\n\n\n24\nQQQ\np\n160\n0.2636\n164.10\n0\n1\n0.000000\n0.00\n0.000\n-0.21\n0.20100\n-0.00900\n\n\n\n\n25 rows × 13 columns\n\n\n\nAs we can see, the total PNL of the delta-hedged option position is close to zero, but not exactly zero.\n\ndf_path['total_pnl'].sum()\n\n0.18382999999999813\n\n\n\nCode Challenge: Copy and past the above code into the space below, and then modify it to calculate the PNL for selling this option for 2.25 and then delta-hedging it over this same scenario.\n\n\nSolution\n# creating the DataFrame\ndf_path_test = \\\n    (\n    pd.DataFrame(\n        {'underlying':'QQQ',\n         'cp':'p',\n         'strike':160,\n         'volatility':0.2636,\n         'upx':single_path, \n         'd2x':list(range(24, -1, -1)),\n         'buy_sell':-1,\n        }       \n    )\n    .assign(t2x = lambda df: df.d2x / 252)\n    )\n\n# calculating prices, greeks, and PNLs\ndf_path_test['option_price'] = df_path_test[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\ndf_path_test['delta'] = df_path_test[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\ndf_path_test['option_pnl'] =  df_path_test['buy_sell'] * df_path_test['option_price'].diff()\ndf_path_test['delta_hedge_pnl'] = -df_path_test['buy_sell'] * df_path_test['delta'].shift(1) * df_path_test['upx'].diff()\ndf_path_test['total_pnl'] = df_path_test['option_pnl'] + df_path_test['delta_hedge_pnl']\n\n# calculating total PNL\ndf_path_test['total_pnl'].sum()\n\n\n-0.18382999999999813",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Black-Scholes-Merton Option Replication</span>"
    ]
  },
  {
    "objectID": "chapters/15a_option_replication/option_replication.html#delta-hedging-multiple-simulated-underlying-price-paths",
    "href": "chapters/15a_option_replication/option_replication.html#delta-hedging-multiple-simulated-underlying-price-paths",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.6 Delta-Hedging: Multiple Simulated Underlying Price Paths",
    "text": "15.6 Delta-Hedging: Multiple Simulated Underlying Price Paths\nAs we saw in the above example, we came fairly close to a zero PNL from delta hedging on a daily basis, which is what the BSM framework suggests. However, this was just for a single hypothetical path, so we may have just gotten lucky. In this section, we will generate PNL data for daily delta-hedging over a variety of paths, and analyze the resulting distribution.\nLet’s begin by initializing our option position and scenario generation parameters.\n\nbuy_sell = -1\nd2x = 24\ncp = 'p'\nspot = 168.\nstrike = 160.\ntenor = np.double(d2x)/252.\noption_price = 2.25\npricing_vol = implied_volatility(price = option_price, S = spot, K = strike, t = tenor, r = 0, q = 0, flag = cp)\npath_vol = pricing_vol\nhedge_frequency = d2x\ndt = tenor / hedge_frequency\nr = 0\nnum_paths = 1000\n\nNext, we initialize an array that will hold all of our paths.\n\nmultiple_paths = np.zeros((hedge_frequency + 1, num_paths))\nmultiple_paths\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\nThe first price in every path is the current spot price of the underlying.\n\nmultiple_paths[0] = spot\nmultiple_paths\n\narray([[168., 168., 168., ..., 168., 168., 168.],\n       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n       ...,\n       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n       [  0.,   0.,   0., ...,   0.,   0.,   0.]])\n\n\nUsing the convenience of broadcasting in numpy.arrays, we can easily calculate all of the required scenarios in a few lines of code.\n\n# setting the random seed\nnp.random.seed(1)\n\nfor t in range(1, hedge_frequency + 1):\n    z = np.random.standard_normal(num_paths) \n    multiple_paths[t] = multiple_paths[t - 1] * np.exp((r - 0.5 * path_vol ** 2) * dt + path_vol * np.sqrt(dt) * z)\n    multiple_paths[t] = np.round(multiple_paths[t], 2)\n    \nmultiple_paths\n\narray([[168.  , 168.  , 168.  , ..., 168.  , 168.  , 168.  ],\n       [172.57, 166.28, 166.51, ..., 167.78, 168.97, 167.46],\n       [172.11, 159.67, 167.9 , ..., 165.21, 170.77, 171.34],\n       ...,\n       [159.97, 156.87, 163.42, ..., 144.66, 174.18, 169.42],\n       [161.34, 160.23, 164.69, ..., 145.86, 170.28, 170.59],\n       [164.14, 157.3 , 162.92, ..., 145.92, 176.16, 169.25]])\n\n\nJust to make sure we understand our data structures, let’s pull out the first scenario and perform our delta-hedge calculations with it. After we do that, we will wrap this code in a for-loop in order to perform the delta-hedge calculations for all the scenarios.\n\n# creating the DataFrame\ndf_path = \\\n    pd.DataFrame(\n        {'cp':cp,\n         'strike':strike,\n         'volatility':path_vol,\n         'upx':multiple_paths[:, 0], \n         't2x':np.linspace(tenor, 0, hedge_frequency + 1),\n         'buy_sell': buy_sell,\n        }       \n    )\n\n# calculating prices, greeks, and PNLs\ndf_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\ndf_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\ndf_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\ndf_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff()\ndf_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\n\n# viewing the path\ndf_path\n\n\n\n\n\n\n\n\ncp\nstrike\nvolatility\nupx\nt2x\nbuy_sell\noption_price\ndelta\noption_pnl\ndelta_hedge_pnl\ntotal_pnl\n\n\n\n\n0\np\n160.0\n0.263631\n168.00\n0.095238\n-1\n2.25\n-0.261\nNaN\nNaN\nNaN\n\n\n1\np\n160.0\n0.263631\n172.57\n0.091270\n-1\n1.21\n-0.161\n1.04\n-1.19277\n-0.15277\n\n\n2\np\n160.0\n0.263631\n172.11\n0.087302\n-1\n1.21\n-0.165\n-0.00\n0.07406\n0.07406\n\n\n3\np\n160.0\n0.263631\n173.49\n0.083333\n-1\n0.93\n-0.135\n0.28\n-0.22770\n0.05230\n\n\n4\np\n160.0\n0.263631\n173.24\n0.079365\n-1\n0.90\n-0.134\n0.03\n0.03375\n0.06375\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\np\n160.0\n0.263631\n163.76\n0.015873\n-1\n0.77\n-0.237\n0.55\n-0.38913\n0.16087\n\n\n21\np\n160.0\n0.263631\n158.57\n0.011905\n-1\n2.63\n-0.617\n-1.86\n1.23003\n-0.62997\n\n\n22\np\n160.0\n0.263631\n159.97\n0.007937\n-1\n1.51\n-0.499\n1.12\n-0.86380\n0.25620\n\n\n23\np\n160.0\n0.263631\n161.34\n0.003968\n-1\n0.53\n-0.305\n0.98\n-0.68363\n0.29637\n\n\n24\np\n160.0\n0.263631\n164.14\n0.000000\n-1\n0.00\n0.000\n0.53\n-0.85400\n-0.32400\n\n\n\n\n25 rows × 11 columns\n\n\n\nLet’s check the cumulative PNL for this scenario; clearly the PNL outcome is much farther for this scenario than the one above.\n\ndf_path['total_pnl'].sum()\n\n-1.612160000000005\n\n\nWe can now generalize the above code and perform the delta-hedging calculations on each scenario. We will save the calculations for each scenario to analyze.\n\nlst_scenarios = []\nfor ix_path in range(0, num_paths):\n    \n    # creating dataframe\n    df_path = \\\n        pd.DataFrame(\n            {'cp':cp,\n             'strike':strike,\n             'volatility':pricing_vol,\n             'upx':multiple_paths[:, ix_path], \n             't2x':np.linspace(tenor, 0, hedge_frequency + 1),\n             'buy_sell':buy_sell\n            }\n        )\n    \n    # calculating prices, greeks, and PNLs\n    df_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\n    df_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\n    df_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\n    df_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff()\n    df_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\n    df_path['scenario'] = ix_path\n    \n    # storing df_path into a list\n    lst_scenarios.append(df_path)\n\n# creating a single DataFrame that contains all scenarios\ndf_all_paths = pd.concat(lst_scenarios)\n\n# viewing the DataFrame\ndf_all_paths\n\n\n\n\n\n\n\n\ncp\nstrike\nvolatility\nupx\nt2x\nbuy_sell\noption_price\ndelta\noption_pnl\ndelta_hedge_pnl\ntotal_pnl\nscenario\n\n\n\n\n0\np\n160.0\n0.263631\n168.00\n0.095238\n-1\n2.25\n-0.261\nNaN\nNaN\nNaN\n0\n\n\n1\np\n160.0\n0.263631\n172.57\n0.091270\n-1\n1.21\n-0.161\n1.04\n-1.19277\n-0.15277\n0\n\n\n2\np\n160.0\n0.263631\n172.11\n0.087302\n-1\n1.21\n-0.165\n-0.00\n0.07406\n0.07406\n0\n\n\n3\np\n160.0\n0.263631\n173.49\n0.083333\n-1\n0.93\n-0.135\n0.28\n-0.22770\n0.05230\n0\n\n\n4\np\n160.0\n0.263631\n173.24\n0.079365\n-1\n0.90\n-0.134\n0.03\n0.03375\n0.06375\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20\np\n160.0\n0.263631\n171.42\n0.015873\n-1\n0.04\n-0.018\n0.03\n0.00837\n0.03837\n999\n\n\n21\np\n160.0\n0.263631\n173.24\n0.011905\n-1\n0.00\n-0.003\n0.04\n-0.03276\n0.00724\n999\n\n\n22\np\n160.0\n0.263631\n169.42\n0.007937\n-1\n0.01\n-0.007\n-0.01\n0.01146\n0.00146\n999\n\n\n23\np\n160.0\n0.263631\n170.59\n0.003968\n-1\n0.00\n-0.000\n0.01\n-0.00819\n0.00181\n999\n\n\n24\np\n160.0\n0.263631\n169.25\n0.000000\n-1\n0.00\n0.000\n-0.00\n0.00000\n0.00000\n999\n\n\n\n\n25000 rows × 12 columns\n\n\n\nLet’s use a .groupby() to calculate the cummulative PNL for each scenario\n\ndf_pnl = df_all_paths.groupby(['scenario'], as_index = False)[['total_pnl']].sum()\ndf_pnl\n\n\n\n\n\n\n\n\nscenario\ntotal_pnl\n\n\n\n\n0\n0\n-1.61216\n\n\n1\n1\n0.40480\n\n\n2\n2\n0.34073\n\n\n3\n3\n0.31858\n\n\n4\n4\n0.16595\n\n\n...\n...\n...\n\n\n995\n995\n1.75078\n\n\n996\n996\n-0.83710\n\n\n997\n997\n0.13385\n\n\n998\n998\n0.54689\n\n\n999\n999\n-1.77710\n\n\n\n\n1000 rows × 2 columns\n\n\n\nAs we can see, the average of the total_pnls is zero, which further demonstrates the manufacturing result of the Black-Scholes-Merton framework.\n\ndf_pnl['total_pnl'].mean()\n\n-0.0009835299999998704\n\n\n\nDiscussion Queston: If you sold this option for 0.25 more than fair-value, what would your average PNL be?\n\n\nSolution\n##&gt; Approximately $0.25\n\n\n\nCode Challenge: Is the delta-hedging reducing risk? Try to verify this with a bit of data analysis.\n\n\nSolution\nprint(df_all_paths.groupby('scenario')['option_pnl'].sum().std())\nprint(df_all_paths.groupby('scenario')['total_pnl'].sum().std())\n\n\n5.006480605770481\n0.7586453989501201\n\n\n\nCode Challenge: Calculate the standard deviation, minimum, and maximum of the cumulative PNLs.\n\n\nSolution\nprint(\"Std Dev:\", np.round(df_pnl['total_pnl'].std(), 2))\nprint(\"Min:   \", np.round(df_pnl['total_pnl'].min(), 2))\nprint(\"Max:    \",np.round(df_pnl['total_pnl'].max(), 2))\n\n\nStd Dev: 0.76\nMin:    -2.9\nMax:     2.49\n\n\n\nDiscussion Question: What are your thoughts on the range of possible PNL outcomes? Does option replication via discrete delta-hedging seem like a risk-free endeavor?\n\n\nSolution\n##&gt; There is a wide variation of PNLs, and it is possible to have a \n##&gt; gain or a loss that is greater than the value of the option itself.\n##&gt; Discrete delta-hedging is far from riskless.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Black-Scholes-Merton Option Replication</span>"
    ]
  },
  {
    "objectID": "chapters/15a_option_replication/option_replication.html#what-if-realized-volatility-is-different-that-pricing-volatility",
    "href": "chapters/15a_option_replication/option_replication.html#what-if-realized-volatility-is-different-that-pricing-volatility",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.7 What If Realized Volatility is Different that Pricing Volatility?",
    "text": "15.7 What If Realized Volatility is Different that Pricing Volatility?\nAs we can see above, if the realized volatility of the underlying during the life of the option is equal to the pricing volatility, then the daily delta-hedging trader will break even on average (however, the outcome can vary substantially depending on the particular scenario).\nIn this section, we explore what happens when realized volatility differs from pricing volatility. In particular, we will see what happens when a trader sells an option, delta-hedges daily, but the realized volatility is 5% higher than implied.\n\n# setting simulation parameters\nbuy_sell = -1\nd2x = 24\ncp = 'p'\nspot = 168.\nstrike = 160.\ntenor = np.double(d2x)/252.\noption_price = 2.25\npricing_vol = implied_volatility(price = option_price, S = spot, K = strike, t = tenor, r = 0, q = 0, flag = cp)\npath_vol = pricing_vol + 0.05\nhedge_frequency = d2x\ndt = tenor / hedge_frequency\nr = 0\nnum_paths = 1000\n\n\n# initializing paths\nmultiple_paths = np.zeros((hedge_frequency + 1, num_paths))\nmultiple_paths[0] = spot\n\n# setting the random seed\nnp.random.seed(1)\n\n# calculating paths\nfor t in range(1, hedge_frequency + 1):\n    z = np.random.standard_normal(num_paths) \n    multiple_paths[t] = multiple_paths[t - 1] * np.exp((r - 0.5 * path_vol ** 2) * dt + path_vol * np.sqrt(dt) * z)\n    multiple_paths[t] = np.round(multiple_paths[t], 2)\n\n# performing delta-hedge calculations on all the paths\nlst_scenarios = []\nfor ix_path in range(0, num_paths):\n    \n    # creating the DataFrame\n    df_path = \\\n        pd.DataFrame(\n            {'cp':cp,\n             'strike':strike,\n             'volatility':pricing_vol,\n             'upx':multiple_paths[:, ix_path], \n             't2x':np.linspace(tenor, 0, hedge_frequency + 1),\n             'buy_sell':buy_sell\n            }\n        )\n    \n    # calculating prices, greeks, and PNLs\n    df_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\n    df_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\n    df_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\n    df_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff()\n    df_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\n    df_path['scenario'] = ix_path\n    \n    # storing df_path into a list\n    lst_scenarios.append(df_path)\n    \n# creating a single DataFrame that contains all scenarios    \ndf_all_paths = pd.concat(lst_scenarios)\n\n# calculating cumulative PNLs\ndf_pnl = df_all_paths.groupby(['scenario'], as_index = False)[['total_pnl']].sum()\n\nAs we can see, since realized is greater than implied, we lose money on average.\n\ndf_pnl['total_pnl'].mean()\n\n-0.8403840600000003\n\n\n\nCode Challenge: Use bsm_vega and verify that this PNL is consistent with the identity: \\(vega * (implied - realzed)\\).\n\n\nSolution\n-bsm_vega(df_all_paths[['cp', 'upx', 'strike', 't2x', 'volatility']].iloc[0,:]) * 5\n\n\n-0.8400000000000001",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Black-Scholes-Merton Option Replication</span>"
    ]
  },
  {
    "objectID": "chapters/15a_option_replication/option_replication.html#increasing-delta-hedge-frequency-reduces-pnl-variability",
    "href": "chapters/15a_option_replication/option_replication.html#increasing-delta-hedge-frequency-reduces-pnl-variability",
    "title": "15  Black-Scholes-Merton Option Replication",
    "section": "15.8 Increasing Delta-Hedge Frequency Reduces PNL Variability",
    "text": "15.8 Increasing Delta-Hedge Frequency Reduces PNL Variability\nThe BSM manufacturing framework states that in the limit of continuous delta-hedging, that these results become deterministic. The means that the delta-hedging outcomes are always the same for each scenario. Your final code challenge is to explore this via data analysis.\n\nCode Challenge: Copy and paste the above code, and see what happens to the dispersion of the distribution of the delta-hedge PNL outcomes when you double and quadruple the hedge_frequency.\n\n\nSolution\n# setting simulation parameters\nbuy_sell = -1\nd2x = 24\ncp = 'p'\nspot = 168.\nstrike = 160.\ntenor = np.double(d2x)/252. # I want to generalize this\noption_price = 2.25\npricing_vol = implied_volatility(price = option_price, S = spot, K = strike, t = tenor, r = 0, q = 0, flag = cp)\npath_vol = pricing_vol\nhedge_frequency = d2x * 4\ndt = tenor / hedge_frequency  # I want to generalize this\nr = 0\nnum_paths = 1000\n\n\n# initializing paths\nmultiple_paths = np.zeros((hedge_frequency + 1, num_paths))\nmultiple_paths[0] = spot\n\n# setting the random seed\nnp.random.seed(1)\n\n# calculating paths\nfor t in range(1, hedge_frequency + 1):\n    z = np.random.standard_normal(num_paths) \n    multiple_paths[t] = multiple_paths[t - 1] * np.exp((r - 0.5 * path_vol ** 2) * dt + path_vol * np.sqrt(dt) * z)\n    multiple_paths[t] = np.round(multiple_paths[t], 2)\n\n# performing delta-hedge calculations on all the paths\nlst_scenarios = []\nfor ix_path in range(0, num_paths):\n    \n    # creating the DataFrame\n    df_path = \\\n        pd.DataFrame(\n            {'cp':cp,\n             'strike':strike,\n             'volatility':pricing_vol,\n             'upx':multiple_paths[:, ix_path], \n             't2x':np.linspace(tenor, 0, hedge_frequency + 1),\n             'buy_sell':buy_sell\n            }\n        )\n    \n    # calculating prices, greeks, and PNLs\n    df_path['option_price'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_px, axis = 1)\n    df_path['delta'] = df_path[['cp', 'upx', 'strike', 't2x', 'volatility']].apply(bsm_delta, axis = 1)\n    df_path['option_pnl'] = df_path['buy_sell'] * df_path['option_price'].diff()\n    df_path['delta_hedge_pnl'] = -df_path['buy_sell'] * df_path['delta'].shift(1) * df_path['upx'].diff()\n    df_path['total_pnl'] = df_path['option_pnl'] + df_path['delta_hedge_pnl']\n    df_path['scenario'] = ix_path\n    \n    # storing df_path into a list\n    lst_scenarios.append(df_path)\n    \n# creating a single DataFrame that contains all scenarios    \ndf_all_paths = pd.concat(lst_scenarios)\n\n# calculating cumulative PNLs\ndf_pnl = df_all_paths.groupby(['scenario'], as_index = False)[['total_pnl']].sum()\n\n# calculating distrubution statistics\nprint(\"Std Dev:\", np.round(df_pnl['total_pnl'].std(), 2))\nprint(\"Min:   \", np.round(df_pnl['total_pnl'].min(), 2))\nprint(\"Max:    \",np.round(df_pnl['total_pnl'].max(), 2))\n\n\nStd Dev: 0.42\nMin:    -1.89\nMax:     1.92",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Black-Scholes-Merton Option Replication</span>"
    ]
  },
  {
    "objectID": "chapters/15b_asset_allocation/asset_allocation.html",
    "href": "chapters/15b_asset_allocation/asset_allocation.html",
    "title": "16  Asset Allocation",
    "section": "",
    "text": "16.1 Importing Packages\nIn this chapter we conduct a data analysis related to asset allocation. Specifically, we analyze several portfolios consisting of six ETFs: DBA, GLD, LQD, SPY, TLT, UUP.\nUsing 2019 data, we construct four portfolios that we assume we hold during 2020, and then perform some analysis on how each of the portfolios performed during 2020. In particular we will:\nLet’s begin by importing the packages that we will need. All of them are standard except for the riskparityportfolio package, which we will use to calculate the weights for the risk-parity portfolio.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport riskparityportfolio as rp",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Asset Allocation</span>"
    ]
  },
  {
    "objectID": "chapters/15b_asset_allocation/asset_allocation.html#weights-portfolio-volatilities-risk-contributions-2019-data",
    "href": "chapters/15b_asset_allocation/asset_allocation.html#weights-portfolio-volatilities-risk-contributions-2019-data",
    "title": "16  Asset Allocation",
    "section": "16.2 Weights, Portfolio Volatilities, Risk Contributions (2019 Data)",
    "text": "16.2 Weights, Portfolio Volatilities, Risk Contributions (2019 Data)\nUsing data from 2019 we will calculate the weights, estimates of portfolio volatility, and estimates of asset risk contributions for our various portfolios.\n\n16.2.1 Reading-In Data\nLet’s begin by reading in the 2019 data.\n\nassets = ['SPY', 'TLT', 'GLD', 'UUP', 'DBA', 'LQD']\ndf_assets_weights = yf.download(\n    assets, start='2018-12-31', end='2020-01-01',\n    auto_adjust = False\n)\ndf_assets_weights = df_assets_weights['Close']\ndf_assets_weights\n\n[*********************100%***********************]  6 of 6 completed\n\n\n\n\n\n\n\n\nTicker\nDBA\nGLD\nLQD\nSPY\nTLT\nUUP\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2018-12-31\n16.940001\n121.250000\n112.820000\n249.919998\n121.510002\n25.450001\n\n\n2019-01-02\n16.910000\n121.330002\n113.169998\n250.179993\n122.150002\n25.639999\n\n\n2019-01-03\n16.969999\n122.430000\n113.220001\n244.210007\n123.540001\n25.500000\n\n\n2019-01-04\n16.940001\n121.440002\n113.150002\n252.389999\n122.110001\n25.459999\n\n\n2019-01-07\n17.160000\n121.860001\n113.160004\n254.380005\n121.750000\n25.350000\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2019-12-24\n16.340000\n141.270004\n127.849998\n321.230011\n136.839996\n26.260000\n\n\n2019-12-26\n16.370001\n142.380005\n128.080002\n322.940002\n137.169998\n26.219999\n\n\n2019-12-27\n16.520000\n142.330002\n128.259995\n322.859985\n137.320007\n26.090000\n\n\n2019-12-30\n16.530001\n142.630005\n128.490005\n321.079987\n136.820007\n26.040001\n\n\n2019-12-31\n16.559999\n142.899994\n127.959999\n321.859985\n135.479996\n25.969999\n\n\n\n\n253 rows × 6 columns\n\n\n\n\n\n16.2.2 Returns and Covariance Matrix\nNext, we use pandas DataFrame methods to calculated the daily returns and returns covariance matrix for 2019.\n\ndf_returns_weights = df_assets_weights.pct_change()\ndf_returns_weights\n\n\n\n\n\n\n\nTicker\nDBA\nGLD\nLQD\nSPY\nTLT\nUUP\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2018-12-31\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2019-01-02\n-0.001771\n0.000660\n0.003102\n0.001040\n0.005267\n0.007466\n\n\n2019-01-03\n0.003548\n0.009066\n0.000442\n-0.023863\n0.011379\n-0.005460\n\n\n2019-01-04\n-0.001768\n-0.008086\n-0.000618\n0.033496\n-0.011575\n-0.001569\n\n\n2019-01-07\n0.012987\n0.003458\n0.000088\n0.007885\n-0.002948\n-0.004320\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2019-12-24\n0.004920\n0.009432\n0.001018\n0.000031\n0.002858\n0.000000\n\n\n2019-12-26\n0.001836\n0.007857\n0.001799\n0.005323\n0.002412\n-0.001523\n\n\n2019-12-27\n0.009163\n-0.000351\n0.001405\n-0.000248\n0.001094\n-0.004958\n\n\n2019-12-30\n0.000605\n0.002108\n0.001793\n-0.005513\n-0.003641\n-0.001916\n\n\n2019-12-31\n0.001815\n0.001893\n-0.004125\n0.002429\n-0.009794\n-0.002688\n\n\n\n\n253 rows × 6 columns\n\n\n\n\ndf_covariance_returns = df_returns_weights.cov()\ndf_covariance_returns\n\n\n\n\n\n\n\nTicker\nDBA\nGLD\nLQD\nSPY\nTLT\nUUP\n\n\nTicker\n\n\n\n\n\n\n\n\n\n\nDBA\n0.000049\n-0.000006\n-3.023833e-06\n1.074811e-05\n-0.000011\n1.983865e-06\n\n\nGLD\n-0.000006\n0.000054\n1.262861e-05\n-1.586571e-05\n0.000032\n-1.120170e-05\n\n\nLQD\n-0.000003\n0.000013\n9.849972e-06\n-7.090972e-07\n0.000019\n-1.459647e-06\n\n\nSPY\n0.000011\n-0.000016\n-7.090972e-07\n6.250532e-05\n-0.000027\n9.363556e-07\n\n\nTLT\n-0.000011\n0.000032\n1.916895e-05\n-2.706504e-05\n0.000057\n-1.888845e-06\n\n\nUUP\n0.000002\n-0.000011\n-1.459647e-06\n9.363556e-07\n-0.000002\n1.038399e-05\n\n\n\n\n\n\n\n\n\n16.2.3 Weights\nLet’s now calculate the weights of each asset in our four portfolios. The weighting schemes from SPY-only and equal-weight are self explanatory. Inverse volatility and risk-parity both have the effect of underweighting high volatility assets while overweighting low volatility assets.\nInverse Volatility: the weight for the \\(i\\)th asset is \\(w_i = \\frac{1/\\sigma_i}{\\sum_j 1/\\sigma_j}\\), where \\(\\sigma_i\\) is the volatility of the \\(i\\)th asset.\nRisk-Parity: intuitively, this methods chooses weight such that the risk that each asset contributes to the portfolio is equal. This idea can be translated in to an optimization problem involving the covariance matrix of the assets. We will use the riskparityportfolio package to do the optimization for us.\n\n# SPY only\nspy = np.array([0, 0, 0, 1, 0, 0])\n\n\n# equal weight\nequal_weight = np.ones(6) / 6\n\n# inverse-volatility\nvolatilities = np.sqrt(np.diagonal(df_covariance_returns))\ninverse_volatility = (1 / volatilities) / np.sum(1 / volatilities)\n\n# risk-parity\nrisk_budget = np.ones(6) / 6\nrisk_parity = rp.vanilla.design(df_covariance_returns, risk_budget)\n\n# putting weights into a DataFrame\ndf_weights = \\\n    pd.DataFrame({\n        'symbol':df_assets_weights.columns,\n        'spy': spy,\n        'equal_weight': equal_weight,\n        'inverse_volatility': inverse_volatility,\n        'risk_parity': risk_parity,\n    })\ndf_weights\n\n\n\n\n\n\n\n\nsymbol\nspy\nequal_weight\ninverse_volatility\nrisk_parity\n\n\n\n\n0\nDBA\n0\n0.166667\n0.122781\n0.123998\n\n\n1\nGLD\n0\n0.166667\n0.116644\n0.129908\n\n\n2\nLQD\n0\n0.166667\n0.272929\n0.169122\n\n\n3\nSPY\n1\n0.166667\n0.108345\n0.134926\n\n\n4\nTLT\n0\n0.166667\n0.113483\n0.097552\n\n\n5\nUUP\n0\n0.166667\n0.265819\n0.344494\n\n\n\n\n\n\n\nLet’s plot the weights to compare the equal-weight, inverse-volatility, and risk-parity portfolios.\n\ndf_weights.plot(\n    kind='bar', \n    x='symbol', \n    y=['equal_weight', 'inverse_volatility', 'risk_parity'],\n    ylabel='weight'\n\n);\n\n\n\n\n\n\n\n\n\n\n16.2.4 Portfolio Volatilities\nNext, we calculate the estimated portfolio volatilities at end-of-day 12/31/2019. Note that these are not the actual (2020) realized volatilites of the portfolios.\n\ndef portfolio_volatility(weights, cov):\n    return np.sqrt(np.dot(weights, np.dot(cov, weights.T))) * np.sqrt(252)\n\n\n# portfolio volatilities\ndf_portfolio_volatilities = \\\n    pd.DataFrame({\n        'spy': [portfolio_volatility(df_weights['spy'], df_covariance_returns)],\n        'equal_weight': [portfolio_volatility(df_weights['equal_weight'], df_covariance_returns)],\n        'inverse_volatility': [portfolio_volatility(df_weights['inverse_volatility'], df_covariance_returns)],\n        'risk_parity': [portfolio_volatility(df_weights['risk_parity'], df_covariance_returns)],\n    })\ndf_portfolio_volatilities\n\n\n\n\n\n\n\n\nspy\nequal_weight\ninverse_volatility\nrisk_parity\n\n\n\n\n0\n0.125504\n0.040973\n0.03509\n0.032783\n\n\n\n\n\n\n\n\n\n16.2.5 Risk Contributions\nFinally, we calculate the risk-contributions of each asset in the various portfolios. Notice, that buy construction, in the risk-parity portfolio the risk contributions of each of the assets is equal.\n\ndef risk_contributions(weights, cov):\n    mctar = np.dot(weights, cov * 252) / portfolio_volatility(weights, cov)\n    return mctar * weights / portfolio_volatility(weights, cov)\n\n\ndf_risk_contributions = \\\n    pd.DataFrame({\n        'symbol':df_assets_weights.columns,\n        'spy': risk_contributions(df_weights['spy'], df_covariance_returns),\n        'equal_weight': risk_contributions(df_weights['equal_weight'], df_covariance_returns),\n        'inverse_volatility': risk_contributions(df_weights['inverse_volatility'], df_covariance_returns),\n        'risk_parity': risk_contributions(df_weights['risk_parity'], df_covariance_returns),\n    })\ndf_risk_contributions\n\n\n\n\n\n\n\n\nsymbol\nspy\nequal_weight\ninverse_volatility\nrisk_parity\n\n\n\n\n0\nDBA\n0.0\n0.170063\n0.121254\n0.166660\n\n\n1\nGLD\n-0.0\n0.271574\n0.188357\n0.166691\n\n\n2\nLQD\n-0.0\n0.152007\n0.307252\n0.166669\n\n\n3\nSPY\n1.0\n0.127385\n0.071514\n0.166655\n\n\n4\nTLT\n-0.0\n0.284166\n0.247104\n0.166663\n\n\n5\nUUP\n0.0\n-0.005195\n0.064519\n0.166662\n\n\n\n\n\n\n\nLet’s plots the risk contributions to compare the equal-weight, inverse-volatility, and risk-parity portfolios.\n\ndf_risk_contributions.plot(\n    kind='bar', \n    x='symbol',\n    y=['equal_weight', 'inverse_volatility', 'risk_parity'],\n    ylabel='risk contribution'\n);",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Asset Allocation</span>"
    ]
  },
  {
    "objectID": "chapters/15b_asset_allocation/asset_allocation.html#portfolio-performance-2020-data",
    "href": "chapters/15b_asset_allocation/asset_allocation.html#portfolio-performance-2020-data",
    "title": "16  Asset Allocation",
    "section": "16.3 Portfolio Performance (2020 Data)",
    "text": "16.3 Portfolio Performance (2020 Data)\nNow that we have our portfolios constructed, let’s see how they perform in 2020.\n\n16.3.1 Reading-In Data\nWe begin by reading in price data from 2020 and calculating daily returns for each of the assets.\n\nassets = ['SPY', 'TLT', 'GLD', 'UUP', 'DBA', 'LQD']\ndf_assets_performance = yf.download(\n    assets, start='2019-12-31', end='2021-01-01', auto_adjust=True,\n)\ndf_assets_performance = df_assets_performance['Close']\ndf_assets_performance\n\n[*********************100%***********************]  6 of 6 completed\n\n\n\n\n\n\n\n\nTicker\nDBA\nGLD\nLQD\nSPY\nTLT\nUUP\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2019-12-31\n15.140183\n142.899994\n105.076500\n296.632355\n115.825302\n23.153391\n\n\n2020-01-02\n15.085328\n143.949997\n105.298218\n299.406372\n117.133331\n23.233629\n\n\n2020-01-03\n14.911618\n145.860001\n105.659538\n297.139313\n118.937202\n23.260376\n\n\n2020-01-06\n14.948189\n147.389999\n105.306435\n298.272858\n118.261795\n23.206882\n\n\n2020-01-07\n14.984758\n147.970001\n105.035454\n297.434204\n117.680466\n23.287123\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-12-24\n14.472772\n176.350006\n116.299553\n346.428406\n136.468445\n21.709091\n\n\n2020-12-28\n14.417917\n175.710007\n116.375504\n349.404449\n136.529221\n21.709091\n\n\n2020-12-29\n14.463630\n176.350006\n116.426208\n348.737946\n136.355698\n21.637768\n\n\n2020-12-30\n14.609912\n177.699997\n116.578140\n349.235474\n136.650635\n21.548611\n\n\n2020-12-31\n14.756194\n178.360001\n116.603455\n351.009857\n136.850174\n21.611019\n\n\n\n\n254 rows × 6 columns\n\n\n\n\ndf_returns_performance = df_assets_performance.pct_change().dropna()\ndf_returns_performance\n\n\n\n\n\n\n\nTicker\nDBA\nGLD\nLQD\nSPY\nTLT\nUUP\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n-0.003623\n0.007348\n0.002110\n0.009352\n0.011293\n0.003466\n\n\n2020-01-03\n-0.011515\n0.013269\n0.003431\n-0.007572\n0.015400\n0.001151\n\n\n2020-01-06\n0.002452\n0.010490\n-0.003342\n0.003815\n-0.005679\n-0.002300\n\n\n2020-01-07\n0.002446\n0.003935\n-0.002573\n-0.002812\n-0.004916\n0.003458\n\n\n2020-01-08\n-0.004881\n-0.007502\n-0.001408\n0.005329\n-0.006611\n0.002680\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2020-12-24\n0.003168\n0.003985\n0.003423\n0.003890\n0.003957\n0.000822\n\n\n2020-12-28\n-0.003790\n-0.003629\n0.000653\n0.008591\n0.000445\n0.000000\n\n\n2020-12-29\n0.003171\n0.003642\n0.000436\n-0.001908\n-0.001271\n-0.003285\n\n\n2020-12-30\n0.010114\n0.007655\n0.001305\n0.001427\n0.002163\n-0.004120\n\n\n2020-12-31\n0.010013\n0.003714\n0.000217\n0.005081\n0.001460\n0.002896\n\n\n\n\n253 rows × 6 columns\n\n\n\n\n\n16.3.2 Portfolio Daily Returns\nNow we can calculate the daily returns of each of the weighted portfolios.\n\ndf_performance = \\\n    pd.DataFrame({\n        'spy_ret': (df_weights['spy'].values * df_returns_performance[df_assets_performance.columns]).sum(axis=1),\n        'equal_weight_ret': (df_weights['equal_weight'].values * df_returns_performance[df_assets_performance.columns]).sum(axis=1),\n        'inverse_volatility_ret': (df_weights['inverse_volatility'].values * df_returns_performance[df_assets_performance.columns]).sum(axis=1),\n        'risk_parity_ret': (df_weights['risk_parity'].values * df_returns_performance[df_assets_performance.columns]).sum(axis=1)\n    })\ndf_performance\n\n\n\n\n\n\n\n\nspy_ret\nequal_weight_ret\ninverse_volatility_ret\nrisk_parity_ret\n\n\nDate\n\n\n\n\n\n\n\n\n2020-01-02\n0.009352\n0.004991\n0.004204\n0.004419\n\n\n2020-01-03\n-0.007572\n0.002361\n0.002304\n0.001753\n\n\n2020-01-06\n0.003815\n0.000906\n-0.000230\n0.000270\n\n\n2020-01-07\n-0.002812\n-0.000077\n0.000114\n0.000712\n\n\n2020-01-08\n0.005329\n-0.002065\n-0.001319\n-0.000820\n\n\n...\n...\n...\n...\n...\n\n\n2020-12-24\n0.003890\n0.003208\n0.002877\n0.002684\n\n\n2020-12-28\n0.008591\n0.000378\n0.000271\n0.000372\n\n\n2020-12-29\n-0.001908\n0.000131\n-0.000291\n-0.000573\n\n\n2020-12-30\n0.001427\n0.003091\n0.001796\n0.001453\n\n\n2020-12-31\n0.005081\n0.003897\n0.003208\n0.003586\n\n\n\n\n253 rows × 4 columns\n\n\n\n\n\n16.3.3 Annualized Returns\nNow, let’s calculate the annualized return from the daily returns.\n\ndef annualized_return(ret):\n    return (((1 + ret).prod()) ** (252 / len(ret))) - 1\n\n\ndf_annualized_return = \\\n    pd.DataFrame({\n        'spy': [annualized_return(df_performance['spy_ret'])],\n        'equal_weight': [annualized_return(df_performance['equal_weight_ret'])],\n        'inverse_volatility': [annualized_return(df_performance['inverse_volatility_ret'])],\n        'risk_parity': [annualized_return(df_performance['risk_parity_ret'])],\n    })\ndf_annualized_return\n\n\n\n\n\n\n\n\nspy\nequal_weight\ninverse_volatility\nrisk_parity\n\n\n\n\n0\n0.182529\n0.116983\n0.086656\n0.075005\n\n\n\n\n\n\n\n\n\n16.3.4 Realized Volatility\nThe realized volatility of the portfolios is simply the annualized standard deviation of the daily returns. Notice that the ordering of the realized volatilities of the strategies matches the estimated/forecasted volatilities calculated with 2019 data.\n\ndef realized_vol(ret):\n    return ret.std() * np.sqrt(252)\n\n\n# realized volatility\ndf_realized_vol = \\\n    pd.DataFrame({\n        'spy': [realized_vol(df_performance['spy_ret'])],\n        'equal_weight': [realized_vol(df_performance['equal_weight_ret'])],\n        'inverse_volatility': [realized_vol(df_performance['inverse_volatility_ret'])],\n        'risk_parity': [realized_vol(df_performance['risk_parity_ret'])],\n    })\ndf_realized_vol\n\n\n\n\n\n\n\n\nspy\nequal_weight\ninverse_volatility\nrisk_parity\n\n\n\n\n0\n0.33398\n0.094486\n0.085564\n0.079952\n\n\n\n\n\n\n\n\n\n16.3.5 Sharpe-Ratio\nNext, we calculate the Sharpe-ratio of each of the portfolios.\n\ndef sharpe_ratio(ret):\n    return (ret.mean() / ret.std()) * np.sqrt(252)\n\n\ndf_sharpe = \\\n    pd.DataFrame({\n        'spy': [sharpe_ratio(df_performance['spy_ret'])],\n        'equal_weight': [sharpe_ratio(df_performance['equal_weight_ret'])],\n        'inverse_volatility': [sharpe_ratio(df_performance['inverse_volatility_ret'])],\n        'risk_parity': [sharpe_ratio(df_performance['risk_parity_ret'])],\n    })\ndf_sharpe\n\n\n\n\n\n\n\n\nspy\nequal_weight\ninverse_volatility\nrisk_parity\n\n\n\n\n0\n0.670112\n1.218346\n1.014076\n0.944655\n\n\n\n\n\n\n\n\n\n16.3.6 Equity Curve\nWe can also calculate and plot the equity curve of the three strategies.\n\n# equity curve\ndf_performance['spy_equity'] = (1 + df_performance['spy_ret']).cumprod()\ndf_performance['equal_weight_equity'] = (1 + df_performance['equal_weight_ret']).cumprod()\ndf_performance['inverse_volatility_equity'] = (1 + df_performance['inverse_volatility_ret']).cumprod()\ndf_performance['risk_parity_equity'] = (1 + df_performance['risk_parity_ret']).cumprod()\ndf_performance.head().T\n\n\n\n\n\n\n\nDate\n2020-01-02\n2020-01-03\n2020-01-06\n2020-01-07\n2020-01-08\n\n\n\n\nspy_ret\n0.009352\n-0.007572\n0.003815\n-0.002812\n0.005329\n\n\nequal_weight_ret\n0.004991\n0.002361\n0.000906\n-0.000077\n-0.002065\n\n\ninverse_volatility_ret\n0.004204\n0.002304\n-0.000230\n0.000114\n-0.001319\n\n\nrisk_parity_ret\n0.004419\n0.001753\n0.000270\n0.000712\n-0.000820\n\n\nspy_equity\n1.009352\n1.001709\n1.005530\n1.002703\n1.008047\n\n\nequal_weight_equity\n1.004991\n1.007363\n1.008276\n1.008199\n1.006116\n\n\ninverse_volatility_equity\n1.004204\n1.006517\n1.006286\n1.006400\n1.005073\n\n\nrisk_parity_equity\n1.004419\n1.006181\n1.006452\n1.007169\n1.006342\n\n\n\n\n\n\n\n\ndf_performance.plot(\n    y=['spy_equity', 'equal_weight_equity', 'inverse_volatility_equity', 'risk_parity_equity'],\n    ylabel='equity curve',\n    grid=True,\n);\n\n\n\n\n\n\n\n\n\n\n16.3.7 Maximum Draw Down\nTo calculate maximum draw down, let’s first calculate the draw downs through time for each of the portofolios.\n\ndef draw_down(equity_curve):\n    return (equity_curve - equity_curve.cummax()) / equity_curve.cummax()\n\n\ndf_performance['spy_draw_down'] = draw_down(df_performance['spy_equity'])\ndf_performance['equal_weight_draw_down'] = draw_down(df_performance['equal_weight_equity'])\ndf_performance['inverse_volatility_draw_down'] = draw_down(df_performance['inverse_volatility_equity'])\ndf_performance['risk_parity_draw_down'] = draw_down(df_performance['risk_parity_equity'])\ndf_performance.head().T # transposed only because it's easier to look at the DataFrame\n\n\n\n\n\n\n\nDate\n2020-01-02\n2020-01-03\n2020-01-06\n2020-01-07\n2020-01-08\n\n\n\n\nspy_ret\n0.009352\n-0.007572\n0.003815\n-0.002812\n0.005329\n\n\nequal_weight_ret\n0.004991\n0.002361\n0.000906\n-0.000077\n-0.002065\n\n\ninverse_volatility_ret\n0.004204\n0.002304\n-0.000230\n0.000114\n-0.001319\n\n\nrisk_parity_ret\n0.004419\n0.001753\n0.000270\n0.000712\n-0.000820\n\n\nspy_equity\n1.009352\n1.001709\n1.005530\n1.002703\n1.008047\n\n\nequal_weight_equity\n1.004991\n1.007363\n1.008276\n1.008199\n1.006116\n\n\ninverse_volatility_equity\n1.004204\n1.006517\n1.006286\n1.006400\n1.005073\n\n\nrisk_parity_equity\n1.004419\n1.006181\n1.006452\n1.007169\n1.006342\n\n\nspy_draw_down\n0.000000\n-0.007572\n-0.003786\n-0.006587\n-0.001293\n\n\nequal_weight_draw_down\n0.000000\n0.000000\n0.000000\n-0.000077\n-0.002142\n\n\ninverse_volatility_draw_down\n0.000000\n0.000000\n-0.000230\n-0.000116\n-0.001435\n\n\nrisk_parity_draw_down\n0.000000\n0.000000\n0.000000\n0.000000\n-0.000820\n\n\n\n\n\n\n\nNow, we can easily calculate the greatest draw downs.\n\n# max draw down\ndf_max_draw_downs = \\\n    pd.DataFrame({\n        'spy': [df_performance['spy_draw_down'].min()],\n        'equal_weight': [df_performance['equal_weight_draw_down'].min()],\n        'inverse_volatility': [df_performance['inverse_volatility_draw_down'].min()],\n        'risk_parity': [df_performance['risk_parity_draw_down'].min()],\n    })\ndf_max_draw_downs\n\n\n\n\n\n\n\n\nspy\nequal_weight\ninverse_volatility\nrisk_parity\n\n\n\n\n0\n-0.337172\n-0.111999\n-0.102974\n-0.089722\n\n\n\n\n\n\n\n\n\n16.3.8 99% DVaR\nThe 99% DVaR can easily be calculated from the daily returns.\n\ndef dvar(ret):\n    return np.percentile(ret, 0.01)\n\n\n# 99% DVaR\ndf_dvar= \\\n    pd.DataFrame({\n        'spy': [dvar(df_performance['spy_ret'])],\n        'equal_weight': [dvar(df_performance['equal_weight_ret'])],\n        'inverse_volatility': [dvar(df_performance['inverse_volatility_ret'])],\n        'risk_parity': [dvar(df_performance['risk_parity_ret'])],\n    })\ndf_dvar\n\n\n\n\n\n\n\n\nspy\nequal_weight\ninverse_volatility\nrisk_parity\n\n\n\n\n0\n-0.109077\n-0.033927\n-0.029064\n-0.028557",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Asset Allocation</span>"
    ]
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html",
    "href": "chapters/16_trend_following/trend_following.html",
    "title": "17  Trend Following",
    "section": "",
    "text": "17.1 Importing Packages\nThis chapter demonstrates a simple trend-following strategy using Yahoo Finance data. The methodology is inspired by the Meb Faber’s paper A Quantitative Approach to Tactical Asset Allocation.\nThe basic workflow to use this notebook is to set the parameters in the Setting Parameters section and then to run the Restart Kernel and Run All Cells... command in the Kernel dropdown menu.\nThe strategy we implement is quite straight forward. On each trading day, we check if the current price is above or below the trailing simple moving average (SMA) of the prices (the number of days in the moving average is a parameter). If the current price is above the SMA we go long the asset, if it is below the SMA we hold no position. We compare this simple strategy to the buying and holding the asset, i.e. always being long the asset.\nOne major difference between our strategy and the one that is used in the aforementioned paper is that in our strategy, positions are recalculated on a daily basis. In the paper, positions are recalculated at the end of each month, and then held fixed for the duration of the following month. This will yield some differences in our results and the results in the paper.\nAt the end of this chapter we analyze a particular case study in which we try to recreate Figure 9 from Faber’s paper.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport yfinance as yf",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trend Following</span>"
    ]
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#setting-parameters",
    "href": "chapters/16_trend_following/trend_following.html#setting-parameters",
    "title": "17  Trend Following",
    "section": "17.2 Setting Parameters",
    "text": "17.2 Setting Parameters\nNext we set the parameters of our analysis. The parameter sma_days determines the number of days used in the trailing simple moving average.\n\nticker = '^SP500TR'\nstart_date = '1989-12-29'\nend_date = '2012-01-01'\nsma_days = 200",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trend Following</span>"
    ]
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#reading-in-data",
    "href": "chapters/16_trend_following/trend_following.html#reading-in-data",
    "title": "17  Trend Following",
    "section": "17.3 Reading-In Data",
    "text": "17.3 Reading-In Data\nWe now read-in our data from Yahoo Finance.\n\ndf_asset = yf.download(ticker, auto_adjust=False)\ndf_asset = df_asset.droplevel(level=1, axis=1)\ndf_asset.reset_index(inplace=True)\ndf_asset.columns = df_asset.columns.str.lower().str.replace(' ','_')\ndf_asset = df_asset[['date', 'adj_close']].copy()\ndf_asset['sma'] = df_asset['adj_close'].rolling(sma_days).mean()\ndf_asset = df_asset.query('date &gt;= @start_date and date &lt;= @end_date').copy()\ndf_asset.plot(x='date', y=['adj_close', 'sma'], grid=True, title='Adjusted Close and Moving Average Through Time');\n\n[*********************100%***********************]  1 of 1 completed",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trend Following</span>"
    ]
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#determining-position-based-on-trend",
    "href": "chapters/16_trend_following/trend_following.html#determining-position-based-on-trend",
    "title": "17  Trend Following",
    "section": "17.4 Determining Position Based on Trend",
    "text": "17.4 Determining Position Based on Trend\nHere we define the function that will help to determine our daily position in the asset.\n\ndef calc_position(row):\n    adj_close = row['adj_close']\n    sma = row['sma']\n\n    position = 0\n    if sma &lt; adj_close:\n        position = 1\n\n    return position\n\nLet’s now use the DataFrame.apply() method to calculate all the positions through time.\n\ndf_asset['trend_position'] = df_asset.apply(calc_position, axis = 1)\ndf_asset\n\n\n\n\n\n\n\nPrice\ndate\nadj_close\nsma\ntrend_position\n\n\n\n\n504\n1989-12-29\n379.410004\n351.375200\n1\n\n\n505\n1990-01-02\n386.160004\n351.776050\n1\n\n\n506\n1990-01-03\n385.170013\n352.186400\n1\n\n\n507\n1990-01-04\n382.019989\n352.573600\n1\n\n\n508\n1990-01-05\n378.299988\n352.946600\n1\n\n\n...\n...\n...\n...\n...\n\n\n6047\n2011-12-23\n2171.500000\n2141.526601\n1\n\n\n6048\n2011-12-27\n2171.709961\n2141.442350\n1\n\n\n6049\n2011-12-28\n2145.090088\n2141.347501\n1\n\n\n6050\n2011-12-29\n2168.120117\n2141.578401\n1\n\n\n6051\n2011-12-30\n2158.939941\n2141.620952\n1\n\n\n\n\n5548 rows × 4 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trend Following</span>"
    ]
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#calculating-returns-equity-curves-and-drawdowns",
    "href": "chapters/16_trend_following/trend_following.html#calculating-returns-equity-curves-and-drawdowns",
    "title": "17  Trend Following",
    "section": "17.5 Calculating Returns, Equity Curves, and Drawdowns",
    "text": "17.5 Calculating Returns, Equity Curves, and Drawdowns\nWe now have all the data the we need to calculate daily returns, the equity curves, and drawdowns of the two strategies.\n\n# returns\ndf_asset['buy_hold_return'] = df_asset['adj_close'].pct_change()\ndf_asset['trend_return'] = df_asset['buy_hold_return'] * df_asset['trend_position'].shift(1)\n\n# growth factors\ndf_asset['buy_hold_factor'] = 1 + df_asset['buy_hold_return']\ndf_asset['trend_factor'] = 1 + df_asset['trend_return']\n\n# equity curves\ndf_asset['buy_hold_equity'] = df_asset['buy_hold_factor'].cumprod()\ndf_asset['trend_equity'] = df_asset['trend_factor'].cumprod()\n\n# maximum cumulative equity\ndf_asset['buy_hold_max_equity'] = df_asset['buy_hold_equity'].cummax()\ndf_asset['trend_max_equity'] = df_asset['trend_equity'].cummax()\n\n# draw-down\ndf_asset['buy_hold_drawdown'] = (df_asset['buy_hold_equity'] - df_asset['buy_hold_max_equity']) / df_asset['buy_hold_max_equity']\ndf_asset['trend_drawdown'] = (df_asset['trend_equity'] - df_asset['trend_max_equity']) / df_asset['trend_max_equity']\n\n# graphing equity curves\ndf_asset.plot(x='date', y=['buy_hold_equity','trend_equity'], grid=True, title='Equity Graph');",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trend Following</span>"
    ]
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#return-characteristics",
    "href": "chapters/16_trend_following/trend_following.html#return-characteristics",
    "title": "17  Trend Following",
    "section": "17.6 Return Characteristics",
    "text": "17.6 Return Characteristics\nFinally, we calculate some basic performance metrics.\nAnnualized Return\n\nprint('buy-hold return: ', np.round(df_asset['buy_hold_equity'].iloc[-1] ** (252 / (len(df_asset) - 1)) - 1, 3) * 100, '%')\nprint('trend return:    ', np.round(df_asset['trend_equity'].iloc[-1] ** (252 / (len(df_asset) - 1)) - 1, 3) * 100, '%')\n\nbuy-hold return:  8.200000000000001 %\ntrend return:     7.199999999999999 %\n\n\nSharpe-Ratio\n\nprint('buy-hold sharpe-ratio: ', np.round((np.mean(df_asset['buy_hold_return']) / np.std(df_asset['buy_hold_return'])) * np.sqrt(252), 2))\nprint('trend sharpe-ratio:    ', np.round((np.mean(df_asset['trend_return']) / np.std(df_asset['trend_return'])) * np.sqrt(252), 2))\n\nbuy-hold sharpe-ratio:  0.51\ntrend sharpe-ratio:     0.66\n\n\nMaximum Drawdown\n\nprint('buy-hold max-drawdown: ', np.round(np.min(df_asset['buy_hold_drawdown']), 2))\nprint('trend max-drawdown:    ', np.round(np.min(df_asset['trend_drawdown']), 2))\n\nbuy-hold max-drawdown:  -0.55\ntrend max-drawdown:     -0.24",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trend Following</span>"
    ]
  },
  {
    "objectID": "chapters/16_trend_following/trend_following.html#case-study-sp500-1990-2012",
    "href": "chapters/16_trend_following/trend_following.html#case-study-sp500-1990-2012",
    "title": "17  Trend Following",
    "section": "17.7 Case Study: S&P500 1990-2012",
    "text": "17.7 Case Study: S&P500 1990-2012\nLet’s now take a look at a particular case study of the S&P500 in the period of 1990-2012. We will use the Total Returns futures to represent an investment in the S&P500. In order to get the result we describe, rerun this notebook with the following inputs:\nticker = '^SP500TR'\nstart_date = '1989-12-29'\nend_date = '2012-01-01'\nsma_days = 200\nWe first compare our equity graph above to Faber’s equity graph which we present below:\n\nAs we can see in Faber’s analysis, trend-following slightly outperforms the buy-and-hold strategy, while in our analysis the trend-following strategy underperforms. This difference is likely due to the difference in rebalance frequency. Our strategy rebalances daily (which is probably too much) while Faber’s rebalances monthly.\nAnalyzing our results more closely, we see that buy-and-hold has an annualized return of 8.2%, while trend-following has an annualized return of 7.2%. There is, however, significantly less downside risk with trend-following, as the strategy tends to sit-out bear markets (e.g. 2000 and 2008). This results in a max-drawdown of -24% for trend following, while buy-and-hold had a max-drawdown of -55% during the 2008 financial crisis. This reduced downside risk can also be seen in the Sharpe-ratio, a metric by which trend-following (0.66) outperforms buy-and-hold (0.51).",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Trend Following</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "",
    "text": "18.1 Loading Packages\nIn this chapter, we will use the skills that we have learned so far to analyze some mutual fund data. In particular, we have end of day NAVs from two different funds, called fund_1 and fund_2, as well as benchmark index, called benchmark. The data spans from 2016Q1 to 2019Q3.\nIn our analysis we will do the following:\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#reading-in-data",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#reading-in-data",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.2 Reading-In Data",
    "text": "18.2 Reading-In Data\nNext, we’ll read-in the data, rename the columns, and convert the trade_date column to datetime.\n\n# reading-in data\ndf_px_raw = pd.read_csv('mutual_fund_data.csv')\n# renaming columns\ndf_px_raw.rename(\n    columns={'Date':'trade_date', 'BM':'benchmark', 'Fund_1':'fund_1', 'Fund_2':'fund_2'}\n    , inplace=True)\n# converting data-type of trade_date\ndf_px_raw['trade_date'] = pd.to_datetime(df_px_raw['trade_date'])\n\ndf_px_raw.head()\n\n\n\n\n\n\n\n\ntrade_date\nbenchmark\nfund_1\nfund_2\n\n\n\n\n0\n2015-12-31\n203.8700\n108.29\n254.04\n\n\n1\n2016-01-01\n203.8700\n108.29\n254.04\n\n\n2\n2016-01-04\n201.0192\n106.61\n250.82\n\n\n3\n2016-01-05\n201.3600\n106.10\n250.90\n\n\n4\n2016-01-06\n198.8200\n104.66\n247.36",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#visualizing-raw-price-data",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#visualizing-raw-price-data",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.3 Visualizing Raw Price Data",
    "text": "18.3 Visualizing Raw Price Data\nThe pandas package was created by Wes McKinney when he was a quant at the hedge fund AQR. Because of this, pandas was built with a lot of functionality that is particularly useful for working with financial timeseries. This includes some rudimentary plotting functionality, which is built on top of the package matplotlib.\nLet’s quickly plot the price series for our three funds using the DataFrame.plot() method.\n\ndf_px_raw.plot(\n    x='trade_date', \n    y=['benchmark', 'fund_1', 'fund_2'],\n    figsize=(8,5),\n    title='Daily Fund Prices: 2016Q1-2019Q3',\n    grid = True\n);\n\n\n\n\n\n\n\n\n\nCode Challenge: Copy and paste the code above and try setting subplots=True.\n\n\nSolution\ndf_px_raw.plot(\n    x='trade_date', \n    y=['benchmark', 'fund_1', 'fund_2'],\n    figsize=(8,8),\n    title='Fund Prices: 2016Q1-2019Q3',\n    grid = True,\n    subplots = True,\n);\nplt.subplots_adjust(top=0.94);",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#tidying-the-data",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#tidying-the-data",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.4 Tidying the Data",
    "text": "18.4 Tidying the Data\nOur data, in its current form, is not tidy. In order for data to be tidy, each row should be a single observation. Currently, each row of df_px_raw is three price observations - one for each of the three funds - which will not allow for easy use of the .groupby() function.\nWe can use the pandas.melt() method to tidy our data.\n\ndf_px = \\\n    pd.melt(\n        df_px_raw, \n        id_vars = ['trade_date'],\n        value_vars = ['benchmark', 'fund_1', 'fund_2'], \n        var_name = 'symbol', \n        value_name = 'close',\n    )\n\ndf_px.head()\n\n\n\n\n\n\n\n\ntrade_date\nsymbol\nclose\n\n\n\n\n0\n2015-12-31\nbenchmark\n203.8700\n\n\n1\n2016-01-01\nbenchmark\n203.8700\n\n\n2\n2016-01-04\nbenchmark\n201.0192\n\n\n3\n2016-01-05\nbenchmark\n201.3600\n\n\n4\n2016-01-06\nbenchmark\n198.8200\n\n\n\n\n\n\n\nOur analysis will involve calculating quarterly statistics, so let’s add columns year, quarter, and month.\n\ndf_px['year'] = df_px['trade_date'].dt.year\ndf_px['quarter'] = df_px['trade_date'].dt.quarter\ndf_px['month'] = df_px['trade_date'].dt.month\ndf_px.head()\n\n\n\n\n\n\n\n\ntrade_date\nsymbol\nclose\nyear\nquarter\nmonth\n\n\n\n\n0\n2015-12-31\nbenchmark\n203.8700\n2015\n4\n12\n\n\n1\n2016-01-01\nbenchmark\n203.8700\n2016\n1\n1\n\n\n2\n2016-01-04\nbenchmark\n201.0192\n2016\n1\n1\n\n\n3\n2016-01-05\nbenchmark\n201.3600\n2016\n1\n1\n\n\n4\n2016-01-06\nbenchmark\n198.8200\n2016\n1\n1\n\n\n\n\n\n\n\nLet’s also rearrange columns to make our table a little more human readable.\n\ndf_px = df_px[['symbol','trade_date', 'year', 'quarter', 'month',  'close',]]\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#visualizing-the-tidy-price-data-with-seaborn",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#visualizing-the-tidy-price-data-with-seaborn",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.5 Visualizing the Tidy Price Data with seaborn",
    "text": "18.5 Visualizing the Tidy Price Data with seaborn\nLet’s now import the seaborn package, which a popular visualization package that is also built on top of matplotlib.\n\nimport seaborn as sns\nsns.set()\n\nIn the code below, we use sns.relplot() to graph the three fund price series using our data in its tidy form. Notice that by setting hue='symbol', the sns.relplot() function knows to graph the close prices for each of the three symbols separately and with three different colors.\n\ng = \\\nsns.relplot(\n    data = df_px, kind = 'line',\n    x = 'trade_date', y = 'close', hue = 'symbol',\n    aspect = 1.5,\n);\n\n# creating and tweaking the title\ng.fig.suptitle('Daily Fund Close Prices: 2016Q1-2019Q3')\nplt.subplots_adjust(top=0.93);",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#calculating-daily-returns-1",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#calculating-daily-returns-1",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.6 Calculating Daily Returns (#1)",
    "text": "18.6 Calculating Daily Returns (#1)\nNotice that our data does not contain daily returns.\n\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n\n\n\n\n\n\n\nNow that our data is tidy, we can easily obtain daily log-returns using a grouped calculation.\nWe use log-returns because, they are very close in value to simple returns, and multi-day log-returns are easily calculated as sums of single-day returns.\n\ndf_px['daily_ret'] = \\\n    np.log(df_px['close']).groupby(df_px['symbol']).diff() #log returns\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\nNaN\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n0.000000\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n-0.014082\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n0.001694\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n-0.012694\n\n\n\n\n\n\n\n\nCode Challenge: Verify that all the log-returns on the first date in the data set are NaN.\n\n\nSolution\ndf_px[df_px['trade_date'] == np.min(df_px['trade_date'])]\n\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.87\nNaN\n\n\n978\nfund_1\n2015-12-31\n2015\n4\n12\n108.29\nNaN\n\n\n1956\nfund_2\n2015-12-31\n2015\n4\n12\n254.04\nNaN",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#rolling-252-day-return-2",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#rolling-252-day-return-2",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.7 Rolling 252-day Return (#2)",
    "text": "18.7 Rolling 252-day Return (#2)\nNow that we have daily log-returns, we can easily calculate a rolling 252-day return by using the .rolling() method.\n\ndf_px['ret_252'] = \\\n    df_px['daily_ret'].groupby(df_px['symbol']).rolling(252).sum().values\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\nret_252\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\nNaN\nNaN\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n0.000000\nNaN\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n-0.014082\nNaN\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n0.001694\nNaN\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n-0.012694\nNaN\n\n\n\n\n\n\n\nNext we’ll use sns.FacetGrid() to graph three separate subplots of the rolling returns.\n\ng = sns.FacetGrid(df_px, col='symbol', aspect=1.25, height=3,)\ng.map(plt.plot, 'trade_date', 'ret_252', alpha=0.7,)\ng.add_legend();\ng.set_xticklabels(rotation=35, horizontalalignment='right');\n\n\n\n\n\n\n\n\n\nCode Challenge: Copy and paste the code above, and try changing col='symbol' to row='symbol'.\n\n\nSolution\ng = sns.FacetGrid(df_px, row='symbol', aspect=1.25, height=3)\ng.map(plt.plot, 'trade_date', 'ret_252', alpha=0.7, )\ng.add_legend();",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#rolling-252-day-volatility-3",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#rolling-252-day-volatility-3",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.8 Rolling 252-day Volatility (#3)",
    "text": "18.8 Rolling 252-day Volatility (#3)\nWe can calculate a rolling 252-day volatility in a similar fashion as the rolling returns.\n\ndf_px['vol_252'] = \\\n    df_px['daily_ret'].groupby(df_px['symbol']).rolling(252).std().values * np.sqrt(252)\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\nret_252\nvol_252\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\nNaN\nNaN\nNaN\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n0.000000\nNaN\nNaN\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n-0.014082\nNaN\nNaN\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n0.001694\nNaN\nNaN\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n-0.012694\nNaN\nNaN\n\n\n\n\n\n\n\nThis code creates a seaborn.FacetGrid() of the rolling volatilities.\n\ng = sns.FacetGrid(df_px, col='symbol', aspect=1.25, height=3,)\ng.map(plt.plot, 'trade_date', 'vol_252', alpha=0.7)\ng.set_xticklabels(rotation=35, horizontalalignment='right');\ng.add_legend();\n\n\n\n\n\n\n\n\nThe following code uses pandas to graph the rolling returns and volatility on the same plot for fund_1.\n\n# filtering conditions\nbln_symbol = df_px['symbol'] == 'fund_1'\nbln_nans = ~(np.isnan(df_px['ret_252']))\nbln_filter = bln_symbol & bln_nans\n\n# graphing\ndf_px[bln_filter].plot(x='trade_date', y=['ret_252', 'vol_252']);\n\n\n\n\n\n\n\n\nCode Challenge Copy and paste the above code and modify it to graph only the data from the beginning of 2019 and onwards.\n\n\nSolution\nbln_symbol = df_px['symbol'] == 'fund_1'\nbln_date = df_px['trade_date'] &gt;= '2019-01-01'\nbln_filter = bln_symbol & bln_date\n\ndf_px[bln_filter].plot(x='trade_date', y=['ret_252', 'vol_252']);",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#quarterly-returns-and-excess-return-4",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#quarterly-returns-and-excess-return-4",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.9 Quarterly Returns and Excess Return (#4)",
    "text": "18.9 Quarterly Returns and Excess Return (#4)\nOur next analysis objective is to calculate quarterly excess returns for fund_1 and fund_2 relative the benchmark.\nAs we can see, df_px consist of daily data.\n\ndf_px.head()\n\n\n\n\n\n\n\n\nsymbol\ntrade_date\nyear\nquarter\nmonth\nclose\ndaily_ret\nret_252\nvol_252\n\n\n\n\n0\nbenchmark\n2015-12-31\n2015\n4\n12\n203.8700\nNaN\nNaN\nNaN\n\n\n1\nbenchmark\n2016-01-01\n2016\n1\n1\n203.8700\n0.000000\nNaN\nNaN\n\n\n2\nbenchmark\n2016-01-04\n2016\n1\n1\n201.0192\n-0.014082\nNaN\nNaN\n\n\n3\nbenchmark\n2016-01-05\n2016\n1\n1\n201.3600\n0.001694\nNaN\nNaN\n\n\n4\nbenchmark\n2016-01-06\n2016\n1\n1\n198.8200\n-0.012694\nNaN\nNaN\n\n\n\n\n\n\n\nCalculating the quarterly returns is a straight-forward application of .groupby().agg().\n\ndf_quarter = \\\n    df_px[df_px.year &gt; 2015] \\\n        .groupby(['symbol', 'year', 'quarter'])['daily_ret'].agg([np.sum]).reset_index() \\\n        .rename(columns={'sum':'quarterly_ret'})\n\ndf_quarter.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nquarter\nquarterly_ret\n\n\n\n\n0\nbenchmark\n2016\n1\n0.008061\n\n\n1\nbenchmark\n2016\n2\n0.019061\n\n\n2\nbenchmark\n2016\n3\n0.032062\n\n\n3\nbenchmark\n2016\n4\n0.032879\n\n\n4\nbenchmark\n2017\n1\n0.053184\n\n\n\n\n\n\n\nSince we ultimately want to calculate excess return relative to the benchmark, it would be helpful to have the benchmark returns as a separate column in df_quarter.\nIn order to do this, let’s first separate out the benchmark quarterly returns into a separate DataFrame called df_bench.\n\ndf_bench = \\\n    df_quarter[df_quarter['symbol'] == 'benchmark'] \\\n    [['year', 'quarter', 'quarterly_ret']] \\\n    .rename(columns={'quarterly_ret':'bench_ret'})\n\ndf_bench.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nbench_ret\n\n\n\n\n0\n2016\n1\n0.008061\n\n\n1\n2016\n2\n0.019061\n\n\n2\n2016\n3\n0.032062\n\n\n3\n2016\n4\n0.032879\n\n\n4\n2017\n1\n0.053184\n\n\n\n\n\n\n\nIn order to add the bench_ret column to df_quarter we will utilize a left-join.\n\ndf_excess = \\\n    pd.merge(\n        df_quarter, df_bench, how='left',\n        left_on=['year','quarter'], right_on=['year', 'quarter']\n    )\n\ndf_excess.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nquarter\nquarterly_ret\nbench_ret\n\n\n\n\n0\nbenchmark\n2016\n1\n0.008061\n0.008061\n\n\n1\nbenchmark\n2016\n2\n0.019061\n0.019061\n\n\n2\nbenchmark\n2016\n3\n0.032062\n0.032062\n\n\n3\nbenchmark\n2016\n4\n0.032879\n0.032879\n\n\n4\nbenchmark\n2017\n1\n0.053184\n0.053184\n\n\n\n\n\n\n\nFinally, we can caluclate excess return for each of the funds.\n\ndf_excess['excess_ret'] = df_excess['quarterly_ret'] - df_excess['bench_ret']\ndf_excess['year_quarter'] = df_excess['year'] * 100 + df_excess['quarter']\ndf_excess = df_excess[['symbol', 'year', 'quarter', 'year_quarter', 'quarterly_ret', 'bench_ret', 'excess_ret']]\ndf_excess.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\nquarter\nyear_quarter\nquarterly_ret\nbench_ret\nexcess_ret\n\n\n\n\n0\nbenchmark\n2016\n1\n201601\n0.008061\n0.008061\n0.0\n\n\n1\nbenchmark\n2016\n2\n201602\n0.019061\n0.019061\n0.0\n\n\n2\nbenchmark\n2016\n3\n201603\n0.032062\n0.032062\n0.0\n\n\n3\nbenchmark\n2016\n4\n201604\n0.032879\n0.032879\n0.0\n\n\n4\nbenchmark\n2017\n1\n201701\n0.053184\n0.053184\n0.0\n\n\n\n\n\n\n\nNext, we create a bar-plot with of the three funds quarterly returns.\n\ng = \\\n    sns.catplot(\n        data=df_excess, x=\"year_quarter\", y=\"quarterly_ret\",\n        hue=\"symbol\", kind=\"bar\",\n        aspect= 1.65, \n    )\ng.fig.suptitle('Fund Quarterly Returns')\ng.set_xticklabels(rotation=35, horizontalalignment='right')\nplt.subplots_adjust(top=0.93);\n\n\n\n\n\n\n\n\nAnd finally, we create a bar-plot of the excess returns of fund_1 and fund_2.\n\ng = \\\n    sns.catplot(\n        data=df_excess[df_excess.symbol != 'benchmark'],\n        x=\"year_quarter\", y=\"excess_ret\", \n        hue=\"symbol\", kind=\"bar\",\n        aspect=1.5\n    );\ng.fig.suptitle('Quarterly Excess Returns: fund_1 & fund_2')\ng.set_xticklabels(rotation=35, horizontalalignment='right')\nplt.subplots_adjust(top=0.93);",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#wrangling-returns-data",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#wrangling-returns-data",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.10 Wrangling Returns Data",
    "text": "18.10 Wrangling Returns Data\nFor the remainder of the tutorial, will be helpful to have an untidy DataFrame with the daily returns for the three funds in separate columns.\nLet’s first separate out the returns for each fund into their own DataFrames.\n\ncols = ['trade_date', 'daily_ret']\ndf_bench = \\\n    df_px[df_px.symbol == 'benchmark'][cols].copy().rename(columns={'daily_ret':'benchmark'})\ndf_fund1 = \\\n    df_px[df_px.symbol == 'fund_1'][cols].copy().rename(columns={'daily_ret':'fund_1'})\ndf_fund2 = \\\n    df_px[df_px.symbol == 'fund_2'][cols].copy().rename(columns={'daily_ret':'fund_2'})\n\nNext, we left-join these three DataFrames together into a single variable called df_ret.\n\ndf_ret = \\\n    df_bench \\\n        .merge(right=df_fund1, how='left', left_on='trade_date', right_on='trade_date') \\\n        .merge(right=df_fund2, how='left', left_on='trade_date', right_on='trade_date') \\\n        .query('trade_date &gt; \"2015-12-31\"')\ndf_ret.head()\n\n\n\n\n\n\n\n\ntrade_date\nbenchmark\nfund_1\nfund_2\n\n\n\n\n1\n2016-01-01\n0.000000\n0.000000\n0.000000\n\n\n2\n2016-01-04\n-0.014082\n-0.015635\n-0.012756\n\n\n3\n2016-01-05\n0.001694\n-0.004795\n0.000319\n\n\n4\n2016-01-06\n-0.012694\n-0.013665\n-0.014210\n\n\n5\n2016-01-07\n-0.024284\n-0.032729\n-0.025882",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#pair-plots-and-correlation-matrix-5",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#pair-plots-and-correlation-matrix-5",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.11 Pair-Plots and Correlation Matrix (#5)",
    "text": "18.11 Pair-Plots and Correlation Matrix (#5)\nIn a subsequent section, we will perform regressions on the two most recent years of data.\nPrior to modeling, I find it useful to look at scatter plots and correlations.\nAs a first step, let’s programmatically calculate the start date for our analysis. We will utilize the numpy.timedelta data structure for this purpose.\n\ndt_curr = df_ret.sort_values(['trade_date'])[-1:]['trade_date'].values[0]\ndt_start = dt_curr - np.timedelta64(730, 'D')\n\nprint(dt_curr)\nprint(dt_start)\n\n2019-09-30T00:00:00.000000000\n2017-09-30T00:00:00.000000000\n\n\nNext, we use the seaborn.pairplot() function to quickly graph all the pairwise correlatior for two years of fund data.\n\ncols = ['benchmark', 'fund_1', 'fund_2']\nsns.pairplot(\n    df_ret[df_ret['trade_date'] &gt; dt_start][cols]\n);\n\n\n\n\n\n\n\n\nClearly all three of our returns are highly correlated.\nLet’s use the DataFrame.corr() method to calculate the correlations explicitly.\n\ncols = ['benchmark', 'fund_1', 'fund_2']\ndf_ret[df_ret['trade_date'] &gt; dt_start][cols].corr()\n\n\n\n\n\n\n\n\nbenchmark\nfund_1\nfund_2\n\n\n\n\nbenchmark\n1.000000\n0.927068\n0.919859\n\n\nfund_1\n0.927068\n1.000000\n0.836144\n\n\nfund_2\n0.919859\n0.836144\n1.000000\n\n\n\n\n\n\n\n\nCode Challenge: Copy and paste the above correlation code and modify it calculate the correlations for the entirety of the data set.\n\n\nSolution\ncols = ['benchmark', 'fund_1', 'fund_2']\ndf_ret[cols].corr()\n\n\n\n\n\n\n\n\n\nbenchmark\nfund_1\nfund_2\n\n\n\n\nbenchmark\n1.000000\n0.914292\n0.914148\n\n\nfund_1\n0.914292\n1.000000\n0.812403\n\n\nfund_2\n0.914148\n0.812403\n1.000000",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#regression-of-funds-against-benchmark-6-and-7",
    "href": "chapters/17_regression_mutual_fund/regression_mutual_fund.html#regression-of-funds-against-benchmark-6-and-7",
    "title": "18  Regression: Mutual Fund Analysis",
    "section": "18.12 Regression of Funds against Benchmark (#6 and #7)",
    "text": "18.12 Regression of Funds against Benchmark (#6 and #7)\nFinally, we will fit regressions of fund_1 and fund_2 against the benchmark.\nIn order to do this, we will utilize the sklearn package which is extremely useful for implementing a variety of machine learning techniques.\nLet’s begin by importing the LinearRegression() constructor function from sklearn.\n\nfrom sklearn.linear_model import LinearRegression\n\nWe again want to restrict our analysis to the most recent two years, so let’s recalculate dt_start just for good measure.\n\ndt_curr = df_ret.sort_values(['trade_date'])[-1:]['trade_date'].values[0]\ndt_start = dt_curr - np.timedelta64(730, 'D')\n\n\n18.12.1 Fund 1\nLet’s begin by regressing fund_1 against the benchmark.\nAs a preliminary step, let’s graph the scatter plot of the returns series.\n\ndf_ret \\\n    [df_ret.trade_date &gt; dt_start] \\\n    .plot.scatter('benchmark', 'fund_1', c='k', figsize=(6, 4));\n\n\n\n\n\n\n\n\nThe first step in using sklearn for a regression analysis is to instantiate a regression model object as assign it to a variable, which we will call reg_fund1.\n\nreg_fund1 = LinearRegression(fit_intercept=True)\n\nNext, let’s separate out the benchmark returns and the fund_1 returns into their own DataFrames.\n\ndf_bm = df_ret[['benchmark']][df_ret.trade_date &gt; dt_start]\ndf_fund1 = df_ret[['fund_1']][df_ret.trade_date &gt; dt_start]\n\nTo fit the regression we call the .fit() method of our model object reg_fund1.\n\nreg_fund1.fit(X = df_bm, y = df_fund1)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nThe coefficients \\((\\beta)\\) and the y-intercept \\((\\alpha)\\) are attributes of reg_fund1 which we can access as follows:\n\nprint(\"beta:  \" + str(np.round(reg_fund1.coef_[0, 0], 4)))\nprint(\"alpha: \" + str(np.round(reg_fund1.intercept_[0], 4)))\n\nbeta:  1.2982\nalpha: 0.0003\n\n\nEvery class of machine learning model has a .score() method, which gives some kind of accuracy measure. For LinearRegression the .score() gives the \\(R^2\\).\n\nreg_fund1.score(df_ret[['benchmark']], df_ret[['fund_1']])\n\n0.8349465372078845\n\n\nLet’s use pandas plotting along with matplotlib.pyplot to graph our regression line along with the scatter plot of the data:\n\nxfit = np.linspace(-0.05, 0.05, 100)           # range of line\nyfit = reg_fund1.predict(xfit[:, np.newaxis])  # model values in range\n\n\ndf_ret \\\n    [df_ret.trade_date &gt; dt_start] \\\n    .plot.scatter('benchmark', 'fund_1', c='k', figsize=(6, 4));\nplt.plot(xfit, yfit);\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n18.12.2 Fund 2\nLet’s implement the same kind of regression analysis for fund_2.\nScatter Plot of fund_2 vs benchmark\n\ndf_ret\\\n    [df_ret.trade_date &gt; dt_start]\\\n    .plot.scatter('benchmark', 'fund_2', c='k', figsize=(6, 4));\n\n\n\n\n\n\n\n\nInstantiate a LinearRegression Object\n\nreg_fund2 = LinearRegression(fit_intercept=True)\n\nSeparate out Features and Labels for Regression Analysis\n\ndf_bm = df_ret[['benchmark']][df_ret.trade_date &gt; dt_start]\ndf_fund2 = df_ret[['fund_2']][df_ret.trade_date &gt; dt_start]\n\nFit the Regression Model\n\nreg_fund2.fit(X = df_bm, y = df_fund2)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nPrint Coefficients\n\nprint(reg_fund2.coef_)\nprint(reg_fund2.intercept_)\n\n[[0.94373491]]\n[-0.00015386]\n\n\nCheck the \\(R^2\\)\n\nreg_fund2.score(df_ret[['benchmark']], df_ret[['fund_1']])\n\n0.7823936517887883\n\n\nPlot the Fitted Regression Line\n\nxfit = np.linspace(-0.05, 0.05, 100)           # range of line\nyfit = reg_fund2.predict(xfit[:, np.newaxis])  # model values in range\n\n\ndf_ret \\\n    [df_ret.trade_date &gt; dt_start] \\\n    .plot.scatter('benchmark', 'fund_2', c='k', figsize=(6, 4));\nplt.plot(xfit, yfit);\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regression: Mutual Fund Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html",
    "href": "chapters/18_close_to_close/close_to_close.html",
    "title": "19  Volatility Forecasting: Close-to-Close Estimator",
    "section": "",
    "text": "19.1 Loading Packages\nThis material is closely related to a the following white papers:\nOur main objective will be to implement the code for the close-to-close volatility estimator. To test our work, we will attempt to replicate Sepp’s results for weekly volatility forecasts for SPY (see pp 38-43).\nFor the related project you will be asked to implement code for other estimators and a number of ETFs.\nLet’s begin by loading the packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport sklearn\npd.options.display.max_rows = 10",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Volatility Forecasting: Close-to-Close Estimator</span>"
    ]
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#reading-in-spy-data-from-yahoo-finance",
    "href": "chapters/18_close_to_close/close_to_close.html#reading-in-spy-data-from-yahoo-finance",
    "title": "19  Volatility Forecasting: Close-to-Close Estimator",
    "section": "19.2 Reading-In SPY Data From Yahoo Finance",
    "text": "19.2 Reading-In SPY Data From Yahoo Finance\nSepp’s analysis covers data starting from 1/1/2005 and ending on 4/2/2016. Let’s grab these SPY prices from Yahoo Finance using pandas_datareader.\n\ndf_spy = yf.download(\n    'SPY', start = '2004-12-31', end = '2016-04-02', auto_adjust=False\n).reset_index()\ndf_spy = df_spy.droplevel(level=1, axis=1)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.rename(columns = {'date':'trade_date'}, inplace = True)\ndf_spy = df_spy[['trade_date', 'close']].copy()\ndf_spy.insert(0, 'ticker', 'SPY')\ndf_spy\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nPrice\nticker\ntrade_date\nclose\n\n\n\n\n0\nSPY\n2004-12-31\n120.870003\n\n\n1\nSPY\n2005-01-03\n120.300003\n\n\n2\nSPY\n2005-01-04\n118.830002\n\n\n3\nSPY\n2005-01-05\n118.010002\n\n\n4\nSPY\n2005-01-06\n118.610001\n\n\n...\n...\n...\n...\n\n\n2827\nSPY\n2016-03-28\n203.240005\n\n\n2828\nSPY\n2016-03-29\n205.119995\n\n\n2829\nSPY\n2016-03-30\n206.020004\n\n\n2830\nSPY\n2016-03-31\n205.520004\n\n\n2831\nSPY\n2016-04-01\n206.919998\n\n\n\n\n2832 rows × 3 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Volatility Forecasting: Close-to-Close Estimator</span>"
    ]
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#calculating-daily-returns-realized-volatility",
    "href": "chapters/18_close_to_close/close_to_close.html#calculating-daily-returns-realized-volatility",
    "title": "19  Volatility Forecasting: Close-to-Close Estimator",
    "section": "19.3 Calculating Daily Returns & Realized Volatility",
    "text": "19.3 Calculating Daily Returns & Realized Volatility\nThe close-to-close estimator is a function of daily returns so let’s calculate those now. In particular, we will use log-returns.\n\ndf_spy['dly_ret'] = np.log(df_spy['close']).diff()\ndf_spy = df_spy[1:].reset_index(drop = True)\ndf_spy\n\n\n\n\n\n\n\nPrice\nticker\ntrade_date\nclose\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n120.300003\n-0.004727\n\n\n1\nSPY\n2005-01-04\n118.830002\n-0.012295\n\n\n2\nSPY\n2005-01-05\n118.010002\n-0.006925\n\n\n3\nSPY\n2005-01-06\n118.610001\n0.005071\n\n\n4\nSPY\n2005-01-07\n118.440002\n-0.001434\n\n\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n203.240005\n0.000591\n\n\n2827\nSPY\n2016-03-29\n205.119995\n0.009208\n\n\n2828\nSPY\n2016-03-30\n206.020004\n0.004378\n\n\n2829\nSPY\n2016-03-31\n205.520004\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n206.919998\n0.006789\n\n\n\n\n2831 rows × 4 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Volatility Forecasting: Close-to-Close Estimator</span>"
    ]
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#organizing-dates-for-backtest",
    "href": "chapters/18_close_to_close/close_to_close.html#organizing-dates-for-backtest",
    "title": "19  Volatility Forecasting: Close-to-Close Estimator",
    "section": "19.4 Organizing Dates for Backtest",
    "text": "19.4 Organizing Dates for Backtest\nOrganizing dates is an important step in a historical analysis.\nWe are performing a weekly analysis, which means that in later steps we will performing aggregation calculations of daily calculations grouped into weeks. Therefore, we will need to add a column to df_spy that will allow us to group by weeks.\nThe key to our approach will be to use the .dt.weekday attribute of the trade_date columns. In the following code, the variable weekday is a Series that contains the weekday associated with each date. Notice that Monday is encoded by 0 and Friday is encoded by 4.\n\nweekday = df_spy['trade_date'].dt.weekday\nweekday\n\n0       0\n1       1\n2       2\n3       3\n4       4\n       ..\n2826    0\n2827    1\n2828    2\n2829    3\n2830    4\nName: trade_date, Length: 2831, dtype: int32\n\n\nThe following code is a simple for-loop that has the effect of creating a week-number for each week.\n\nweek_num = []\nix_week = 0\nweek_num.append(ix_week)\nfor ix in range(0, len(weekday) - 1):\n    prev_day = weekday[ix]\n    curr_day = weekday[ix + 1]\n    if curr_day &lt; prev_day:\n        ix_week = ix_week + 1\n    week_num.append(ix_week)\nnp.array(week_num) # I use the array function simply because it looks better when it prints\n\narray([  0,   0,   0, ..., 586, 586, 586])\n\n\nLet’s now insert the week numbers into df_spy.\n\ndf_spy.insert(2, 'week_num', week_num)\ndf_spy\n\n\n\n\n\n\n\nPrice\nticker\ntrade_date\nweek_num\nclose\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n0\n120.300003\n-0.004727\n\n\n1\nSPY\n2005-01-04\n0\n118.830002\n-0.012295\n\n\n2\nSPY\n2005-01-05\n0\n118.010002\n-0.006925\n\n\n3\nSPY\n2005-01-06\n0\n118.610001\n0.005071\n\n\n4\nSPY\n2005-01-07\n0\n118.440002\n-0.001434\n\n\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.240005\n0.000591\n\n\n2827\nSPY\n2016-03-29\n586\n205.119995\n0.009208\n\n\n2828\nSPY\n2016-03-30\n586\n206.020004\n0.004378\n\n\n2829\nSPY\n2016-03-31\n586\n205.520004\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n586\n206.919998\n0.006789\n\n\n\n\n2831 rows × 5 columns\n\n\n\n\nDiscussion Question: The pandas.Series.dt.week attribute gives the week-of-the-year for a give trade-date. My initial idea was to use .dt.week and dt.year for my grouping, but I ran into an issue. Can you think what the issue was?\n\n\nSolution\n##&gt; Weeks at the beginning and end of the year may be partial weeks.\n\n\n\nWe can now use .groupby() to calculate the starting and ending dates for each week.\n\ndf_start_end = \\\n    (\n    df_spy.groupby(['week_num'], as_index = False)[['trade_date']].agg([min, max])['trade_date']\n    .rename(columns = {'min':'week_start', 'max':'week_end'})\n    .reset_index()\n    .rename(columns = {'index':'week_num'})\n    )\ndf_start_end\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n\n\n1\n1\n2005-01-10\n2005-01-14\n\n\n2\n2\n2005-01-18\n2005-01-21\n\n\n3\n3\n2005-01-24\n2005-01-28\n\n\n4\n4\n2005-01-31\n2005-02-04\n\n\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n\n\n583\n583\n2016-03-07\n2016-03-11\n\n\n584\n584\n2016-03-14\n2016-03-18\n\n\n585\n585\n2016-03-21\n2016-03-24\n\n\n586\n586\n2016-03-28\n2016-04-01\n\n\n\n\n587 rows × 3 columns\n\n\n\nLet’s merge these columns into df_spy.\n\ndf_spy = df_spy.merge(df_start_end)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nweek_num\nclose\ndly_ret\nweek_start\nweek_end\n\n\n\n\n0\nSPY\n2005-01-03\n0\n120.300003\n-0.004727\n2005-01-03\n2005-01-07\n\n\n1\nSPY\n2005-01-04\n0\n118.830002\n-0.012295\n2005-01-03\n2005-01-07\n\n\n2\nSPY\n2005-01-05\n0\n118.010002\n-0.006925\n2005-01-03\n2005-01-07\n\n\n3\nSPY\n2005-01-06\n0\n118.610001\n0.005071\n2005-01-03\n2005-01-07\n\n\n4\nSPY\n2005-01-07\n0\n118.440002\n-0.001434\n2005-01-03\n2005-01-07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.240005\n0.000591\n2016-03-28\n2016-04-01\n\n\n2827\nSPY\n2016-03-29\n586\n205.119995\n0.009208\n2016-03-28\n2016-04-01\n\n\n2828\nSPY\n2016-03-30\n586\n206.020004\n0.004378\n2016-03-28\n2016-04-01\n\n\n2829\nSPY\n2016-03-31\n586\n205.520004\n-0.002430\n2016-03-28\n2016-04-01\n\n\n2830\nSPY\n2016-04-01\n586\n206.919998\n0.006789\n2016-03-28\n2016-04-01\n\n\n\n\n2831 rows × 7 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Volatility Forecasting: Close-to-Close Estimator</span>"
    ]
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#calculating-weekly-realized-volatility",
    "href": "chapters/18_close_to_close/close_to_close.html#calculating-weekly-realized-volatility",
    "title": "19  Volatility Forecasting: Close-to-Close Estimator",
    "section": "19.5 Calculating Weekly Realized Volatility",
    "text": "19.5 Calculating Weekly Realized Volatility\nNow that we have a week_num associated with each trade_date, we can use .groupby() to calculate the realized volatility.\nThese weekly realized volatilities are the labels that we will be predicting later in our analysis.\n\ndf_realized = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']].agg(lambda x: np.std(x) * np.sqrt(252))\n        .rename(columns = {'dly_ret':'realized_vol'})\n    )\ndf_realized = df_realized[1:]\ndf_realized\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nrealized_vol\n\n\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.093295\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.126557\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.029753\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.069583\n\n\n5\n5\n2005-02-07\n2005-02-11\n0.084567\n\n\n...\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.159055\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.137591\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.057861\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.048135\n\n\n586\n586\n2016-03-28\n2016-04-01\n0.066437\n\n\n\n\n586 rows × 4 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Volatility Forecasting: Close-to-Close Estimator</span>"
    ]
  },
  {
    "objectID": "chapters/18_close_to_close/close_to_close.html#close-to-close-estimator",
    "href": "chapters/18_close_to_close/close_to_close.html#close-to-close-estimator",
    "title": "19  Volatility Forecasting: Close-to-Close Estimator",
    "section": "19.6 Close-to-Close Estimator",
    "text": "19.6 Close-to-Close Estimator\nLet’s now implement the close-to-close estimator.\n\ndef close_to_close(r):\n    T = r.shape[0]\n    r_bar = r.mean()\n    vol = np.sqrt((1 / (T - 1)) * ((r - r_bar) ** 2).sum()) * np.sqrt(252)\n    return(vol)\n\nNotice that close_to_close() is an aggregation function that takes in an array of daily returns and returns back a number. In order to calculate weekly estimates we use close_to_close() as the aggregation function applied to a .groupby().\n\ndf_close_to_close = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n        .agg(close_to_close)\n        .rename(columns = {'dly_ret':'close_to_close'})\n    )\ndf_close_to_close = df_close_to_close[0:-1]\ndf_close_to_close\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nclose_to_close\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n0.102492\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.104307\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.146136\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.033265\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.077796\n\n\n...\n...\n...\n...\n...\n\n\n581\n581\n2016-02-22\n2016-02-26\n0.175394\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.177829\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.153831\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.064691\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.055581\n\n\n\n\n586 rows × 4 columns\n\n\n\n\nDiscussion Question: Verify that the .groupby() above works just fine with out including week_start and week_end. If that is the case, then why did I include it?\n\n\nSolution\n(\ndf_spy\n    .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n    .agg(close_to_close)\n    .rename(columns = {'dly_ret':'close_to_close'})\n)\n\n# It makes the code and the dataframe more readable.\n\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nclose_to_close\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n0.102492\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.104307\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.146136\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.033265\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.077796\n\n\n...\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.177829\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.153831\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.064691\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.055581\n\n\n586\n586\n2016-03-28\n2016-04-01\n0.074279\n\n\n\n\n587 rows × 4 columns\n\n\n\n\nCode Challenge: Create an alternative version of our close-to-close function using np.std(). Call the new function close_to_close_std(). Verify that your values match.\n\n\nSolution\ndef close_to_close_std(r):\n    vol = np.std(r, ddof = 1) * np.sqrt(252)\n    return(vol)\n\ndf_std = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']]\n        .agg(close_to_close_std)\n        .rename(columns = {'dly_ret':'close_to_close'})\n    )\n    \ndf_std = df_std[:-1]\nprint(df_std['close_to_close'].sum())\nprint(df_close_to_close['close_to_close'].sum())\n\n\n90.6980642016463\n90.6980642016463\n\n\nIn Sepp 2016, the author uses the \\(R^2\\) between the forecasts and the realized labels as a means of assessing the quality of a particular estimator. Let’s utilize sklearn to do the same.\nWe being by importing the LinearRegression() constructor and instantiating a model.\n\nfrom sklearn.linear_model import LinearRegression\nmdl_reg = LinearRegression(fit_intercept = True)\n\nNext, let’s organize our features and labels.\n\nX = df_close_to_close[['close_to_close']]\ny = df_realized['realized_vol']\n\nWe can now fit the model.\n\nmdl_reg.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nThe .score() method of a LinearRegression model returns the \\(R^2\\).\n\nmdl_reg.score(X, y)\n\n0.4093645253435927\n\n\nAnd we can examine the slope and intercept of our model as follows:\n\nprint(\"Intercept:\", mdl_reg.intercept_)\nprint(\"Slope:   \", mdl_reg.coef_)\n\nIntercept: 0.04933384095053199\nSlope:    [0.57068844]\n\n\n\nDiscussion Question: How do our results compare to Sepp’s?\n\n\nSolution\n# They seem close enough that there probably isn't some error in my calculations.\n# The differences probably come down to differences in data.\n# I do wish the results were a bit closer to feel totally comfortable.\n\n\n\nLet’s also measure the bias of the close-to-close estimator.\n\n# bias\nprint(\"Bias:\", np.mean(df_close_to_close['close_to_close'] - df_realized['realized_vol']))\n\nBias: 0.01708041424884867",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Volatility Forecasting: Close-to-Close Estimator</span>"
    ]
  },
  {
    "objectID": "chapters/19_garch/garch.html",
    "href": "chapters/19_garch/garch.html",
    "title": "20  Volatility Forecasting: GARCH",
    "section": "",
    "text": "20.1 Loading Packages\nFinancial asset returns exhibit volatility clustering (also called volatility regimes), as well as mean reversion. This means that high volatility periods beget high volatility periods, low volatility periods beget low volatility periods, and that volatility doesn’t not converge to zero, or diverge to infinity. GARCH (generalized autoregressive conditional heteroskedasticity) time series models are popular for capturing these characteristics.\nThis chapter demonstrates code that implements a GARCH(1, 1) model using the arch package. We will view GARCH as a volatility prediction mechanism, and our interest will be on empirically determining the performance of GARCH relative to other prediction techniques (see the associated project).\nIn this chapter we will be focused exclusively on implementation, and thus we won’t be digging into the mathematical or statistical details of GARCH models (if you are interested, check out the references at the end). Specifically, we will generate some GARCH forecasts of SPY weekly volatility for the backtest period 1/1/2005 - 3/30/2016, and see how our results compare to Sepp 2016 (Volatility Modeling and Trading, pg 57).\nWe begin by loading the packages and functions that we will need.\nimport numpy as np\nimport pandas as pd\nfrom arch import arch_model\nimport yfinance as yf\nimport sys\npd.options.display.max_rows = 10",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Volatility Forecasting: GARCH</span>"
    ]
  },
  {
    "objectID": "chapters/19_garch/garch.html#spy-data-for-backtest-period",
    "href": "chapters/19_garch/garch.html#spy-data-for-backtest-period",
    "title": "20  Volatility Forecasting: GARCH",
    "section": "20.2 SPY Data for Backtest Period",
    "text": "20.2 SPY Data for Backtest Period\nNext, let’s read-in the data for the backtest period.\n\ndf_spy = yf.download(\n    'SPY', start = '2004-12-31', end = '2016-04-02', auto_adjust=False\n).reset_index()\ndf_spy = df_spy.droplevel(level=1, axis=1)\ndf_spy.columns = df_spy.columns.str.lower().str.replace(' ', '_')\ndf_spy.rename(columns = {'date':'trade_date'}, inplace = True)\ndf_spy = df_spy[['trade_date', 'close']].copy()\ndf_spy.insert(0, 'ticker', 'SPY')\ndf_spy\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nPrice\nticker\ntrade_date\nclose\n\n\n\n\n0\nSPY\n2004-12-31\n120.870003\n\n\n1\nSPY\n2005-01-03\n120.300003\n\n\n2\nSPY\n2005-01-04\n118.830002\n\n\n3\nSPY\n2005-01-05\n118.010002\n\n\n4\nSPY\n2005-01-06\n118.610001\n\n\n...\n...\n...\n...\n\n\n2827\nSPY\n2016-03-28\n203.240005\n\n\n2828\nSPY\n2016-03-29\n205.119995\n\n\n2829\nSPY\n2016-03-30\n206.020004\n\n\n2830\nSPY\n2016-03-31\n205.520004\n\n\n2831\nSPY\n2016-04-01\n206.919998\n\n\n\n\n2832 rows × 3 columns\n\n\n\nWe will need the daily log returns, so let’s calculate those now.\n\ndf_spy['dly_ret'] = np.log(df_spy['close']).diff()\ndf_spy = df_spy.dropna().reset_index(drop = True)\ndf_spy\n\n\n\n\n\n\n\nPrice\nticker\ntrade_date\nclose\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n120.300003\n-0.004727\n\n\n1\nSPY\n2005-01-04\n118.830002\n-0.012295\n\n\n2\nSPY\n2005-01-05\n118.010002\n-0.006925\n\n\n3\nSPY\n2005-01-06\n118.610001\n0.005071\n\n\n4\nSPY\n2005-01-07\n118.440002\n-0.001434\n\n\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n203.240005\n0.000591\n\n\n2827\nSPY\n2016-03-29\n205.119995\n0.009208\n\n\n2828\nSPY\n2016-03-30\n206.020004\n0.004378\n\n\n2829\nSPY\n2016-03-31\n205.520004\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n206.919998\n0.006789\n\n\n\n\n2831 rows × 4 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Volatility Forecasting: GARCH</span>"
    ]
  },
  {
    "objectID": "chapters/19_garch/garch.html#organizing-dates",
    "href": "chapters/19_garch/garch.html#organizing-dates",
    "title": "20  Volatility Forecasting: GARCH",
    "section": "20.3 Organizing Dates",
    "text": "20.3 Organizing Dates\nLet’s organize our backtest period into weeks (Monday - Friday), since our analysis is of weekly volatility predictions.\n\n# adding a new columns consisting of the weekday number for each trade_date \nweekday = df_spy['trade_date'].dt.weekday\n\n# assigning a week number to each trade-date; M-F is all the same week\nweek_num = []\nix_week = 0\nweek_num.append(ix_week)\nfor ix in range(0, len(weekday) - 1):\n    prev_day = weekday.iloc[ix]\n    curr_day = weekday.iloc[ix + 1]\n    if curr_day &lt; prev_day:\n        ix_week = ix_week + 1\n    week_num.append(ix_week)\n\n# inserting the week-number column into df_spy\ndf_spy.insert(2, 'week_num', week_num)\ndf_spy\n\n\n\n\n\n\n\nPrice\nticker\ntrade_date\nweek_num\nclose\ndly_ret\n\n\n\n\n0\nSPY\n2005-01-03\n0\n120.300003\n-0.004727\n\n\n1\nSPY\n2005-01-04\n0\n118.830002\n-0.012295\n\n\n2\nSPY\n2005-01-05\n0\n118.010002\n-0.006925\n\n\n3\nSPY\n2005-01-06\n0\n118.610001\n0.005071\n\n\n4\nSPY\n2005-01-07\n0\n118.440002\n-0.001434\n\n\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.240005\n0.000591\n\n\n2827\nSPY\n2016-03-29\n586\n205.119995\n0.009208\n\n\n2828\nSPY\n2016-03-30\n586\n206.020004\n0.004378\n\n\n2829\nSPY\n2016-03-31\n586\n205.520004\n-0.002430\n\n\n2830\nSPY\n2016-04-01\n586\n206.919998\n0.006789\n\n\n\n\n2831 rows × 5 columns\n\n\n\nThe following code generates a DataFrame that contains the start-date and end-date for each week.\n\ndf_start_end = \\\n    (\n    df_spy.groupby(['week_num'], as_index = False)[['trade_date']].agg([min, max])['trade_date']\n    .rename(columns = {'min':'week_start', 'max':'week_end'})\n    .reset_index()\n    .rename(columns = {'index':'week_num'})\n    )\ndf_start_end\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n\n\n1\n1\n2005-01-10\n2005-01-14\n\n\n2\n2\n2005-01-18\n2005-01-21\n\n\n3\n3\n2005-01-24\n2005-01-28\n\n\n4\n4\n2005-01-31\n2005-02-04\n\n\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n\n\n583\n583\n2016-03-07\n2016-03-11\n\n\n584\n584\n2016-03-14\n2016-03-18\n\n\n585\n585\n2016-03-21\n2016-03-24\n\n\n586\n586\n2016-03-28\n2016-04-01\n\n\n\n\n587 rows × 3 columns\n\n\n\nAnd finally let’s join the weekly start/end dates into df_spy.\n\ndf_spy = df_spy.merge(df_start_end)\ndf_spy\n\n\n\n\n\n\n\n\nticker\ntrade_date\nweek_num\nclose\ndly_ret\nweek_start\nweek_end\n\n\n\n\n0\nSPY\n2005-01-03\n0\n120.300003\n-0.004727\n2005-01-03\n2005-01-07\n\n\n1\nSPY\n2005-01-04\n0\n118.830002\n-0.012295\n2005-01-03\n2005-01-07\n\n\n2\nSPY\n2005-01-05\n0\n118.010002\n-0.006925\n2005-01-03\n2005-01-07\n\n\n3\nSPY\n2005-01-06\n0\n118.610001\n0.005071\n2005-01-03\n2005-01-07\n\n\n4\nSPY\n2005-01-07\n0\n118.440002\n-0.001434\n2005-01-03\n2005-01-07\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\nSPY\n2016-03-28\n586\n203.240005\n0.000591\n2016-03-28\n2016-04-01\n\n\n2827\nSPY\n2016-03-29\n586\n205.119995\n0.009208\n2016-03-28\n2016-04-01\n\n\n2828\nSPY\n2016-03-30\n586\n206.020004\n0.004378\n2016-03-28\n2016-04-01\n\n\n2829\nSPY\n2016-03-31\n586\n205.520004\n-0.002430\n2016-03-28\n2016-04-01\n\n\n2830\nSPY\n2016-04-01\n586\n206.919998\n0.006789\n2016-03-28\n2016-04-01\n\n\n\n\n2831 rows × 7 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Volatility Forecasting: GARCH</span>"
    ]
  },
  {
    "objectID": "chapters/19_garch/garch.html#calculating-realized-volatility",
    "href": "chapters/19_garch/garch.html#calculating-realized-volatility",
    "title": "20  Volatility Forecasting: GARCH",
    "section": "20.4 Calculating Realized Volatility",
    "text": "20.4 Calculating Realized Volatility\nThis code cell calculates the realized volatility for each week. In machine learning parlance, these are the labels that we are trying to predict.\n\ndf_realized = \\\n    (\n    df_spy\n        .groupby(['week_num', 'week_start', 'week_end'], as_index = False)[['dly_ret']].agg(lambda x: np.std(x) * np.sqrt(252))\n        .rename(columns = {'dly_ret':'realized_vol'})\n    )\ndf_realized = df_realized[1:]\ndf_realized\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\nrealized_vol\n\n\n\n\n1\n1\n2005-01-10\n2005-01-14\n0.093295\n\n\n2\n2\n2005-01-18\n2005-01-21\n0.126557\n\n\n3\n3\n2005-01-24\n2005-01-28\n0.029753\n\n\n4\n4\n2005-01-31\n2005-02-04\n0.069583\n\n\n5\n5\n2005-02-07\n2005-02-11\n0.084567\n\n\n...\n...\n...\n...\n...\n\n\n582\n582\n2016-02-29\n2016-03-04\n0.159055\n\n\n583\n583\n2016-03-07\n2016-03-11\n0.137591\n\n\n584\n584\n2016-03-14\n2016-03-18\n0.057861\n\n\n585\n585\n2016-03-21\n2016-03-24\n0.048135\n\n\n586\n586\n2016-03-28\n2016-04-01\n0.066437\n\n\n\n\n586 rows × 4 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Volatility Forecasting: GARCH</span>"
    ]
  },
  {
    "objectID": "chapters/19_garch/garch.html#garch-training-data",
    "href": "chapters/19_garch/garch.html#garch-training-data",
    "title": "20  Volatility Forecasting: GARCH",
    "section": "20.5 GARCH Training Data",
    "text": "20.5 GARCH Training Data\nA practical rule of thumb for fitting garch models to equity index returns is that you should use 5-10 years worth of data. In this tutorial we will use 10-years worth.\nLet’s begin by grabbing all the data that we will need for fitting.\n\n#df_train = pdr.get_data_yahoo('SPY', start = '1994-12-30', end = '2016-04-02').reset_index()\ndf_train = yf.download(\n    'SPY', start = '1994-12-30', end = '2016-04-02', auto_adjust=False\n).reset_index()\ndf_train = df_train.droplevel(level=1, axis=1)\ndf_train.columns = df_train.columns.str.lower().str.replace(' ', '_')\ndf_train.rename(columns = {'date':'trade_date'}, inplace = True)\ndf_train = df_train[['trade_date', 'close']].copy()\ndf_train.insert(0, 'ticker', 'SPY')\ndf_train\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nPrice\nticker\ntrade_date\nclose\n\n\n\n\n0\nSPY\n1994-12-30\n45.562500\n\n\n1\nSPY\n1995-01-03\n45.781250\n\n\n2\nSPY\n1995-01-04\n46.000000\n\n\n3\nSPY\n1995-01-05\n46.000000\n\n\n4\nSPY\n1995-01-06\n46.046875\n\n\n...\n...\n...\n...\n\n\n5346\nSPY\n2016-03-28\n203.240005\n\n\n5347\nSPY\n2016-03-29\n205.119995\n\n\n5348\nSPY\n2016-03-30\n206.020004\n\n\n5349\nSPY\n2016-03-31\n205.520004\n\n\n5350\nSPY\n2016-04-01\n206.919998\n\n\n\n\n5351 rows × 3 columns\n\n\n\nNext we calculate the daily returns.\n\ndf_train['dly_ret'] = np.log(df_train['close']).diff()\ndf_train.dropna(inplace = True)\ndf_train.reset_index(drop = True, inplace = True)\ndf_train\n\n\n\n\n\n\n\nPrice\nticker\ntrade_date\nclose\ndly_ret\n\n\n\n\n0\nSPY\n1995-01-03\n45.781250\n0.004790\n\n\n1\nSPY\n1995-01-04\n46.000000\n0.004767\n\n\n2\nSPY\n1995-01-05\n46.000000\n0.000000\n\n\n3\nSPY\n1995-01-06\n46.046875\n0.001019\n\n\n4\nSPY\n1995-01-09\n46.093750\n0.001017\n\n\n...\n...\n...\n...\n...\n\n\n5345\nSPY\n2016-03-28\n203.240005\n0.000591\n\n\n5346\nSPY\n2016-03-29\n205.119995\n0.009208\n\n\n5347\nSPY\n2016-03-30\n206.020004\n0.004378\n\n\n5348\nSPY\n2016-03-31\n205.520004\n-0.002430\n\n\n5349\nSPY\n2016-04-01\n206.919998\n0.006789\n\n\n\n\n5350 rows × 4 columns\n\n\n\nThe GARCH fitting process has better convergence if we express the returns as percents rather than decimals.\n\nser_returns = df_train['dly_ret'] * 100\nser_returns.index = df_train['trade_date']\n\nNext, we instantiate the model.\n\nmodel = arch_model(ser_returns, vol = 'Garch', p = 1, o = 0, q = 1, dist = 'Normal')\nresult = model.fit(update_freq = 5)\n\nIteration:      5,   Func. Count:     37,   Neg. LLF: 7638.164309626905\nIteration:     10,   Func. Count:     65,   Neg. LLF: 7614.014588774797\nOptimization terminated successfully    (Exit mode 0)\n            Current function value: 7614.0145792503545\n            Iterations: 12\n            Function evaluations: 74\n            Gradient evaluations: 12\n\n\nThe following code loops through and performs the fitting for each day. It takes a while to run, so I’ll leave it to you to run it and examine on your own time. See the package documentation linked below for details - you will probably need to read those in order to complete the associated project.\n\n# ix_start = df_train.query('trade_date == trade_date.min()').index[0]\n# ix_end = df_train.query('trade_date == \"2004-12-31\"').index[0]\n# forecasts = {}\n# for ix in range(2518, 5349):\n#     sys.stdout.write('.')\n#     sys.stdout.flush()\n#     result = model.fit(first_obs = (ix - 2518), last_obs = ix, disp = 'off')\n#     temp = result.forecast(horizon = 5, reindex = True).variance\n#     fcast = temp.iloc[ix - 1]\n#     forecasts[fcast.name] = fcast\n# print()\n# df_forecast = pd.DataFrame(pd.DataFrame(forecasts).T)\n# df_forecast = df_forecast.reset_index().rename(columns = {'index':'trade_date'})\n# df_forecast\n\n# # writing variance estimates to csv-file\n# df_forecast.to_csv('variance_forecast.csv', index = False)\n\nThe above code saves the variance estimates to a CSV file, and we will read those in now.\n\ndf_forecast = pd.read_csv('variance_forecast.csv')\ndf_forecast['trade_date'] = pd.to_datetime(df_forecast['trade_date'])\ndf_forecast\n\n\n\n\n\n\n\n\ntrade_date\nh.1\nh.2\nh.3\nh.4\nh.5\n\n\n\n\n0\n2004-12-30\n0.353439\n0.362420\n0.371362\n0.380267\n0.389134\n\n\n1\n2004-12-31\n0.341843\n0.350909\n0.359935\n0.368924\n0.377874\n\n\n2\n2005-01-03\n0.348320\n0.357434\n0.366509\n0.375543\n0.384539\n\n\n3\n2005-01-04\n0.464023\n0.472827\n0.481590\n0.490313\n0.498996\n\n\n4\n2005-01-05\n0.481935\n0.490661\n0.499346\n0.507992\n0.516598\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\n2016-03-23\n0.578490\n0.593375\n0.607975\n0.622293\n0.636337\n\n\n2827\n2016-03-24\n0.528614\n0.544496\n0.560071\n0.575345\n0.590325\n\n\n2828\n2016-03-28\n0.483814\n0.500593\n0.517046\n0.533181\n0.549003\n\n\n2829\n2016-03-29\n0.529147\n0.545053\n0.560652\n0.575948\n0.590947\n\n\n2830\n2016-03-30\n0.500307\n0.516825\n0.533021\n0.548903\n0.564477\n\n\n\n\n2831 rows × 6 columns\n\n\n\nThe columns of the df_forecast are the one day variance estimates for the next five days. In order to calculate a 5-day volatility forecasts we will add up these columns, divide by 100 (the variances are stated as percentages), and then take a square-root.\n\ndf_forecast['volatility_forecast'] = \\\n    np.sqrt((df_forecast['h.1'] + df_forecast['h.2'] + df_forecast['h.3'] + df_forecast['h.4'] + df_forecast['h.5']) / 100)\ndf_forecast\n\n\n\n\n\n\n\n\ntrade_date\nh.1\nh.2\nh.3\nh.4\nh.5\nvolatility_forecast\n\n\n\n\n0\n2004-12-30\n0.353439\n0.362420\n0.371362\n0.380267\n0.389134\n0.136258\n\n\n1\n2004-12-31\n0.341843\n0.350909\n0.359935\n0.368924\n0.377874\n0.134145\n\n\n2\n2005-01-03\n0.348320\n0.357434\n0.366509\n0.375543\n0.384539\n0.135364\n\n\n3\n2005-01-04\n0.464023\n0.472827\n0.481590\n0.490313\n0.498996\n0.155169\n\n\n4\n2005-01-05\n0.481935\n0.490661\n0.499346\n0.507992\n0.516598\n0.158004\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2826\n2016-03-23\n0.578490\n0.593375\n0.607975\n0.622293\n0.636337\n0.174312\n\n\n2827\n2016-03-24\n0.528614\n0.544496\n0.560071\n0.575345\n0.590325\n0.167298\n\n\n2828\n2016-03-28\n0.483814\n0.500593\n0.517046\n0.533181\n0.549003\n0.160737\n\n\n2829\n2016-03-29\n0.529147\n0.545053\n0.560652\n0.575948\n0.590947\n0.167384\n\n\n2830\n2016-03-30\n0.500307\n0.516825\n0.533021\n0.548903\n0.564477\n0.163203\n\n\n\n\n2831 rows × 7 columns\n\n\n\nLet’s plot these forecasts to make sure that they look reasonable.\n\ndf_forecast.plot(x = 'trade_date', y = 'volatility_forecast', figsize = (12, 6));\n\n\n\n\n\n\n\n\nWe actually have far more forecasts than we need, because we calculated forecasts for each day. We can isolate the ones in question by using a join.\n\ndf_start_end.merge(df_forecast[['trade_date', 'volatility_forecast']], left_on = 'week_end', right_on = 'trade_date')\n\n\n\n\n\n\n\n\nweek_num\nweek_start\nweek_end\ntrade_date\nvolatility_forecast\n\n\n\n\n0\n0\n2005-01-03\n2005-01-07\n2005-01-07\n0.151672\n\n\n1\n1\n2005-01-10\n2005-01-14\n2005-01-14\n0.153685\n\n\n2\n2\n2005-01-18\n2005-01-21\n2005-01-21\n0.171956\n\n\n3\n3\n2005-01-24\n2005-01-28\n2005-01-28\n0.149470\n\n\n4\n4\n2005-01-31\n2005-02-04\n2005-02-04\n0.153561\n\n\n...\n...\n...\n...\n...\n...\n\n\n581\n581\n2016-02-22\n2016-02-26\n2016-02-26\n0.251327\n\n\n582\n582\n2016-02-29\n2016-03-04\n2016-03-04\n0.243445\n\n\n583\n583\n2016-03-07\n2016-03-11\n2016-03-11\n0.231719\n\n\n584\n584\n2016-03-14\n2016-03-18\n2016-03-18\n0.188915\n\n\n585\n585\n2016-03-21\n2016-03-24\n2016-03-24\n0.167298\n\n\n\n\n586 rows × 5 columns\n\n\n\nLet’s grab the forecasts and put them into their own variable.\n\nvolatility_forecasts = df_start_end.merge(df_forecast, left_on = 'week_end', right_on = 'trade_date')['volatility_forecast']\nvolatility_forecasts\n\n0      0.151672\n1      0.153685\n2      0.171956\n3      0.149470\n4      0.153561\n         ...   \n581    0.251327\n582    0.243445\n583    0.231719\n584    0.188915\n585    0.167298\nName: volatility_forecast, Length: 586, dtype: float64\n\n\nNext, let’s use the \\(R^2\\) metric that Sepp uses in his studies.\n\nnp.corrcoef(volatility_forecasts, df_realized['realized_vol'])[0,1] ** 2\n\n0.5610123510046198\n\n\nAnd finally, let’s calculate the bias and efficiency. Errata (9/14/2025): Efficiency should have close-to-close estimator in the numerator, not realized vol. Come back and fix this.\n\n# bias\nprint(np.mean(volatility_forecasts - df_realized['realized_vol']))\n\n# efficiency\nprint(np.std(df_realized['realized_vol']) / np.std(volatility_forecasts))\n\n0.10300437701453104\n0.8245692042744442",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Volatility Forecasting: GARCH</span>"
    ]
  },
  {
    "objectID": "chapters/19_garch/garch.html#references",
    "href": "chapters/19_garch/garch.html#references",
    "title": "20  Volatility Forecasting: GARCH",
    "section": "20.6 References",
    "text": "20.6 References\nDoes Anything Beat Garch(1, 1) - Hansen and Lunde 2004\nOptions, Futures, and Other Derivatives 9th Edition - John Hull (Chapter 23)\nVolatility Modeling and Trading - Artur Sepp 2016\nhttps://arch.readthedocs.io/en/latest/univariate/univariate_volatility_forecasting.html",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Volatility Forecasting: GARCH</span>"
    ]
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html",
    "title": "21  Exploration of the Feed Grains Database",
    "section": "",
    "text": "21.1 Import Pacakges\nThe purpose of this chapter is to get our hands dirty with the Feed Grains database maintained by the USDA ERS.\nWe use this data to reproduce several of the graphs in from Mindy Mallory’s book Price Analysis - in particular from Chapters 12 (Forecasting Production), 13 (Forecasting Use of Corn), and 15 (Ending Stocks and Price).\nThe focus of this analysis is on corn.\nLet’s begin by loading the packages that we will need.\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Exploration of the Feed Grains Database</span>"
    ]
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#read-in-data",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#read-in-data",
    "title": "21  Exploration of the Feed Grains Database",
    "section": "21.2 Read-In Data",
    "text": "21.2 Read-In Data\nNext, let’s read-in our data.\n\ndf_feed_grains = pd.read_csv('../data/FeedGrains.csv')\ndf_feed_grains\n\n\n\n\n\n\n\n\nSC_Group_ID\nSC_Group_Desc\nSC_GroupCommod_ID\nSC_GroupCommod_Desc\nSC_Geography_ID\nSortOrder\nSC_GeographyIndented_Desc\nSC_Commodity_ID\nSC_Commodity_Desc\nSC_Attribute_ID\nSC_Attribute_Desc\nSC_Unit_ID\nSC_Unit_Desc\nYear_ID\nSC_Frequency_ID\nSC_Frequency_Desc\nTimeperiod_ID\nTimeperiod_Desc\nAmount\n\n\n\n\n0\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1926\n3\nAnnual\n69\nCommodity Market Year\n8.796000\n\n\n1\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1927\n3\nAnnual\n69\nCommodity Market Year\n9.513000\n\n\n2\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1928\n3\nAnnual\n69\nCommodity Market Year\n12.828000\n\n\n3\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1929\n3\nAnnual\n69\nCommodity Market Year\n14.703000\n\n\n4\n2\nSupply and use\n9.0\nBarley\n1\n0.80\nUnited States\n1\nBarley\n1\nPlanted acreage\n2\nMillion acres\n1930\n3\nAnnual\n69\nCommodity Market Year\n13.581000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n496558\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2022\n1\nMonthly\n12\nDec\n0.063158\n\n\n496559\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2022\n3\nAnnual\n19\nMY Jun-May\n0.442165\n\n\n496560\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2023\n1\nMonthly\n1\nJan\n0.051325\n\n\n496561\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2023\n1\nMonthly\n2\nFeb\n0.036997\n\n\n496562\n3\nExports and imports\n17.0\nOats\n300\n1.02\nCaribbean Basin (CBERA)\n79\nOats products\n24\nExports, from U.S. to specified destination\n7\n1,000 metric tons\n2023\n1\nMonthly\n3\nMar\n0.022716\n\n\n\n\n496563 rows × 19 columns",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Exploration of the Feed Grains Database</span>"
    ]
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#production",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#production",
    "title": "21  Exploration of the Feed Grains Database",
    "section": "21.3 Production",
    "text": "21.3 Production\nLet’s grab all the attributes that are related to production side of the WASDE balance sheet analysis. We have to do this in two separate parts because the Timeperiod_Desc is different for some of the attributes.\n\nattributes = ['Harvested acreage', 'Yield per harvested acre', 'Planted acreage',\n       'Prices received by farmers', 'Production', 'Imports', 'Total Supply']\n\ndf_tidy_corn = \\\n(\ndf_feed_grains\n    .query('SC_Commodity_Desc == \"Corn\"')\n    .query('SC_GeographyIndented_Desc == \"United States\"')\n    .query('SC_Attribute_Desc == @attributes')\n    .query('Timeperiod_Desc == \"Commodity Market Year\"')\n    [['SC_Commodity_Desc', 'SC_GeographyIndented_Desc', 'SC_Attribute_Desc', 'Timeperiod_Desc','Year_ID', 'Amount', 'SC_Unit_Desc']]\n)\ndf_tidy_corn\n\n\n\n\n\n\n\n\nSC_Commodity_Desc\nSC_GeographyIndented_Desc\nSC_Attribute_Desc\nTimeperiod_Desc\nYear_ID\nAmount\nSC_Unit_Desc\n\n\n\n\n13840\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1866\n30.017\nMillion acres\n\n\n13841\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1867\n32.116\nMillion acres\n\n\n13842\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1868\n35.116\nMillion acres\n\n\n13843\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1869\n35.833\nMillion acres\n\n\n13844\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1870\n38.388\nMillion acres\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15962\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2019\n3.560\nDollars per bushel\n\n\n15979\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2020\n4.530\nDollars per bushel\n\n\n15996\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2021\n6.000\nDollars per bushel\n\n\n16007\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2022\n6.600\nDollars per bushel\n\n\n16014\nCorn\nUnited States\nPrices received by farmers\nCommodity Market Year\n2023\n4.800\nDollars per bushel\n\n\n\n\n730 rows × 7 columns\n\n\n\n\nattributes = ['Imports, market year', 'Beginning stocks']\n\ndf_imports_stocks = \\\n    (\n    df_feed_grains\n        .query('SC_Commodity_Desc == \"Corn\"')\n        .query('SC_GeographyIndented_Desc == \"United States\"')\n        .query('SC_Attribute_Desc == @attributes')\n        .query('Timeperiod_Desc.str.contains(\"MY\")')\n        [['SC_Commodity_Desc', 'SC_GeographyIndented_Desc', 'SC_Attribute_Desc', 'Timeperiod_Desc','Year_ID', 'Amount', 'SC_Unit_Desc']]\n    )\n\ndf_tidy_corn = pd.concat([df_tidy_corn, df_imports_stocks])\ndf_tidy_corn\n\n\n\n\n\n\n\n\nSC_Commodity_Desc\nSC_GeographyIndented_Desc\nSC_Attribute_Desc\nTimeperiod_Desc\nYear_ID\nAmount\nSC_Unit_Desc\n\n\n\n\n13840\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1866\n30.017\nMillion acres\n\n\n13841\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1867\n32.116\nMillion acres\n\n\n13842\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1868\n35.116\nMillion acres\n\n\n13843\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1869\n35.833\nMillion acres\n\n\n13844\nCorn\nUnited States\nHarvested acreage\nCommodity Market Year\n1870\n38.388\nMillion acres\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16416\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2019\n41.885\nMillion bushels\n\n\n16421\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2020\n24.233\nMillion bushels\n\n\n16516\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2021\n24.227\nMillion bushels\n\n\n16519\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2022\n40.000\nMillion bushels\n\n\n16520\nCorn\nUnited States\nImports, market year\nMY Sep-Aug\n2023\n25.000\nMillion bushels\n\n\n\n\n828 rows × 7 columns\n\n\n\nOur analysis will be easier if we pivot our tidy data.\n\ndf_supply = \\\n(\ndf_tidy_corn\n    .pivot(index='Year_ID', columns='SC_Attribute_Desc', values='Amount')\n    .reset_index()\n    [['Year_ID', 'Beginning stocks','Planted acreage', 'Harvested acreage', 'Yield per harvested acre', \n      'Production', 'Prices received by farmers', 'Imports, market year']]\n    .assign(total_supply = lambda df: df['Beginning stocks'] + df['Production'] + df['Imports, market year'])\n)\ndf_supply.columns.name = None\ndf_supply\n\n\n\n\n\n\n\n\nYear_ID\nBeginning stocks\nPlanted acreage\nHarvested acreage\nYield per harvested acre\nProduction\nPrices received by farmers\nImports, market year\ntotal_supply\n\n\n\n\n0\n1866\nNaN\nNaN\n30.017\n24.3000\n730.814\n0.657\nNaN\nNaN\n\n\n1\n1867\nNaN\nNaN\n32.116\n24.7000\n793.905\n0.781\nNaN\nNaN\n\n\n2\n1868\nNaN\nNaN\n35.116\n26.2000\n919.590\n0.617\nNaN\nNaN\n\n\n3\n1869\nNaN\nNaN\n35.833\n21.8000\n782.084\n0.725\nNaN\nNaN\n\n\n4\n1870\nNaN\nNaN\n38.388\n29.3000\n1124.775\n0.521\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n153\n2019\n2220.749\n89.745\n81.337\n167.5000\n13619.928\n3.560\n41.885\n15882.562\n\n\n154\n2020\n1919.462\n90.652\n82.313\n171.4000\n14111.449\n4.530\n24.233\n16055.144\n\n\n155\n2021\n1234.512\n93.252\n85.318\n176.7000\n15073.820\n6.000\n24.227\n16332.559\n\n\n156\n2022\n1376.890\n88.579\n79.207\n173.3397\n13729.719\n6.600\n40.000\n15146.609\n\n\n157\n2023\n1416.609\n91.996\n84.100\n181.5101\n15265.000\n4.800\n25.000\n16706.609\n\n\n\n\n158 rows × 9 columns\n\n\n\n\n# recreating graph in 12.1.1 Forecasting Harvested Acres\n(\ndf_supply\n    .assign(difference = lambda df: df['Planted acreage'] - df['Harvested acreage'])\n    .query('Year_ID &gt; 1999')\n).plot(x='Year_ID', y=['Harvested acreage', 'Planted acreage', 'difference'], grid=True);\n\n\n\n\n\n\n\n\n\n# recreating graph in 12.2 Forecasting Yield\ndf_supply.plot(x='Year_ID', y=['Yield per harvested acre'], grid=True);\n\n\n\n\n\n\n\n\n\n# recreating graph in 12.2 Forecasting Yield\ndf_supply.query('Year_ID &gt; 1950').plot(x='Year_ID', y=['Yield per harvested acre'], grid=True);\n\n\n\n\n\n\n\n\n\n# recreating graph in 12.2 Forecasting Yield\ndf_supply.query('Year_ID &gt; 1980').plot(x='Year_ID', y=['Yield per harvested acre'], grid=True);",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Exploration of the Feed Grains Database</span>"
    ]
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#consumption",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#consumption",
    "title": "21  Exploration of the Feed Grains Database",
    "section": "21.4 Consumption",
    "text": "21.4 Consumption\nLet’s now switch our attention to the demand side. We begin by grabbing all the demand related attributes.\n\nattributes = ['Food, alcohol, and industrial use', 'Feed and residual use', 'Seed use', 'Exports, market year']\n\ndf_demand_tidy = \\\n    (\n    df_feed_grains\n        .query('SC_Commodity_Desc == \"Corn\"')\n        .query('SC_GeographyIndented_Desc == \"United States\"')\n        .query('SC_Attribute_Desc == @attributes')\n        .query('Timeperiod_Desc.str.contains(\"MY\")')\n        [['SC_Commodity_Desc', 'SC_GeographyIndented_Desc', 'SC_Attribute_Desc', 'Timeperiod_Desc','Year_ID', 'Amount', 'SC_Unit_Desc']]\n    )\ndf_demand_tidy\n\n\n\n\n\n\n\n\nSC_Commodity_Desc\nSC_GeographyIndented_Desc\nSC_Attribute_Desc\nTimeperiod_Desc\nYear_ID\nAmount\nSC_Unit_Desc\n\n\n\n\n16525\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1975\n1664.494\nMillion bushels\n\n\n16530\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1976\n1645.119\nMillion bushels\n\n\n16535\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1977\n1896.396\nMillion bushels\n\n\n16540\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1978\n2113.117\nMillion bushels\n\n\n16633\nCorn\nUnited States\nExports, market year\nMY Sep-Aug\n1979\n2401.517\nMillion bushels\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n18601\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2019\n6256.213\nMillion bushels\n\n\n18606\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2020\n6435.942\nMillion bushels\n\n\n18611\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2021\n6734.439\nMillion bushels\n\n\n18614\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2022\n6649.000\nMillion bushels\n\n\n18615\nCorn\nUnited States\nFood, alcohol, and industrial use\nMY Sep-Aug\n2023\n6704.000\nMillion bushels\n\n\n\n\n196 rows × 7 columns\n\n\n\nNow let’s pivot our tidy data to make it a bit more usable.\n\ndf_demand = \\\n    (\n    df_demand_tidy\n        .pivot(index='Year_ID', columns='SC_Attribute_Desc', values='Amount')\n        .reset_index()\n        [['Year_ID', 'Food, alcohol, and industrial use', 'Feed and residual use', 'Seed use', 'Exports, market year']]\n        .assign(total_demand = lambda df: df['Food, alcohol, and industrial use'] + df['Feed and residual use'] + \n                df['Seed use'] + df['Exports, market year'])\n    )\ndf_demand.head()\n\n\n\n\n\n\n\nSC_Attribute_Desc\nYear_ID\nFood, alcohol, and industrial use\nFeed and residual use\nSeed use\nExports, market year\ntotal_demand\n\n\n\n\n0\n1975\n500.7\n3581.760\n20.1\n1664.494\n5767.054\n\n\n1\n1976\n522.1\n3601.881\n20.1\n1645.119\n5789.200\n\n\n2\n1977\n561.5\n3729.743\n19.5\n1896.396\n6207.139\n\n\n3\n1978\n588.5\n4274.362\n19.5\n2113.117\n6995.479\n\n\n4\n1979\n619.5\n4563.043\n20.0\n2401.517\n7604.060\n\n\n\n\n\n\n\n\n# 13.1 Food, alcohol, and industrial use\ndf_demand.plot(x='Year_ID', y='Food, alcohol, and industrial use', grid=True);\n\n\n\n\n\n\n\n\n\n# 13.1 Food, alcohol, and industrial use as proportion of total demand\n(\ndf_demand.\n    assign(prop = lambda df: df['Food, alcohol, and industrial use'] / df['total_demand'])\n).plot(x='Year_ID', y='prop', grid=True);\n\n\n\n\n\n\n\n\n\n# 13.2 Exports\ndf_demand.plot(x='Year_ID', y='Exports, market year', grid=True);\n\n\n\n\n\n\n\n\n\n# 13.2 Exports as a proportion of total demand\n(\ndf_demand.\n    assign(prop = lambda df: df['Exports, market year'] / df['total_demand'])\n).plot(x='Year_ID', y='prop', grid=True);\n\n\n\n\n\n\n\n\n\n# 13.3 Feed and residuals\ndf_demand.plot(x='Year_ID', y='Feed and residual use', grid=True);\n\n\n\n\n\n\n\n\n\n# 13.3 Feed and residuals as a proportion of total demand\n(\ndf_demand.\n    assign(prop = lambda df: df['Feed and residual use'] / df['total_demand'])\n).plot(x='Year_ID', y='prop', grid=True);",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Exploration of the Feed Grains Database</span>"
    ]
  },
  {
    "objectID": "chapters/20_feed_grains_database/feed_grains_database.html#ending-stocks-and-price",
    "href": "chapters/20_feed_grains_database/feed_grains_database.html#ending-stocks-and-price",
    "title": "21  Exploration of the Feed Grains Database",
    "section": "21.5 Ending Stocks and Price",
    "text": "21.5 Ending Stocks and Price\nLet’s now analyze the relationship between price and ending stocks. This is done in Chapter 15 (Ending Stocks and Price).\nWe begin by examining a scatter plot of stocks-to-use vs prices received by farmers. I would guess there would be a negative relationship here and that is the case.\n\n# corn stocks-to-use and prices received by farmers\ndf_surplus = \\\n    (\n    df_supply[['Year_ID', 'total_supply', 'Prices received by farmers']]\n        .merge(df_demand[['Year_ID', 'total_demand']], how='inner')\n        .assign(stocks_ratio = lambda df: (df['total_supply'] - df['total_demand']) / df['total_demand'])\n    )\ndf_surplus.plot(x='stocks_ratio', y='Prices received by farmers', kind='scatter', grid=True);\n\n\n\n\n\n\n\n\nLet’s run a regression on this data set.\n\nfrom sklearn.linear_model import LinearRegression\ndf_X = df_surplus.dropna()[['stocks_ratio']]\ndf_y = df_surplus.dropna()[['Prices received by farmers']]\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nAs we see we get an \\(R^2\\) of 0.22 which isn’t bad in the world of finance.\n\nmodel.score(df_X, df_y)\n\n0.22025208089867765\n\n\nFor every percent increase in stocks-to-use ratio there is about a $0.05 reduction in the price received by farmers.\n\nmodel.coef_ / 100\n\narray([[-0.04628747]])\n\n\n\n21.5.1 Analyzing Pre-2006\nMallory suggests breaking down the analysis into pre-2006 and post-2006 because that’s when the ethanol mandates came into play.\nWe begin with pre-2006.\n\ndf_surplus_pre_2006 = df_surplus.query('Year_ID &lt; 2006')\ndf_surplus_pre_2006.plot(x='stocks_ratio', y='Prices received by farmers', kind='scatter', grid=True);\n\n\n\n\n\n\n\n\nLet’s fit a regression to the pre-2006 data.\n\nfrom sklearn.linear_model import LinearRegression\ndf_X = df_surplus_pre_2006.dropna()[['stocks_ratio']]\ndf_y = df_surplus_pre_2006.dropna()[['Prices received by farmers']]\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nIt doesn’t seem to help our \\(R^2\\), but it does significantly change our coefficient, which is good to know.\n\nmodel.score(df_X, df_y)\n\n0.1505653587487924\n\n\n\nmodel.coef_ / 100\n\narray([[-0.01018139]])\n\n\n\n\n21.5.2 Analyzing post-2006\nNow let’s do the post-2006 analysis.\n\ndf_surplus_post_2006 = df_surplus.query('Year_ID &gt;= 2006')\ndf_surplus_post_2006.plot(x='stocks_ratio', y='Prices received by farmers', kind='scatter', grid=True);\n\n\n\n\n\n\n\n\nLet’s now fit a regression to the post-2006 data.\n\nfrom sklearn.linear_model import LinearRegression\ndf_X = df_surplus_post_2006.dropna()[['stocks_ratio']]\ndf_y = df_surplus_post_2006.dropna()[['Prices received by farmers']]\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nOur \\(R^2\\) improves and the negative relationship is much more pronounced.\n\nmodel.score(df_X, df_y)\n\n0.5193873474299935\n\n\n\nmodel.coef_ / 100\n\narray([[-0.31178772]])",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Exploration of the Feed Grains Database</span>"
    ]
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html",
    "title": "22  Simple Linear Regression",
    "section": "",
    "text": "22.1 Import Packages\nThis chapter is an intuitive introduction to simple linear regression in a finance context. In particular, we will fit regressions that demonstrate two stylized facts about volatility in SPY:\nOur focus will be on how to implement linear regression in Python, rather than on its mathematical/statistical details.\nLinear regression will serve as our first introduction to sklearn, a popular package for implementing various machine learning models in Python.\nLet’s begin by loading the packages that we will need.\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport pandas as pd\nimport numpy as np\nimport sklearn",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#reading-in-data",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#reading-in-data",
    "title": "22  Simple Linear Regression",
    "section": "22.2 Reading-In Data",
    "text": "22.2 Reading-In Data\nThe dataset that we will analyze in this tutorial consists of weekly volatility metrics for SPY during 2014-2018. Each row of the DataFrame is a set of observations from a specific week. In particular:\n\nrealized_vol - standard deviation of returns during period (annualized).\nret - simple return for the period.\nstart_iv - the implied vol (variance swap rate) at the start of the period.\n\nLet’s read-in the data set and have a look.\n\ndf_spy = pd.read_csv('spy_2014_2018_regression.csv')\ndf_spy.head()\n\n\n\n\n\n\n\n\nunderlying\nstart_date\nend_date\nrealized_vol\nret\nstart_iv\n\n\n\n\n0\nSPY\n2014-01-03\n2014-01-10\n0.052949\n0.006812\n0.104300\n\n\n1\nSPY\n2014-01-10\n2014-01-17\n0.147207\n-0.002719\n0.093948\n\n\n2\nSPY\n2014-01-17\n2014-01-24\n0.176336\n-0.026206\n0.103134\n\n\n3\nSPY\n2014-01-24\n2014-01-31\n0.136391\n-0.003977\n0.195719\n\n\n4\nSPY\n2014-01-31\n2014-02-07\n0.235160\n0.008383\n0.182371",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#exploratory-data-analysis-implied-volatility-as-a-fear-index",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#exploratory-data-analysis-implied-volatility-as-a-fear-index",
    "title": "22  Simple Linear Regression",
    "section": "22.3 Exploratory Data Analysis: Implied Volatility as a Fear Index",
    "text": "22.3 Exploratory Data Analysis: Implied Volatility as a Fear Index\nOptions are simple insurance contracts that are written on top of an underlying stock. They protect against large moves in the price of the underlying. Puts protect against downward moves, calls protect against upward moves.\nThe implied volatility of a stock is a measurement that gauges how much market participants are willing to pay for options on that stock. Thus, the implied volatility of a stock serves as a index of how fearful market participants are about large moves in the stock price.\nThe Leverage Effect: For many stocks, especially index-ETFs, the following two relationships hold:\n\nImplied volatility increases when the stock experiences losses (negative returns).\nImplied volatility decreases when the stock experiences gains (positive returns).\n\nLet’s try to see this relationship in our SPY weekly data by means of a simple scatter plot.\nFirst, let’s create a new column in df_spy - we’ll call it iv_change - to capture the week over week change of the implied volatility.\n\ndf_spy['iv_change'] = (df_spy['start_iv'] - df_spy['start_iv'].shift(1)).shift(-1)\ndf_spy.head()\n\n\n\n\n\n\n\n\nunderlying\nstart_date\nend_date\nrealized_vol\nret\nstart_iv\niv_change\n\n\n\n\n0\nSPY\n2014-01-03\n2014-01-10\n0.052949\n0.006812\n0.104300\n-0.010352\n\n\n1\nSPY\n2014-01-10\n2014-01-17\n0.147207\n-0.002719\n0.093948\n0.009187\n\n\n2\nSPY\n2014-01-17\n2014-01-24\n0.176336\n-0.026206\n0.103134\n0.092585\n\n\n3\nSPY\n2014-01-24\n2014-01-31\n0.136391\n-0.003977\n0.195719\n-0.013349\n\n\n4\nSPY\n2014-01-31\n2014-02-07\n0.235160\n0.008383\n0.182371\n-0.041966\n\n\n\n\n\n\n\nNext, let’s plot the weekly returns (ret) against the implied-vol changes (iv_change) using the pandas built-in plotting functionality.\n\ndf_spy.plot.scatter('ret', 'iv_change', c='k', figsize=(6, 4));\n\n\n\n\n\n\n\n\nClearly there is a negative relationship, which is what we would expect.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#regression-example-1-returns-vs-change-in-implied-volatility",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#regression-example-1-returns-vs-change-in-implied-volatility",
    "title": "22  Simple Linear Regression",
    "section": "22.4 Regression Example 1: Returns vs Change in Implied Volatility",
    "text": "22.4 Regression Example 1: Returns vs Change in Implied Volatility\nIn an exploratory data analysis situation, the visualization above may be all we would need to establish the existance of the leverage effect in SPY. On the other hand, we may want to make this analysis more precise by fitting a linear regression line to the data. A linear regression is a simple model that purports that iv_change is a linear function of the ret. Intuitively, when fitting a linear regression we are trying to find the straight line that has the minimium aggregate distance from all the points in our data.\nIn the language of statistics, the ret is the independent variable and the iv_change is the dependent variable. The field of machine learning uses different terminology: ret is called the feature and iv_change is called the label. In a generic machine learning problem, we seek to predict a label from one or more features.\nWe will use sklearn to fit a linear regression to our data. The first step in any learning task with sklearn is to instantiate the model object with a constructor function. In the case of linear regression, the constructor function is LinearRegression(). We’ll call our model variable iv_model.\n\nfrom sklearn.linear_model import LinearRegression\niv_model = LinearRegression(fit_intercept=False)\n\nBy setting fit_intercept=False we are forcing the line to go through the origin. This seems reasonable from a visual inspection of the data.\nNext, we’ll separate out the data that will be used to fit the model.\n\ndf_ret = df_spy[['ret']][0:-1] # features\ndf_iv = df_spy[['iv_change']][0:-1] # labels\n\nWe are now ready to fit the model by using the .fit() method of iv_model.\n\niv_model.fit(df_ret, df_iv)\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression(fit_intercept=False)\n\n\nNext, let’s check the intercept and coefficient of the line that was fit to our data.\n\nprint(iv_model.coef_)\nprint(iv_model.intercept_)\n\n[[-2.17471827]]\n0.0\n\n\nThis means that our linear regression model has determined that the best fitting line is of the form:\n\\[\\begin{align}\niv\\_change = -2.1747 \\cdot weekly\\_return.\n\\end{align}\\]\nThis can be interpreted to mean that every 1% of positive weekly price return leads to a drop in implied volatility of about 2.175%.\nWe can use the .predict() method of our fitted model iv_model to predict labels for a given set of features. In our example, we can predict implied volatility changes for a given set of weekly returns.\nLet’s try this for -5%, 0%, and 1%.\n\ntest_values = np.array([-0.05, 0, 0.01]).reshape(-1, 1)\niv_model.predict(test_values)\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([[ 0.10873591],\n       [ 0.        ],\n       [-0.02174718]])\n\n\nWe can also use the .predict() method to graph our fitted line along with our data.\n\nxfit = np.linspace(-0.08, 0.055, 100)         # range of line\nyfit = iv_model.predict(xfit[:, np.newaxis])  # model values in range\n\ndf_spy.plot.scatter('ret', 'iv_change', c='k', figsize=(6, 4));\nplt.plot(xfit, yfit);\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nIn sklearn, all learning models have a .score() method which calculates some kind of measure of accuracy or fit. For a LinearRegression model, .score() gives the \\(R^2\\).\nThe \\(R^{2}\\) measures gives a sense for the goodness of fit of a linear regression. It can be interpreted as the percent of variance in the label that is explained by the features.\n\niv_model.score(df_ret, df_iv)\n\n0.5900683519289271\n\n\nOur linear regression explains 59% of the variance in weekly implied volatility changes, from weekly returns.\nThere is no universal notion of what is a good or bad \\(R^2\\). That type of value judgement is context specific. Based on my experience of looking at financial data, this scatter plot looks pretty good, meaning that the relationship is strong.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#regression-example-2-realized-volatility-clustering",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#regression-example-2-realized-volatility-clustering",
    "title": "22  Simple Linear Regression",
    "section": "22.5 Regression Example 2: Realized Volatility Clustering",
    "text": "22.5 Regression Example 2: Realized Volatility Clustering\nA stylized fact about financial asset returns is that realized volatility exhibits clustering. This means that high volatility tends to be followed by high volatility, and low volatility tends to be followed by low volatility.\nLet’s try to observe realized voaltility clustering in our weekly SPY data, and then analyze it with linear regression. In particular, let’s observe the relationship between current-week realized volaltility and subsequent-week realized volatility.\nWe’ll begin by first creating new columns in df_spy to hold this data. Notice that real_vol_0 is just a copy of realized_vol.\n\ndf_spy['real_vol_0'] = df_spy['realized_vol']\ndf_spy['real_vol_1'] = df_spy['realized_vol'].shift(-1)\ndf_spy.head()\n\n\n\n\n\n\n\n\nunderlying\nstart_date\nend_date\nrealized_vol\nret\nstart_iv\niv_change\nreal_vol_0\nreal_vol_1\n\n\n\n\n0\nSPY\n2014-01-03\n2014-01-10\n0.052949\n0.006812\n0.104300\n-0.010352\n0.052949\n0.147207\n\n\n1\nSPY\n2014-01-10\n2014-01-17\n0.147207\n-0.002719\n0.093948\n0.009187\n0.147207\n0.176336\n\n\n2\nSPY\n2014-01-17\n2014-01-24\n0.176336\n-0.026206\n0.103134\n0.092585\n0.176336\n0.136391\n\n\n3\nSPY\n2014-01-24\n2014-01-31\n0.136391\n-0.003977\n0.195719\n-0.013349\n0.136391\n0.235160\n\n\n4\nSPY\n2014-01-31\n2014-02-07\n0.235160\n0.008383\n0.182371\n-0.041966\n0.235160\n0.063975\n\n\n\n\n\n\n\nNext, let’s take a look at a scatter plot of real_vol_0 vs real_vol_1.\n\ndf_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\n\n\n\n\n\n\n\n\nAt first glance, I would say this scatter plot looks good. The relationship is clearly positive (although quite noisy), as we would expect from the stylized fact of volatility clustering.\nHowever, the data in data in it’s current form is not particularly well suited for linear regression. First of all, the volatilies are bunched near zero, with a few extremely large observations. Additionally, standard deviations are by definition always greater than zero.\nFor both of these reasons, let’s take the logs of both variables to make the relationship more clear. We’ll do so by simply repopulating the columns in df_spy with the logged values that we want.\n\ndf_spy['real_vol_0'] = np.log(df_spy['realized_vol'])\ndf_spy['real_vol_1'] = np.log(df_spy['realized_vol']).shift(-1)\ndf_spy.head()\n\n\n\n\n\n\n\n\nunderlying\nstart_date\nend_date\nrealized_vol\nret\nstart_iv\niv_change\nreal_vol_0\nreal_vol_1\n\n\n\n\n0\nSPY\n2014-01-03\n2014-01-10\n0.052949\n0.006812\n0.104300\n-0.010352\n-2.938431\n-1.915913\n\n\n1\nSPY\n2014-01-10\n2014-01-17\n0.147207\n-0.002719\n0.093948\n0.009187\n-1.915913\n-1.735366\n\n\n2\nSPY\n2014-01-17\n2014-01-24\n0.176336\n-0.026206\n0.103134\n0.092585\n-1.735366\n-1.992228\n\n\n3\nSPY\n2014-01-24\n2014-01-31\n0.136391\n-0.003977\n0.195719\n-0.013349\n-1.992228\n-1.447491\n\n\n4\nSPY\n2014-01-31\n2014-02-07\n0.235160\n0.008383\n0.182371\n-0.041966\n-1.447491\n-2.749269\n\n\n\n\n\n\n\nLet’s replot the logged data.\n\ndf_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\n\n\n\n\n\n\n\n\nThe positive relationship looks more linear after taking logs of both the features and the labels.\nAs in the previous section, let’s fit a simple linear regression to this data by executing the following steps:\n\nInstantiate a model with the LinearRegression() constructor.\nIsolate the data for fitting.\nFit the model with .fit().\nCheck for goodness of fit with .score().\n\nFirst, let’s instantiate our model with the LinearRegression() constructor function. We will call our model rv_model.\n\nrv_model = LinearRegression(fit_intercept=True)\n\nNext, let’s isolate the data that we will use to fit the model.\n\ndf_rv_0 = df_spy[['real_vol_0']][:-1]\ndf_rv_1 = df_spy[['real_vol_1']][:-1]\n\nWe can now fit the model to the data using the .fit() method of rv_model.\n\nrv_model.fit(df_rv_0, df_rv_1)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nLastly, we can check the goodness of fit by first visually inspecting the data, and then also by calculating the \\(R^2\\).\n\nxfit = np.linspace(-4.5, -0.75, 100)          # range of line\nyfit = rv_model.predict(xfit[:, np.newaxis])  # model values in range\n\ndf_spy.plot.scatter('real_vol_0', 'real_vol_1', c='k', figsize=(6, 4));\nplt.plot(xfit, yfit);\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nAs we can see from the output code below, our \\(R^2\\) is lower for this regression than the previous one (ret vs iv_change). This is rather obvious from visual inspection of the two graphs - notice how much more spread out the data points are in this graph, versus the graph in the previous analysis.\n\nrv_model.score(df_rv_0, df_rv_1)\n\n0.21785126272072308",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#forecasting-realized-volatility",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#forecasting-realized-volatility",
    "title": "22  Simple Linear Regression",
    "section": "22.6 Forecasting Realized Volatility",
    "text": "22.6 Forecasting Realized Volatility\nThus far, our regression analysis involved using the entirety of the five years of SPY data that we have avaialable. This is typical if you are using regression for exploratory data analysis, or simply to confirm some kind of directional relationship between two variables.\nHowever, machine learning has aspirations beyond mere exploration - the ultimate goal is usually prediction or forecasting. If you’re serious about that objective, it’s appropriate to split your data into a training set and a testing set. These separate sets are used for two distinct purposes in a two-stage approach:\n\nTraing data - used to train/fit/learn the model. This phase is referred to as the learning or training phase.\nTesting data - fed into the trained model to produce predictions; we then analyze the predicted values vs true values in the test set to determine the accuracy of the model. This phase referred to as the testing or generalization phase.\n\nLet’s try using this two-stage approach with our realized volatility data.\nSpecifically, rather than fitting a linear regression to the entirety of our data set, let’s instead fit it to only the first four years of the data (2014-2017). We’ll then use the fitted/trained model to forecast realized volatility in 2018.\nLet’s begin by instantiating a new LinearRegression object and call it fcst_model.\n\nfcst_model = LinearRegression(fit_intercept=True)\n\nNext, let’s grab the training data from 2014-2018.\n\ndf_rv_0_train = df_spy[['real_vol_0']][0:208] # this weeks volatility (feature)\ndf_rv_1_train = df_spy[['real_vol_1']][0:208] # next weeks volatility (label)\n\nWe next fit our model to the training data using fcst_model.fit().\n\nfcst_model.fit(df_rv_0_train, df_rv_1_train) # fitting the model\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nLet’s print the coefficient, the intercept, and the \\(R^{2}\\) from the model.\n\nprint(\"Coefficent:      \", np.round(fcst_model.coef_[0, 0], 2))    # coefficient\nprint(\"Intercept:      \", np.round(fcst_model.intercept_[0], 2))  # intercept\nprint(\"R^2 (training):  \", np.round(fcst_model.score(df_rv_0_train, df_rv_1_train), 2)) # R^2 from training\n\nCoefficent:       0.46\nIntercept:       -1.34\nR^2 (training):   0.21\n\n\nSo our linear model is:\n\\[\\log(next\\_week\\_realized\\_vol) = 0.46 * \\log(this\\_week\\_realized\\_vol) - 1.34.\\]\nIt accounts for about 21% of the variability of the weekly \\(\\log(realized\\_vol)\\) in the training set.\nLet’s now apply our trained model to data from 2018. We begin be separating out the 2018 testing data into it’s own DataFrame.\n\ndf_rv_0_test = df_spy[['real_vol_0']][209:-1] # features\ndf_rv_1_test = df_spy[['real_vol_1']][209:-1] # labels\n\nWe can test how our model predictions compare to the real data by plugging our testing data into the score() method of the model.\n\nfcst_model.score(df_rv_0_test, df_rv_1_test)\n\n0.1484251353811019\n\n\nAlternatively, we can first calculate the predictions, and then calculate the \\(R^2\\) directly on the predicted values. In order to do this we would use the .predict() method of the LinearRegression object along the r2_score() function in the sklearn.metrics module.\n\nsklearn.metrics.r2_score(df_rv_1_test, fcst_model.predict(df_rv_0_test))\n\n0.1484251353811019\n\n\nNotice that the model is more accurate (i.e. has a higher \\(R^2\\)) on the training set than on the testing set - this is almost always the case.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#updating-our-forecasting-model-daily",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#updating-our-forecasting-model-daily",
    "title": "22  Simple Linear Regression",
    "section": "22.7 Updating our Forecasting Model Daily",
    "text": "22.7 Updating our Forecasting Model Daily\nIn our forcasting exercise above, our LinearRegression model was trained on data from 2014-2017, and all of our 2018 forecasts were based on that model. This is probably not what we would do in practice. Instead, we would fit a new model on a regular basis.\nIn the following code, the training period is updated every week to the most recent four years. We would hope to see slightly improved performance over just training the model once. (The code below is also indcative of patterns used when conducting a backtest.)\n\nix_start = 0\nix_end = 208\n\nforecasts = np.zeros(50)\nfcst_model_2 = LinearRegression(fit_intercept=True)\nfor ix_end in range(208, 258, 1):\n    # setting training period start date   \n    ix_start = ix_end - 208\n    \n    # selecting training data\n    df_rv_0_train = df_spy[['real_vol_0']][ix_start:ix_end]\n    df_rv_1_train = df_spy[['real_vol_1']][ix_start:ix_end]\n    \n    # fitting the model\n    fcst_model_2.fit(df_rv_0_train, df_rv_1_train)\n    \n    # forecasting with the newly fitted model\n    real_vol = df_spy['real_vol_0'].values[ix_end]\n    fcst_rv = fcst_model_2.coef_[0, 0] * real_vol  + fcst_model_2.intercept_[0]\n    \n    # saving the current forecast\n    forecasts[ix_start] = fcst_rv\n\nAs we can see, there is a slight improvement when updating the model daily - an \\(R^2\\) of 0.1817 vs 0.1484. I would not expect the improvement to be that significant given the simplistic nature of our forecasting mechanism.\n\nsklearn.metrics.r2_score(df_spy[['real_vol_1']][208:258], forecasts)\n\n0.18178835397754134",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21_simple_linear_regression/simple_linear_regression.html#further-reading",
    "href": "chapters/21_simple_linear_regression/simple_linear_regression.html#further-reading",
    "title": "22  Simple Linear Regression",
    "section": "22.8 Further Reading",
    "text": "22.8 Further Reading\nPython Data Science Handbook (VanderPlas) - 5.1 - What Is Machine Learning?\nPython Data Science Handbook (VanderPlas) - 5.2 - Introducing Scikit-Learn\nPython Data Science Handbook (VanderPlas) - 5.3 - Hyperparameters and Model Validation\nPython Data Science Handbook (VanderPlas) - 5.4 - Feature Engineering\nPython Data Science Handbook (VanderPlas) - 5.6 - In Depth: Linear Regression",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/22_categorical_linear_regression/categorical_linear_regression.html",
    "href": "chapters/22_categorical_linear_regression/categorical_linear_regression.html",
    "title": "23  Linear Regression with Categorical Variables",
    "section": "",
    "text": "23.1 Loading Packages\nOur previous work with linear regression involved cases where all the features were numeric (continuous). In this tutorial, we consider linear regression with categorical (discrete) features. This material is based on section 3.3.1 (pp 83-87) of Introduction to Statistical Learning 2e.\nLet’s begin by loading the packages that we will need. Notice that in addition to sklearn we are also using statsmodels, which is a popular package that includes many classical statistical techniques.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression with Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#reading-in-data",
    "href": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#reading-in-data",
    "title": "23  Linear Regression with Categorical Variables",
    "section": "23.2 Reading-In Data",
    "text": "23.2 Reading-In Data\nNext, we’ll read-in the data that we will be working with; it is the Credit data set from the ISLR2 R package that can be downloaded from CRAN.\n\ndf_credit = pd.read_csv('credit.csv')\ndf_credit.columns = df_credit.columns.str.lower()\ndf_credit\n\n\n\n\n\n\n\n\nincome\nlimit\nrating\ncards\nage\neducation\nown\nstudent\nmarried\nregion\nbalance\n\n\n\n\n0\n14.891\n3606\n283\n2\n34\n11\nNo\nNo\nYes\nSouth\n333\n\n\n1\n106.025\n6645\n483\n3\n82\n15\nYes\nYes\nYes\nWest\n903\n\n\n2\n104.593\n7075\n514\n4\n71\n11\nNo\nNo\nNo\nWest\n580\n\n\n3\n148.924\n9504\n681\n3\n36\n11\nYes\nNo\nNo\nWest\n964\n\n\n4\n55.882\n4897\n357\n2\n68\n16\nNo\nNo\nYes\nSouth\n331\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n395\n12.096\n4100\n307\n3\n32\n13\nNo\nNo\nYes\nSouth\n560\n\n\n396\n13.364\n3838\n296\n5\n65\n17\nNo\nNo\nNo\nEast\n480\n\n\n397\n57.872\n4171\n321\n5\n67\n12\nYes\nNo\nYes\nSouth\n138\n\n\n398\n37.728\n2525\n192\n1\n44\n13\nNo\nNo\nYes\nSouth\n0\n\n\n399\n18.701\n5524\n415\n5\n64\n7\nYes\nNo\nNo\nWest\n966\n\n\n\n\n400 rows × 11 columns\n\n\n\nThe label is balance - average credit card debt for each individual\nThere are several quantitative features: - age - in years - cards - number of credit cards - education - years of education - income - in thousands of dollars - limit - credit limit - rating credit rating (FICO)\nIn addition to these quantitative variables, we also have four qualitative variables: - own - home ownership - student - are they a student or not - status - are they married or not - region - geographic location (East, West, or South)\nWe can examine the pairwise relationships between the quantitative variables with the seaborn.pairplot() function.\n\nsns.pairplot(df_credit, height=1.25, plot_kws={\"s\": 15});",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression with Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#categorical-feature-with-two-levels",
    "href": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#categorical-feature-with-two-levels",
    "title": "23  Linear Regression with Categorical Variables",
    "section": "23.3 Categorical Feature with Two Levels",
    "text": "23.3 Categorical Feature with Two Levels\nSuppose that we wish to investigate diﬀerences in credit card balance between those who own a house and those who don’t, ignoring the other variables. If a qualitative feature only has two possible values (also called levels), then incorporating it into a regression model is simple. We simply create an indicator or dummy variable that takes on two possible numerical values. In particular, our dummy variable takes a value of zero for those who don’t own a house, and a value of one for those who do own a house. This technique is called one-hot encoding.\nWe can use pandas.get_dummies() to perform one-hot encoding.\n\ndf_X = df_credit[['own']]\ndf_X = pd.get_dummies(df_X, drop_first=True, dtype='float')\ndf_X.head()\n\n\n\n\n\n\n\n\nown_Yes\n\n\n\n\n0\n0.0\n\n\n1\n1.0\n\n\n2\n0.0\n\n\n3\n1.0\n\n\n4\n0.0\n\n\n\n\n\n\n\nLet’s isolate our labels into their own DataFrame.\n\ndf_y = df_credit[['balance']]\ndf_y.head()\n\n\n\n\n\n\n\n\nbalance\n\n\n\n\n0\n333\n\n\n1\n903\n\n\n2\n580\n\n\n3\n964\n\n\n4\n331\n\n\n\n\n\n\n\n\n23.3.1 Using sklearn\nWe’ll first perform a linear regression using sklearn.\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nWe can examine the intercept and coefficient as follows.\n\nprint(model.coef_)\nprint(model.intercept_)\n\n[[19.73312308]]\n[509.80310881]\n\n\nLet’s make these values easier to read by putting them into a DataFrame.\n\ndf_coef = pd.DataFrame(\n    data = {'coefficient': list(model.intercept_) + list(np.ravel(model.coef_))},\n    index = ['intercept'] + list(df_X.columns.values), \n)\ndf_coef\n\n\n\n\n\n\n\n\ncoefficient\n\n\n\n\nintercept\n509.803109\n\n\nown_Yes\n19.733123\n\n\n\n\n\n\n\n\n\n23.3.2 Using statsmodels\nNext, let’s perform the same the same linear regression using statsmodels which will allow us to check for statistical significance by examining \\(p\\)-values.\n\ndf_X_sm = sm.add_constant(df_X)\nls = sm.OLS(df_y, df_X_sm).fit()\nprint(ls.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                balance   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.002\nMethod:                 Least Squares   F-statistic:                    0.1836\nDate:                Tue, 29 Aug 2023   Prob (F-statistic):              0.669\nTime:                        13:35:04   Log-Likelihood:                -3019.3\nNo. Observations:                 400   AIC:                             6043.\nDf Residuals:                     398   BIC:                             6051.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        509.8031     33.128     15.389      0.000     444.675     574.931\nown_Yes       19.7331     46.051      0.429      0.669     -70.801     110.267\n==============================================================================\nOmnibus:                       28.438   Durbin-Watson:                   1.940\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               27.346\nSkew:                           0.583   Prob(JB):                     1.15e-06\nKurtosis:                       2.471   Cond. No.                         2.66\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nHere is how to interpret these results: - A non-home owner has an average balance of $509.80 - A home owner has an average balance of 509.80 + 19.73 = $529.53 - However, notice that the \\(p\\)-value of own_Yes is quite high which suggests that there is no statistical evidence of a diﬀerence in average credit card balance based on house ownership.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression with Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#categorical-feature-with-multiple-levels",
    "href": "chapters/22_categorical_linear_regression/categorical_linear_regression.html#categorical-feature-with-multiple-levels",
    "title": "23  Linear Regression with Categorical Variables",
    "section": "23.4 Categorical Feature with Multiple Levels",
    "text": "23.4 Categorical Feature with Multiple Levels\nIn this section we consider regression on a categorical feature that has multiple levels. When a qualitative feature has more than two levels, a single dummy variable cannot represent all possible values. In this situation we can create additional dummy variables. There will always be one fewer dummy variable than the number of levels.\nTo explore this technique, let’s use the region feature and create dummy variables using the pandas.get_dummies() function.\n\ndf_X = df_credit[['region']]\ndf_X = pd.get_dummies(df_X, drop_first=True, dtype='float')\ndf_X.head()\n\n\n\n\n\n\n\n\nregion_South\nregion_West\n\n\n\n\n0\n1.0\n0.0\n\n\n1\n0.0\n1.0\n\n\n2\n0.0\n1.0\n\n\n3\n0.0\n1.0\n\n\n4\n1.0\n0.0\n\n\n\n\n\n\n\nNext, let’s use sklearn to perform a linear regression with the dummy variables.\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(df_X, df_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nWe can examine the coefficients in a DataFrame as follows:\n\ndf_coef = pd.DataFrame(\n    data = {'coefficient': list(model.intercept_) + list(np.ravel(model.coef_))},\n    index = ['intercept'] + list(df_X.columns.values), \n)\ndf_coef\n\n\n\n\n\n\n\n\ncoefficient\n\n\n\n\nintercept\n531.000000\n\n\nregion_South\n-12.502513\n\n\nregion_West\n-18.686275\n\n\n\n\n\n\n\nFinally, we’ll use statsmodels to perform the same regression so that we can examine the \\(p\\)-values.\n\ndf_X_sm = sm.add_constant(df_X)\nls = sm.OLS(df_y, df_X_sm).fit()\nprint(ls.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                balance   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.005\nMethod:                 Least Squares   F-statistic:                   0.04344\nDate:                Tue, 29 Aug 2023   Prob (F-statistic):              0.957\nTime:                        13:37:01   Log-Likelihood:                -3019.3\nNo. Observations:                 400   AIC:                             6045.\nDf Residuals:                     397   BIC:                             6057.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst          531.0000     46.319     11.464      0.000     439.939     622.061\nregion_South   -12.5025     56.681     -0.221      0.826    -123.935      98.930\nregion_West    -18.6863     65.021     -0.287      0.774    -146.515     109.142\n==============================================================================\nOmnibus:                       28.829   Durbin-Watson:                   1.946\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               27.395\nSkew:                           0.581   Prob(JB):                     1.13e-06\nKurtosis:                       2.460   Cond. No.                         4.39\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nHere is how to interpret these results:\n\nThe average balance of a cardholder that lives in the East is $531.00.\nA cardholder in the South has an average balance of 531.00 - 12.50 = $518.50\nA cardholder in the West has an average balance of 531.00 - 18.67 = $512.33\nThe the \\(p\\)-values of both region_South and region_West are quite high which suggests that there is no statistical evidence of a diﬀerence in average credit card balance based on region.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression with Categorical Variables</span>"
    ]
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html",
    "title": "24  Ridge and Lasso Regression",
    "section": "",
    "text": "24.1 Import Packages\nIn this chapter we explore two variants of linear regression: ridge and lasso. Both of these are shrinkage methods, meaning that they shrink regression coefficients to zero. This has the effect of trading a little bit of bias for a reduction in variance. This can improve model stability and fit.\nThis tutorial is based on section 6.2 of Introduction to Statistical Learning 2e.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np\n%matplotlib inline",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ridge and Lasso Regression</span>"
    ]
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#read-in-data",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#read-in-data",
    "title": "24  Ridge and Lasso Regression",
    "section": "24.2 Read-In Data",
    "text": "24.2 Read-In Data\nNext, we’ll read-in the data that we will be working with; it is the Credit data set from the ISLR2 R package that can be downloaded from CRAN.\n\ndf_credit = pd.read_csv('credit.csv')\ndf_credit.columns = df_credit.columns.str.lower()\ndf_credit\n\n\n\n\n\n\n\n\nincome\nlimit\nrating\ncards\nage\neducation\nown\nstudent\nmarried\nregion\nbalance\n\n\n\n\n0\n14.891\n3606\n283\n2\n34\n11\nNo\nNo\nYes\nSouth\n333\n\n\n1\n106.025\n6645\n483\n3\n82\n15\nYes\nYes\nYes\nWest\n903\n\n\n2\n104.593\n7075\n514\n4\n71\n11\nNo\nNo\nNo\nWest\n580\n\n\n3\n148.924\n9504\n681\n3\n36\n11\nYes\nNo\nNo\nWest\n964\n\n\n4\n55.882\n4897\n357\n2\n68\n16\nNo\nNo\nYes\nSouth\n331\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n395\n12.096\n4100\n307\n3\n32\n13\nNo\nNo\nYes\nSouth\n560\n\n\n396\n13.364\n3838\n296\n5\n65\n17\nNo\nNo\nNo\nEast\n480\n\n\n397\n57.872\n4171\n321\n5\n67\n12\nYes\nNo\nYes\nSouth\n138\n\n\n398\n37.728\n2525\n192\n1\n44\n13\nNo\nNo\nYes\nSouth\n0\n\n\n399\n18.701\n5524\n415\n5\n64\n7\nYes\nNo\nNo\nWest\n966\n\n\n\n\n400 rows × 11 columns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ridge and Lasso Regression</span>"
    ]
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#wrangling-data",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#wrangling-data",
    "title": "24  Ridge and Lasso Regression",
    "section": "24.3 Wrangling Data",
    "text": "24.3 Wrangling Data\nOur next task is to preform some wrangling on our data.\nWe begin by creating dummy variables for all our categorial features.\n\ndf_X = pd.get_dummies(df_credit.drop('balance', axis=1), drop_first=True)\ndf_y = df_credit[['balance']]\n\nWhen performing shrinkage methods, it is best to normalize the features.\n\nfrom sklearn.preprocessing import scale\nX_s = scale(df_X, with_mean=False)",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ridge and Lasso Regression</span>"
    ]
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#ridge-regression",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#ridge-regression",
    "title": "24  Ridge and Lasso Regression",
    "section": "24.4 Ridge Regression",
    "text": "24.4 Ridge Regression\nWe are now ready to fit a ridge regression so let’s import the Ridge constructor.\n\nfrom sklearn.linear_model import Ridge\n\n\n24.4.1 Fitting a Ridge for a Single Value of alpha\nWe begin by fitting a ridge with a single value of alpha, which is the parameter that controls the amount of shrinkage to zero. Higher values correspond to more shrinkage. An alpha value of zero corresponds to the standard least squares model.\nLet’s start with alpha = 1.\n\nmodel = Ridge(alpha=1)\nmodel.fit(X_s, df_y)\n\nRidge(alpha=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=1)\n\n\nNext, let’s display the coefficients in a DataFrame.\n\ndf_coef = pd.DataFrame(\n    data = {'coefficient': list(model.intercept_) + list(np.ravel(model.coef_))},\n    index = ['intercept'] + list(df_X.columns.values), \n)\ndf_coef\n\n\n\n\n\n\n\n\ncoefficient\n\n\n\n\nintercept\n-487.398207\n\n\nincome\n-271.271818\n\n\nlimit\n367.168796\n\n\nrating\n245.258668\n\n\ncards\n21.330598\n\n\nage\n-10.896509\n\n\neducation\n-3.013622\n\n\nown_Yes\n-5.222885\n\n\nstudent_Yes\n126.864176\n\n\nmarried_Yes\n-4.718335\n\n\nregion_South\n5.083115\n\n\nregion_West\n7.600739\n\n\n\n\n\n\n\nNotice that effect size of income, limit, rating, and student_Yes are by far the largest. So we will focus on analyzing those in our subsequet analysis when we vary alpha.\n\n\n24.4.2 Fitting A Ridge for Multiple Values of alpha\nWe are now going fit our ridge model for various values of alpha and examine the coefficients and \\(R^2\\).\nThis code loops through alpha values, stores various values in lists, and then puts it all together nicely into a DataFrame\n\nalpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]\nincome_coef = []\nlimit_coef = []\nrating_coef = []\nstudent_yes_coef = []\nr_squared = []\nfor ix in alpha:\n    model = Ridge(alpha=ix)\n    model.fit(X_s, df_y)\n    coef = np.ravel(model.coef_)\n    income_coef.append(coef[0].round(2))\n    limit_coef.append(coef[1].round(2))\n    rating_coef.append(coef[2].round(2))\n    student_yes_coef.append(coef[7].round(2))\n    r_squared.append(model.score(X_s, df_y))\ndf_coefficient = pd.DataFrame(\n    {\n        'alpha':alpha,\n        'income':income_coef,\n        'limit':limit_coef,\n        'rating':rating_coef,\n        'student_Yes':student_yes_coef,\n        'r_squared':r_squared,\n    }\n)\ndf_coefficient\n\n\n\n\n\n\n\n\nalpha\nincome\nlimit\nrating\nstudent_Yes\nr_squared\n\n\n\n\n0\n0.001\n-274.67\n439.94\n175.78\n127.72\n0.955102\n\n\n1\n0.010\n-274.64\n438.55\n177.14\n127.71\n0.955102\n\n\n2\n0.100\n-274.34\n426.09\n189.33\n127.60\n0.955097\n\n\n3\n1.000\n-271.27\n367.17\n245.26\n126.86\n0.954974\n\n\n4\n10.000\n-242.12\n301.99\n281.05\n122.99\n0.952659\n\n\n5\n100.000\n-94.60\n211.39\n209.60\n97.64\n0.882671\n\n\n6\n1000.000\n22.93\n84.08\n84.22\n34.01\n0.550125\n\n\n7\n10000.000\n7.31\n14.44\n14.47\n4.57\n0.123256\n\n\n8\n100000.000\n0.84\n1.57\n1.57\n0.47\n0.014014\n\n\n9\n1000000.000\n0.09\n0.16\n0.16\n0.05\n0.001421\n\n\n\n\n\n\n\nNotice that the \\(R^2\\) decreases monotonically.\nLet’s now plot the coefficient values for various values of alpha.\n\nax = df_coefficient.\\\n        plot(\n            x = 'alpha',\n            y = ['income', 'limit', 'rating', 'student_Yes'],\n            title = 'Coefficient Values for Various alpha',\n            grid = True,\n            alpha = 0.75,\n            figsize = (9, 5),\n            logx=True,\n            );\nax.set_xlabel('alpha');\nax.set_ylabel('coefficient');\n\n\n\n\n\n\n\n\nAs you can see for large values of alpha the coefficients are close to zero, however they never quite reach zero.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ridge and Lasso Regression</span>"
    ]
  },
  {
    "objectID": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#lasso",
    "href": "chapters/23_ridge_lasso_regression/ridge_lasso_regression.html#lasso",
    "title": "24  Ridge and Lasso Regression",
    "section": "24.5 Lasso",
    "text": "24.5 Lasso\nLet’s now repeat the same analysis for lasso regression. We begin by calculating coefficients and \\(R^2\\) for various values of alpha, and then display them in a DataFrame.\n\nfrom sklearn.linear_model import Lasso\nalpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]\nincome_coef = []\nlimit_coef = []\nrating_coef = []\nstudent_yes_coef = []\nr_squared = []\nfor ix in alpha:\n    model = Lasso(alpha=ix)\n    model.fit(X_s, df_y)\n    coef = np.ravel(model.coef_)\n    income_coef.append(coef[0].round(2))\n    limit_coef.append(coef[1].round(2))\n    rating_coef.append(coef[2].round(2))\n    student_yes_coef.append(coef[7].round(2))\n    r_squared.append(model.score(X_s, df_y))\ndf_coefficient = pd.DataFrame(\n    {\n        'alpha':alpha,\n        'income':income_coef,\n        'limit':limit_coef,\n        'rating':rating_coef,\n        'student_Yes':student_yes_coef,\n        'r_squared':r_squared,\n    }\n)\ndf_coefficient\n\n/home/pritam/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.516e+06, tolerance: 8.434e+03\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n\n\n\nalpha\nincome\nlimit\nrating\nstudent_Yes\nr_squared\n\n\n\n\n0\n0.001\n-274.67\n442.55\n173.18\n127.74\n0.955101\n\n\n1\n0.010\n-274.64\n444.80\n170.89\n127.74\n0.955101\n\n\n2\n0.100\n-274.23\n449.54\n165.72\n127.67\n0.955099\n\n\n3\n1.000\n-270.07\n439.92\n171.01\n126.60\n0.955006\n\n\n4\n10.000\n-228.48\n335.30\n233.09\n116.13\n0.948874\n\n\n5\n100.000\n-0.00\n23.36\n273.31\n19.63\n0.718970\n\n\n6\n1000.000\n0.00\n0.00\n0.00\n0.00\n0.000000\n\n\n7\n10000.000\n0.00\n0.00\n0.00\n0.00\n0.000000\n\n\n8\n100000.000\n0.00\n0.00\n0.00\n0.00\n0.000000\n\n\n9\n1000000.000\n0.00\n0.00\n0.00\n0.00\n0.000000\n\n\n\n\n\n\n\nOnce again, \\(R^2\\) decreases monotonically.\nFinally, let’s plot the coefficients for various values of alpha. Notice that unlike ridge, with lasso the coefficients get squashed to zero.\n\nax = df_coefficient.\\\n        plot(\n            x = 'alpha',\n            y = ['income', 'limit', 'rating', 'student_Yes'],\n            title = 'Coefficient Values for Various alpha',\n            grid = True,\n            alpha = 0.75,\n            figsize = (9, 5),\n            logx=True,\n            );\nax.set_xlabel('alpha');\nax.set_ylabel('coefficient');",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Ridge and Lasso Regression</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "",
    "text": "25.0.1 Loading Packages\nSupervised machine learning attempts to predict a label from a set of features. There are two types of supervised problems; they are differentiated by the nature of the labels that are being predicted.\nIn the previous chapters, we discussed several variants of linear regression, which are simple approaches to regression. In this tutorial, we are going to introduce K-nearest-neighbors (KNN) which is a simple machine learning algorithm that can be used for both regression and classification.\nOur focus is going be on KNN as a classifier. In particular, we will set up a simple financial classification problem and use it as a way to explore various concepts in machine learning, including the variance-bias trade off and cross-validation.\nLet’s begin by loading the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#various-vix-indices",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#various-vix-indices",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.1 Various VIX Indices",
    "text": "25.1 Various VIX Indices\nLet’s begin with a brief discussion of the data we will be analyzing.\nThe VIX volatility index is published by the CBOE and is a measure of 30-day implied volatility for the S&P 500 index. Using that same methodology, the CBOE publishes other volatility measures on other stock indices and ETFs, such as the Russell 2000 and EWZ. Most of the CBOE volatility measures have a 30-day tenor, meaning they are calculated using options that have approximately 30 days to maturity.\nThere are, however, several CBOE volatility indices with different tenors. For the S&P 500, in addition to the standard 30-day VIX, there are indices with the following tenors: 9-day, 3-month, 6-month, and 1-year. The analysis in this tutorial is going to involve four of these different S&P 500 VIX tenors.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#reading-in-the-data",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#reading-in-the-data",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.2 Reading-In the Data",
    "text": "25.2 Reading-In the Data\nLet’s read-in our data set into a variable called df_vix.\n\ndf_vix = pd.read_csv('vix_knn.csv')\ndf_vix = df_vix[df_vix.trade_date &gt; '2011-01-03'] #removing the first row of NaNs\ndf_vix.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\nspy_ret\n\n\n\n\n1\n2011-01-04\n0.02\n-0.23\n-0.01\n-0.21\n-0.000551\n\n\n2\n2011-01-05\n-0.49\n-0.36\n-0.56\n-0.41\n0.005198\n\n\n3\n2011-01-06\n0.14\n0.38\n0.30\n0.09\n-0.001959\n\n\n4\n2011-01-07\n-0.70\n-0.26\n-0.06\n0.05\n-0.001962\n\n\n5\n2011-01-10\n0.80\n0.40\n0.19\n0.01\n-0.001259\n\n\n\n\n\n\n\nThis data set consists of daily SPY returns, and also daily changes in the 9-day, 30-day, 3-month, 6-month VIX indices. I excluded the 1-year index because there is limited historical data. The data is from 2011-2018.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#a-visualization-aside-pair-plots-with-seaborn",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#a-visualization-aside-pair-plots-with-seaborn",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.3 A Visualization Aside: Pair-Plots with Seaborn",
    "text": "25.3 A Visualization Aside: Pair-Plots with Seaborn\nBefore jumping into classification with KNN, let’s try the pairplot() function in the seaborn package. This function is useful for simultaneously visualizing pairwise relationships for several variables.\nLet’s apply this function to the various VIX indices in our data set.\n\nsns.pairplot(df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]);\n\n\n\n\n\n\n\n\n\nChallenge Question: How would you characterize the pairwise relationship between the various VIX tenors?\n\n\nSolution\n# All the vix indices are highly correlated with one another.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#our-simple-classification-problem",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#our-simple-classification-problem",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.4 Our Simple Classification Problem",
    "text": "25.4 Our Simple Classification Problem\nThe leverage effect is a stylized fact about equity index volatility markets; it encapsulates the observation there is an inverse relationship between returns and implied volatility. When returns are negative, implied vols increase; when returns are positive, implied vols decrease. In a previous chapter we quantified this relationship with a LinearRegression, and found the relationship to be quite strong.\nBased on our knowledge of this relationship let’s consider a simple classification exercise: identify whether a return was a “gain” or “loss”, based on changes in the various VIX indices.\nIn the language of machine learning:\n\nLabel: today’s return as a “gain” or “loss”\nFeatures: changes in the VIX indices",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#some-simple-classifiers",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#some-simple-classifiers",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.5 Some Simple Classifiers",
    "text": "25.5 Some Simple Classifiers\nFor illustrative purposes, and also for the purposes of benchmarking performance, let’s consider some simple classification schemes.\n\n25.5.1 Random Guessing\nA completely random prediction is right 50% of the time. This represents the lower-bound of performance of any learning model.\nThe code below uses DataFrame masking to check the performance of random guessing.\n\n# setting random seed\nnp.random.seed(100)\n\n# making random label predictions\ndf_vix['rand_label'] = np.random.randint(0, 2, df_vix.shape[0])\n\n# masking conditions that identify successful predictions\ncond1 = (df_vix.rand_label == 0) & (df_vix.spy_ret &lt;= 0)\ncond2 = (df_vix.rand_label == 1) & (df_vix.spy_ret &gt;= 0)\n\n# using masking to calculate the success rate\n(df_vix[cond1 | cond2].shape[0]) / (df_vix.shape[0])\n\n0.5017404276479364\n\n\n\n\n25.5.2 High Bias: Alway Guess ‘Gain’\nWhat if we always predict a gain? This is a great example of a classifier with a high bias and low variance. It’s smarter than random guessing because it is rooted in the knowledge that over long stretches of time, equity markets tend to rise (and also that markets rarely ever jump upwards).\n\n# always predict gain\n(df_vix[df_vix.spy_ret &gt; 0].shape[0]) / (df_vix.shape[0])\n\n0.548483341621084\n\n\nSince about 55% of the days in df_vix were gains for SPY, this predictor would have been right 55% of the time.\n\n\n25.5.3 Leverage Effect Rule\nWe could also create a simple rule-based classification that codifies our knowledge of the implied leverage effect. The rule could simply be that if there is an increase in the VIX, predict a SPY loss; if there is a decrease in the VIX predict an SPY gain. We will use 30-day VIX for this classifier.\nThis simple rule produces accurate labels 80% of the time, which further illustrates the strength of the implied leverate effect.\n\n# conditions that define successful predictions\ncond1 = (df_vix.vix_030 &gt;= 0) & (df_vix.spy_ret &lt;= 0)\ncond2 = (df_vix.vix_030 &lt;= 0) & (df_vix.spy_ret &gt;= 0)\n\n# calculating the proportions of successful conditions\n(df_vix[cond1 | cond2].shape[0]) / (df_vix.shape[0])\n\n0.8050721034311288\n\n\nA more sophisticated classifier involving VIX changes should perform at least as well as this, or else it is not adding much beyond simply capturing the leverage effect.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#k-nearest-neighbors-knn",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#k-nearest-neighbors-knn",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.6 K Nearest Neighbors (KNN)",
    "text": "25.6 K Nearest Neighbors (KNN)\nKNN is a simple classification algorithm that is based on the following intuitive principle: feature observations that are similar, should have similar associated labels. Feature similarity is determined by distance in Euclidean space.\nHere is how the KNN works. Suppose you are trying to predict a label for a given feature observation:\n\nFind the \\(K\\) feature samples in the the training set that are closest to your feature observation.\nFind the labels associated with those \\(K\\) closest samples.\nThe KNN prediction is the label that occurs most often in that set of \\(K\\) labels.\n\nKNN is an example of an instance-based classifier, there really is no “fitting” process other than storing the training data. The prediction algorithm amounts to calculating distances between the feature observation and the other feature observations in the train set, sorting the feature set by this distance, and then surveying the labels of the \\(K\\) closest feature observations.\n\n25.6.1 Preparing the Data\nIn anticipation of performing KNN on our data, let’s add a label column and do some additional data cleaning.\nThe following code defines a simple function that will add a column of labels to our data set: L stands for a loss, G stands for a gain.\n\ndef labeler(ret):\n    if ret &lt; 0:\n        return('L')\n    else:\n        return('G')\n\nNext, let’s add a label column called spy_label_0. We can do this conveniently with the .apply() method which has the effect of vectorizing our labeler function.\n\ndf_vix.drop(['rand_label'], axis=1, inplace=True)\ndf_vix['spy_label_0'] = df_vix['spy_ret'].apply(labeler)\ndf_vix.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\nspy_ret\nspy_label_0\n\n\n\n\n1\n2011-01-04\n0.02\n-0.23\n-0.01\n-0.21\n-0.000551\nL\n\n\n2\n2011-01-05\n-0.49\n-0.36\n-0.56\n-0.41\n0.005198\nG\n\n\n3\n2011-01-06\n0.14\n0.38\n0.30\n0.09\n-0.001959\nL\n\n\n4\n2011-01-07\n-0.70\n-0.26\n-0.06\n0.05\n-0.001962\nL\n\n\n5\n2011-01-10\n0.80\n0.40\n0.19\n0.01\n-0.001259\nL\n\n\n\n\n\n\n\n\n\n25.6.2 Predicting with 30-day VIX\nIn this section we’ll apply KNN to various subsets of our VIX features, and train our model using the entirety of our data set.\nLet’s begin by importing the contructor function that we will need.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nWe are now ready to fit our classifier model, let’s begin by predicting the current day label with the standard (30-day) VIX. We are going to use a \\(K\\) value of 10. As usual, our steps are:\n\nFeature selection: identify the data we will be using to train with.\nModel selection: instantiate the model using the constructor, and set model hyperparameters.\nFitting: using the .fit() of our instantiated mode.\n\n\n# (1) feature selection \nX = df_vix[['vix_030']]\ny = df_vix['spy_label_0'].values\n\n# (2) model selection and hyper-parameters\nclf = KNeighborsClassifier(n_neighbors = 10)\n\n# (3) fitting the model\nclf.fit(X, y)\n\nKNeighborsClassifier(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=10)\n\n\nThe most fundamental metric for investigating the quality of a classifier is accuracy, which is simply the proportion of correct classifications.\nRecall that for a LinearRegression, the .score() method gave the \\(R^2\\) of the model. For a KNeighborsClassifier the .score() method gives the accuracy.\n\nclf.score(X, y)\n\n0.8219791148682247\n\n\nOur accuracy is much better than random guessing, or a constant guess of G. However, our KNN only slightly outperforms the leverage effect rule.\n\n\n25.6.3 Predicting with Multiple VIX Indices\nLet’s now add in the other VIX indices as additional features. Ultimately, we’re interested in seeing if our in-sample accuracy improves.\nWhen considering multiple features it is important to normalize the features to make sure they are all the same order of magnitude. We can do this easily with the scale() function, who’s default behavior is to subtract the mean, and divide by the standard deviation.\n\n# importing scale function\nfrom sklearn.preprocessing import scale\n\n## (1) feature selection\nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n## (2) model selection\nclf = KNeighborsClassifier(n_neighbors = 10)\n\n## (3) fitting the model\nclf.fit(X, y)\n\n# checking in-sample accuracy score\nprint(clf.score(X, y))\n\n0.841869716558926\n\n\nAs we can see, there is a slight improvement in our in-sample accuracy.\n\nDiscussion Question: What would happen if we added option volume to our analysis but didn’t normalize our feature set?\n\n\nSolution\n# Option volumes are numbers that are much larger than VIX values.  \n# Therefore, if they are not normalized, they will dominate the fitting and \n# the VIX values will be ignored.\n\n\n\nCode Challenge: Rerun the analysis with \\(K = 1\\). What do you make of the accuracy score?\n\n\nSolution\n# importing scale function\nfrom sklearn.preprocessing import scale\n\n## (1) feature selection\nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n## (2) model selection\nclf = KNeighborsClassifier(n_neighbors = 1)\n\n## (3) fitting the model\nclf.fit(X, y)\n\n# checking in-sample accuracy score\nprint(clf.score(X, y))\n\n\n1.0",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#hold-out-sets",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#hold-out-sets",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.7 Hold-Out Sets",
    "text": "25.7 Hold-Out Sets\nSo far we have only been looking at our accuracy score in-sample, meaning we are simply calculating the accuracy score on the training set. This gives a skewed perception of how accurate the model will be on new data.\nIn order to account for this, we can partition the data set into two subsets, one for training the model and one for testing the trained model. The model_selection module contains a convenience function for splitting data into training sets and testing sets. It is called train_test_split(). Let’s import it now.\n\nfrom sklearn.model_selection import train_test_split\n\nLet’s again perform a KNN with multiple VIX indices, but this time splitting the data set. We will then calculate the accuracy score of the model on both the training set and the test set.\n\n## (1) feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# train-test_split the data\nX_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.20, random_state=0)\n\n## (2) model selection\nclf = KNeighborsClassifier(n_neighbors = 10)\n\n## (3) fitting the model\nclf.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=10)\n\n\nNow that we have fit our model, let’s check for accuracy on both the training set and the test set.\n\nprint('Train Accuracy: ', np.round(clf.score(X_train, y_train), 4))\nprint('Test Accuracy:  ', np.round(clf.score(X_test, y_test), 4))\n\nTrain Accuracy:  0.8451\nTest Accuracy:   0.8337\n\n\nNotice that the accuracy on the test set is lower than the accuracy on the training set. This is almost always the case.\n\nCoding Challenge: Copy and paste the code from the example above, and then re-run it using a \\(K=1\\). Is the spread between training accuracy and testing accuracy larger or smaller than when \\(K=10\\)?\n\n\nSolution\n## (1) feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# train-test_split the data\nX_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.20, random_state=0)\n\n## (2) model selection\nclf = KNeighborsClassifier(n_neighbors = 1)\n\n## (3) fitting the model\nclf.fit(X_train, y_train)\n\nprint('Train Accuracy: ', np.round(clf.score(X_train, y_train), 4))\nprint('Test Accuracy:  ', np.round(clf.score(X_test, y_test), 4))\n\n\nTrain Accuracy:  1.0\nTest Accuracy:   0.804",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#cross-validation",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#cross-validation",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.8 Cross-Validation",
    "text": "25.8 Cross-Validation\nIn the previous section we used train_test_split() to partition our data so that we could check the accuracy of our model on observations that were not used in the fitting process. When doing this we noticed that the accuracy of our model is lower on the testing set then on the training set. The testing set accuracy is more reflective of how the model would perform in the wild.\nBut why stop with just doing this splitting once? We could could do a train_test_split() multiple times, each time producing a different test accuracy. This collection of accuracies, in aggregate, would form a more robust measure of model performance. This is precisely the notion of model cross-validation.\nThe cross_val_score() function in the model_selection module provides a convenient way to perform cross-validation.\n\n# importing cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# model selection\nclf = KNeighborsClassifier(n_neighbors = 10)\n\n# cross validation\ncross_val_score(clf, Xs, y, cv=5)\n\narray([0.82878412, 0.77363184, 0.83333333, 0.7960199 , 0.84825871])\n\n\nThe code above splits the data into five parts, and then performs a train-test-split on each of them. Convenience functions like cross_val_score() are one of the reasons that sklearn is such a popular library. This function works the same with all kinds of different models and saves us from writing a lot of boilerplate code.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#visualizing-the-variance-bias-trade-off",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#visualizing-the-variance-bias-trade-off",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.9 Visualizing the Variance-Bias Trade-Off",
    "text": "25.9 Visualizing the Variance-Bias Trade-Off\nAs discussed in a code challenge above, when \\(K=1\\) the in-sample accuracy is perfect, but the out-of-sample accuracy is relatively poor. This is a classic illustration of over-fitting the data. By setting \\(K=1\\) we are allowing for maximum model complexity. Said in another way, we are attributing a lot of informational value to each (noisy) training observation.\nThe following code allows us to visualize this.\n\n# feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# train-test_split the data\nX_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.20, random_state=0)\n\n# various choices of neighbors\nk_neighbors = list(range(1, 20))\n\ntraining_error = []\ntesting_error = []\n\nfor k in k_neighbors:\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(X_train, y_train)\n    training_error.append(clf.score(X_train, y_train))\n    testing_error.append(clf.score(X_test, y_test))\n    \n\n# plotting training and testing errors for various K\nplt.figure(figsize=(8, 6))\nplt.plot(k_neighbors, training_error)\nplt.plot(k_neighbors, testing_error)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nAs we reduce the complexity of our algorithm (by increasing the value of \\(K\\)) the training accuracy and testing accuracy converge.\n\nDiscussion Question: Based on this graph, which \\(K\\) value would you choose and why?\n\n\nSolution\n# I would probably choose K=10.  This allows for maximum model variance, \n# while maintaining minimal bias.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#hyperparameter-selection-with-cross-validation",
    "href": "chapters/24_k_nearest_neighbors_cross_validation/k_nearest_neighbors_cross_validation.html#hyperparameter-selection-with-cross-validation",
    "title": "25  K Nearest Neighbors & Cross-Validation",
    "section": "25.10 Hyperparameter Selection with Cross-Validation",
    "text": "25.10 Hyperparameter Selection with Cross-Validation\nAbove we saw that an \\(n\\)-fold cross-validation will produce \\(n\\) different test set errors, resulting from \\(n\\) different training sets of the model. Cross-validation can be used for the purposes of hyperparameter selection because averaging over the cross-validation scores is a more robust measurement of model performance.\nIn the following code, we perform a 10-fold cross validation for \\(K=1, \\ldots, 50\\).\n\n# feature selection \nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ny = df_vix['spy_label_0'].values\n\n# scaling fetures\nXs = scale(X)\n\n# creating odd list of K for KNN\nk_neighbors = list(range(1, 50))\n\n# empty list that will hold cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in k_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n\nHyperparameter selection is usually couched in terms of misclassification rate, which is simply 1 - accuracy.\n\n# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\n\n# plot misclassification error vs k\nplt.figure(figsize=(8, 6))\nplt.plot(k_neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()\n\n\n\n\n\n\n\n\n\nDiscussion Question: Based on this graph, which value of \\(K\\) would you choose?\n\n\nSolution\n# Based on this graph I would probably choose K=15, which allows for maximal \n# model complexity with minimal misclassification.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>K Nearest Neighbors & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html",
    "href": "chapters/25_stock_prediction/stock_prediction.html",
    "title": "26  Predicting Stock Returns",
    "section": "",
    "text": "26.1 Import Packages\nOur objective in this chapter is to predict stock returns using linear regression and k-nearest neighbors (KNN). Specifically, we will try to predict the daily returns of MSFT from the returns of various correlated assets including stock indices, currencies, and other stocks.\nIn the related homework, you will test a simple trading strategy that is based on our predictions and see how it performs relative to a buy and hold strategy.\nLet’s begin by loading the packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport sklearn",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Predicting Stock Returns</span>"
    ]
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#reading-in-data",
    "href": "chapters/25_stock_prediction/stock_prediction.html#reading-in-data",
    "title": "26  Predicting Stock Returns",
    "section": "26.2 Reading-In Data",
    "text": "26.2 Reading-In Data\nNext, let’s read-in our data. We will start the stocks, whose data we will get from Yahoo Finance.\n\nstock_tickers = ['MSFT', 'IBM', 'GOOGL'] # define tickers\ndf_stock = yf.download(\n    stock_tickers, start='2005-01-01', end='2021-07-31', auto_adjust=False,\n)\ndf_stock = df_stock['Adj Close'] # select only the adjusted close price\ndf_stock.columns = df_stock.columns.str.lower() # clean-up column names\ndf_stock.rename_axis('trade_date', inplace=True) # clean-up index name\ndf_stock.rename_axis('', axis=1, inplace=True) # clean-up index name\ndf_stock\n\n[*********************100%***********************]  3 of 3 completed\n\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\n\n\ntrade_date\n\n\n\n\n\n\n\n2005-01-03\n5.038075\n50.237457\n18.454882\n\n\n2005-01-04\n4.834026\n49.697807\n18.523899\n\n\n2005-01-05\n4.809422\n49.595036\n18.482494\n\n\n2005-01-06\n4.686148\n49.440838\n18.461788\n\n\n2005-01-07\n4.817871\n49.224983\n18.406574\n\n\n...\n...\n...\n...\n\n\n2021-07-26\n133.116898\n114.338173\n279.157135\n\n\n2021-07-27\n130.996506\n114.322159\n276.733093\n\n\n2021-07-28\n135.161789\n113.537315\n276.424042\n\n\n2021-07-29\n134.847443\n113.665489\n276.694427\n\n\n2021-07-30\n133.803650\n112.888634\n275.158813\n\n\n\n\n4173 rows × 3 columns\n\n\n\nNext we’ll grab currency data from FRED.\n\ncurrency_tickers = ['JPY=X', 'GBPUSD=X']\ndf_currency = yf.download(\n    currency_tickers, start='2005-01-01', end='2021-07-31',\n    auto_adjust=False, ignore_tz=True\n)\ndf_currency = df_currency['Adj Close']\ndf_currency.columns = df_currency.columns.str.lower()\ndf_currency.rename_axis('trade_date', inplace=True)\ndf_currency.rename_axis('', axis=1, inplace=True)\ndf_currency\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\ngbpusd=x\njpy=x\n\n\ntrade_date\n\n\n\n\n\n\n2005-01-03\n1.904617\n102.739998\n\n\n2005-01-04\n1.883594\n104.339996\n\n\n2005-01-05\n1.885512\n103.930000\n\n\n2005-01-06\n1.876490\n104.889999\n\n\n2005-01-07\n1.871293\n104.889999\n\n\n...\n...\n...\n\n\n2021-07-26\n1.375781\n110.543999\n\n\n2021-07-27\n1.382915\n110.302002\n\n\n2021-07-28\n1.388272\n109.806000\n\n\n2021-07-29\n1.390685\n109.890999\n\n\n2021-07-30\n1.396433\n109.408997\n\n\n\n\n4313 rows × 2 columns\n\n\n\nFinally, we’ll grab index data from Yahoo Finance.\n\nindex_tickers = ['SPY', 'DIA', '^VIX'] \ndf_index = yf.download(\n    index_tickers, start='2005-01-01', end='2021-07-31', auto_adjust=False\n)\ndf_index = df_index['Adj Close']\ndf_index.columns = df_index.columns.str.lower().str.replace('^', '')\ndf_index.rename_axis('trade_date', inplace=True)\ndf_index.rename_axis('', axis=1, inplace=True)\ndf_index\n\n[*********************100%***********************]  3 of 3 completed\n\n\n\n\n\n\n\n\n\ndia\nspy\nvix\n\n\ntrade_date\n\n\n\n\n\n\n\n2005-01-03\n67.762741\n82.074028\n14.080000\n\n\n2005-01-04\n67.118698\n81.071129\n13.980000\n\n\n2005-01-05\n66.746147\n80.511711\n14.090000\n\n\n2005-01-06\n66.954521\n80.921043\n13.580000\n\n\n2005-01-07\n66.828232\n80.805061\n13.490000\n\n\n...\n...\n...\n...\n\n\n2021-07-26\n326.679840\n416.758026\n17.580000\n\n\n2021-07-27\n325.945343\n414.858612\n19.360001\n\n\n2021-07-28\n324.773987\n414.688538\n18.309999\n\n\n2021-07-29\n326.131287\n416.408417\n17.700001\n\n\n2021-07-30\n324.885620\n414.386200\n18.240000\n\n\n\n\n4173 rows × 3 columns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Predicting Stock Returns</span>"
    ]
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#join-and-clean-data",
    "href": "chapters/25_stock_prediction/stock_prediction.html#join-and-clean-data",
    "title": "26  Predicting Stock Returns",
    "section": "26.3 Join and Clean Data",
    "text": "26.3 Join and Clean Data\nNow we can join together our price data and convert it into returns (we actually use differences for VIX as these are more stationary). Notice that we are implicitly adding a time series component to our regression by adding lagged msft returns as a feature.\n\ndf_data = \\\n    (\n    df_stock\n        .merge(df_index, how='left', left_index=True, right_index=True) # join currency data\n        .merge(df_currency, how='left', left_index=True, right_index=True) # join index data\n        .dropna()\n        .assign(msft = lambda df: df['msft'].pct_change())   # percent change\n        .assign(msft_lag_0 = lambda df: df['msft'].shift(0)) #\n        .assign(msft_lag_1 = lambda df: df['msft'].shift(1)) #\n        .assign(ibm = lambda df: df['ibm'].pct_change())     #\n        .assign(googl = lambda df: df['googl'].pct_change()) #\n        .assign(spy = lambda df: df['spy'].pct_change())     #\n        .assign(dia = lambda df: df['dia'].pct_change())     #\n        .assign(vix = lambda df: df['vix'].diff())           # absolute change\n        .assign(dexjpus = lambda df: df['jpy=x'].pct_change()) # percent change\n        .assign(dexusuk = lambda df: df['gbpusd=x'].pct_change()) #\n        .dropna()\n    )\ndf_data\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.002235\n-0.005551\n-0.006900\n0.110001\n1.885512\n103.930000\n-0.002235\n0.003740\n-0.003929\n0.001018\n\n\n2005-01-06\n-0.025632\n-0.003109\n-0.001120\n0.003122\n0.005084\n-0.510000\n1.876490\n104.889999\n-0.001120\n-0.002235\n0.009237\n-0.004785\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.002991\n-0.001886\n-0.001433\n-0.090000\n1.871293\n104.889999\n-0.002991\n-0.001120\n0.000000\n-0.002769\n\n\n2005-01-10\n0.006242\n-0.001044\n0.004874\n0.003401\n0.004728\n-0.260000\n1.876912\n104.169998\n0.004874\n-0.002991\n-0.006864\n0.003003\n\n\n2005-01-11\n-0.007792\n-0.007107\n-0.002612\n-0.006403\n-0.006890\n-0.040000\n1.878605\n103.419998\n-0.002612\n0.004874\n-0.007200\n0.000902\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-26\n0.007668\n0.010117\n-0.002140\n0.002396\n0.002455\n0.379999\n1.375781\n110.543999\n-0.002140\n0.012336\n0.003668\n-0.001168\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.008683\n-0.002248\n-0.004558\n1.780001\n1.382915\n110.302002\n-0.008683\n-0.002140\n-0.002189\n0.005186\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.001117\n-0.003594\n-0.000410\n-1.050001\n1.388272\n109.806000\n-0.001117\n-0.008683\n-0.004497\n0.003873\n\n\n2021-07-29\n-0.002326\n0.001129\n0.000978\n0.004179\n0.004147\n-0.609999\n1.390685\n109.890999\n0.000978\n-0.001117\n0.000774\n0.001738\n\n\n2021-07-30\n-0.007741\n-0.006835\n-0.005550\n-0.003820\n-0.004856\n0.539999\n1.396433\n109.408997\n-0.005550\n0.000978\n-0.004386\n0.004133\n\n\n\n\n4140 rows × 12 columns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Predicting Stock Returns</span>"
    ]
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#training-set-and-testing-set",
    "href": "chapters/25_stock_prediction/stock_prediction.html#training-set-and-testing-set",
    "title": "26  Predicting Stock Returns",
    "section": "26.4 Training Set and Testing Set",
    "text": "26.4 Training Set and Testing Set\nWe’ll train our models on data prior to 2016, and then we’ll use data from 2016 onward for testing. So let’s separate out these two subsets of data.\n\ndf_train = df_data.query('trade_date &lt; \"2016-01-01\"')\ndf_train\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.002235\n-0.005551\n-0.006900\n0.110001\n1.885512\n103.930000\n-0.002235\n0.003740\n-0.003929\n0.001018\n\n\n2005-01-06\n-0.025632\n-0.003109\n-0.001120\n0.003122\n0.005084\n-0.510000\n1.876490\n104.889999\n-0.001120\n-0.002235\n0.009237\n-0.004785\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.002991\n-0.001886\n-0.001433\n-0.090000\n1.871293\n104.889999\n-0.002991\n-0.001120\n0.000000\n-0.002769\n\n\n2005-01-10\n0.006242\n-0.001044\n0.004874\n0.003401\n0.004728\n-0.260000\n1.876912\n104.169998\n0.004874\n-0.002991\n-0.006864\n0.003003\n\n\n2005-01-11\n-0.007792\n-0.007107\n-0.002612\n-0.006403\n-0.006890\n-0.040000\n1.878605\n103.419998\n-0.002612\n0.004874\n-0.007200\n0.000902\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2015-12-24\n-0.003474\n-0.002093\n-0.002687\n-0.003356\n-0.001650\n0.170000\n1.487697\n120.934998\n-0.002687\n0.008491\n-0.000785\n0.003615\n\n\n2015-12-28\n0.021414\n-0.004629\n0.005030\n-0.001370\n-0.002285\n1.170000\n1.493206\n120.231003\n0.005030\n-0.002687\n-0.005821\n0.003703\n\n\n2015-12-29\n0.014983\n0.015769\n0.010724\n0.011430\n0.010672\n-0.830000\n1.489403\n120.349998\n0.010724\n0.005030\n0.000990\n-0.002547\n\n\n2015-12-30\n-0.004610\n-0.003148\n-0.004244\n-0.006668\n-0.007088\n1.210001\n1.482228\n120.528999\n-0.004244\n0.010724\n0.001487\n-0.004817\n\n\n2015-12-31\n-0.015551\n-0.012344\n-0.014740\n-0.010295\n-0.010004\n0.919998\n1.481921\n120.449997\n-0.014740\n-0.004244\n-0.000655\n-0.000207\n\n\n\n\n2739 rows × 12 columns\n\n\n\n\ndf_test = df_data.query('trade_date &gt;= \"2016-01-01\"')\ndf_test\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n-0.023869\n-0.012135\n-0.012257\n-0.015519\n-0.013979\n2.490002\n1.473709\n120.310997\n-0.012257\n-0.014740\n-0.001154\n-0.005541\n\n\n2016-01-05\n0.002752\n-0.000736\n0.004562\n0.000584\n0.001692\n-1.360001\n1.471410\n119.467003\n0.004562\n-0.012257\n-0.007015\n-0.001560\n\n\n2016-01-06\n-0.002889\n-0.005005\n-0.018165\n-0.014294\n-0.012615\n1.250000\n1.467394\n119.101997\n-0.018165\n0.004562\n-0.003055\n-0.002729\n\n\n2016-01-07\n-0.024140\n-0.017089\n-0.034783\n-0.023559\n-0.023991\n4.400000\n1.462994\n118.610001\n-0.034783\n-0.018165\n-0.004131\n-0.002999\n\n\n2016-01-08\n-0.013617\n-0.009258\n0.003067\n-0.010426\n-0.010977\n2.020000\n1.462694\n117.540001\n0.003067\n-0.034783\n-0.009021\n-0.000205\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-26\n0.007668\n0.010117\n-0.002140\n0.002396\n0.002455\n0.379999\n1.375781\n110.543999\n-0.002140\n0.012336\n0.003668\n-0.001168\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.008683\n-0.002248\n-0.004558\n1.780001\n1.382915\n110.302002\n-0.008683\n-0.002140\n-0.002189\n0.005186\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.001117\n-0.003594\n-0.000410\n-1.050001\n1.388272\n109.806000\n-0.001117\n-0.008683\n-0.004497\n0.003873\n\n\n2021-07-29\n-0.002326\n0.001129\n0.000978\n0.004179\n0.004147\n-0.609999\n1.390685\n109.890999\n0.000978\n-0.001117\n0.000774\n0.001738\n\n\n2021-07-30\n-0.007741\n-0.006835\n-0.005550\n-0.003820\n-0.004856\n0.539999\n1.396433\n109.408997\n-0.005550\n0.000978\n-0.004386\n0.004133\n\n\n\n\n1401 rows × 12 columns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Predicting Stock Returns</span>"
    ]
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#training",
    "href": "chapters/25_stock_prediction/stock_prediction.html#training",
    "title": "26  Predicting Stock Returns",
    "section": "26.5 Training",
    "text": "26.5 Training\nIn order to train our model, we first put our training features into X_train and our training labels into y_train\n\nX_train = df_train.drop(columns=['msft'])[0:len(df_train)-1]\nX_train\n\n\n\n\n\n\n\n\ngoogl\nibm\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.005551\n-0.006900\n0.110001\n1.885512\n103.930000\n-0.002235\n0.003740\n-0.003929\n0.001018\n\n\n2005-01-06\n-0.025632\n-0.003109\n0.003122\n0.005084\n-0.510000\n1.876490\n104.889999\n-0.001120\n-0.002235\n0.009237\n-0.004785\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.001886\n-0.001433\n-0.090000\n1.871293\n104.889999\n-0.002991\n-0.001120\n0.000000\n-0.002769\n\n\n2005-01-10\n0.006242\n-0.001044\n0.003401\n0.004728\n-0.260000\n1.876912\n104.169998\n0.004874\n-0.002991\n-0.006864\n0.003003\n\n\n2005-01-11\n-0.007792\n-0.007107\n-0.006403\n-0.006890\n-0.040000\n1.878605\n103.419998\n-0.002612\n0.004874\n-0.007200\n0.000902\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2015-12-23\n0.001799\n0.004423\n0.010344\n0.012383\n-1.030001\n1.482338\n121.029999\n0.008491\n0.009484\n-0.001452\n-0.004936\n\n\n2015-12-24\n-0.003474\n-0.002093\n-0.003356\n-0.001650\n0.170000\n1.487697\n120.934998\n-0.002687\n0.008491\n-0.000785\n0.003615\n\n\n2015-12-28\n0.021414\n-0.004629\n-0.001370\n-0.002285\n1.170000\n1.493206\n120.231003\n0.005030\n-0.002687\n-0.005821\n0.003703\n\n\n2015-12-29\n0.014983\n0.015769\n0.011430\n0.010672\n-0.830000\n1.489403\n120.349998\n0.010724\n0.005030\n0.000990\n-0.002547\n\n\n2015-12-30\n-0.004610\n-0.003148\n-0.006668\n-0.007088\n1.210001\n1.482228\n120.528999\n-0.004244\n0.010724\n0.001487\n-0.004817\n\n\n\n\n2738 rows × 11 columns\n\n\n\nNotice that the label we are predicting is the next day msft return; the features we are using to predict are the current day returns of the various correlated assets.\n\ny_train = df_train[['msft']][1:len(df_train)]\ny_train\n\n\n\n\n\n\n\n\nmsft\n\n\ntrade_date\n\n\n\n\n\n2005-01-06\n-0.001120\n\n\n2005-01-07\n-0.002991\n\n\n2005-01-10\n0.004874\n\n\n2005-01-11\n-0.002612\n\n\n2005-01-12\n0.001871\n\n\n...\n...\n\n\n2015-12-24\n-0.002687\n\n\n2015-12-28\n0.005030\n\n\n2015-12-29\n0.010724\n\n\n2015-12-30\n-0.004244\n\n\n2015-12-31\n-0.014740\n\n\n\n\n2738 rows × 1 columns\n\n\n\n\n26.5.1 Linear Regression\nLet’s first fit a simple linear regression to our training data.\n\nfrom sklearn.linear_model import LinearRegression\nlinear_regression = LinearRegression()\nlinear_regression.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nRecall that the .score() of a Linear Regression gives the \\(R^2\\).\n\nprint(\"LR R^2:\", linear_regression.score(X_train, y_train))\n\nLR R^2: 0.017959337955331\n\n\nWe can also examine the coefficients of our model.\n\nnp.round(linear_regression.coef_, 3)\n\narray([[ 0.002, -0.016,  0.214, -0.328,  0.   , -0.003,  0.   ,  0.012,\n        -0.048, -0.002, -0.002]])\n\n\n\nCode Challenge: Implement a Lasso regression:\n\nExperiment with the alpha parameter.\n\nExamine the coefficients (.coef_ attribute) of the model\n\nDoes using a Lasso over a Linear Regression seem like a good idea?\n\n\nSolution\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\nprint(lasso.score(X_train, y_train))\nprint(lasso.coef_)\n\n# The model seems very sensitive to `alpha`; anything but \n# tiny values makes all the coefficients zero.  Moreover, the $R^2$ \n# doesn't seem to improve so Lasso isn't an improvement over \n# linear regression.\n\n\n0.0\n[-0. -0. -0. -0.  0. -0. -0. -0. -0. -0. -0.]\n\n\n\n\n\n26.5.2 KNN\nNext, let’s fit a KNN model to our data.\n\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=10)\nknn.fit(X_train, y_train)\n\nKNeighborsRegressor(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriFittedKNeighborsRegressor(n_neighbors=10) \n\n\nAs you can see, the in-sample \\(R^2\\) is higher for KNN over Linear Regression.\n\nprint(\"KNN R^2:\", knn.score(X_train, y_train))\n\nKNN R^2: 0.12124018362747302\n\n\n\n\n26.5.3 Mean-Squared Error\nAnother goodness of fit metric is the mean squared error. As you can see the models are close on this metric.\n\nprint(\"LR MSE: \", \\\n      sklearn.metrics.mean_squared_error(y_train, linear_regression.predict(X_train)))\nprint(\"KNN MSE:\", \\\n      sklearn.metrics.mean_squared_error(y_train, knn.predict(X_train)))\n\nLR MSE:  0.0002940295825237298\nKNN MSE: 0.0002631066023362797",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Predicting Stock Returns</span>"
    ]
  },
  {
    "objectID": "chapters/25_stock_prediction/stock_prediction.html#testing",
    "href": "chapters/25_stock_prediction/stock_prediction.html#testing",
    "title": "26  Predicting Stock Returns",
    "section": "26.6 Testing",
    "text": "26.6 Testing\nLet’s now test the model with the data after 2016.\n\nX_test = df_test.drop(columns=['msft'])[0:len(df_test)-1]\nX_test\n\n\n\n\n\n\n\n\ngoogl\nibm\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n-0.023869\n-0.012135\n-0.015519\n-0.013979\n2.490002\n1.473709\n120.310997\n-0.012257\n-0.014740\n-0.001154\n-0.005541\n\n\n2016-01-05\n0.002752\n-0.000736\n0.000584\n0.001692\n-1.360001\n1.471410\n119.467003\n0.004562\n-0.012257\n-0.007015\n-0.001560\n\n\n2016-01-06\n-0.002889\n-0.005005\n-0.014294\n-0.012615\n1.250000\n1.467394\n119.101997\n-0.018165\n0.004562\n-0.003055\n-0.002729\n\n\n2016-01-07\n-0.024140\n-0.017089\n-0.023559\n-0.023991\n4.400000\n1.462994\n118.610001\n-0.034783\n-0.018165\n-0.004131\n-0.002999\n\n\n2016-01-08\n-0.013617\n-0.009258\n-0.010426\n-0.010977\n2.020000\n1.462694\n117.540001\n0.003067\n-0.034783\n-0.009021\n-0.000205\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-23\n0.035769\n0.004477\n0.006633\n0.010288\n-0.490000\n1.377390\n110.139999\n0.012336\n0.016845\n-0.001043\n0.004365\n\n\n2021-07-26\n0.007668\n0.010117\n0.002396\n0.002455\n0.379999\n1.375781\n110.543999\n-0.002140\n0.012336\n0.003668\n-0.001168\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.002248\n-0.004558\n1.780001\n1.382915\n110.302002\n-0.008683\n-0.002140\n-0.002189\n0.005186\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.003594\n-0.000410\n-1.050001\n1.388272\n109.806000\n-0.001117\n-0.008683\n-0.004497\n0.003873\n\n\n2021-07-29\n-0.002326\n0.001129\n0.004179\n0.004147\n-0.609999\n1.390685\n109.890999\n0.000978\n-0.001117\n0.000774\n0.001738\n\n\n\n\n1400 rows × 11 columns\n\n\n\n\ny_test = df_test[['msft']][1:len(df_test)]\ny_test\n\n\n\n\n\n\n\n\nmsft\n\n\ntrade_date\n\n\n\n\n\n2016-01-05\n0.004562\n\n\n2016-01-06\n-0.018165\n\n\n2016-01-07\n-0.034783\n\n\n2016-01-08\n0.003067\n\n\n2016-01-11\n-0.000573\n\n\n...\n...\n\n\n2021-07-26\n-0.002140\n\n\n2021-07-27\n-0.008683\n\n\n2021-07-28\n-0.001117\n\n\n2021-07-29\n0.000978\n\n\n2021-07-30\n-0.005550\n\n\n\n\n1400 rows × 1 columns\n\n\n\nIn terms of \\(R^2\\), the Linear Regression performs better than KNN on the testing data.\n\nprint(\"LR R^2: \", linear_regression.score(X_test, y_test))\nprint(\"KNN R^2:\", knn.score(X_test, y_test))\n\nLR R^2:  0.037103603308828226\nKNN R^2: -0.02269524597068351\n\n\nOn the testing data, the models are again quite similar from an mean square error perspective.\n\nprint(\"LR MSE: \", \\\n      sklearn.metrics.mean_squared_error(y_test, linear_regression.predict(X_test)))\nprint(\"KNN MSE:\", \\\n      sklearn.metrics.mean_squared_error(y_test, knn.predict(X_test)))\n\nLR MSE:  0.0002819198180799986\nKNN MSE: 0.0002994279121680069",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Predicting Stock Returns</span>"
    ]
  },
  {
    "objectID": "chapters/26a_pca/pca.html",
    "href": "chapters/26a_pca/pca.html",
    "title": "27  Principal Components Analysis",
    "section": "",
    "text": "27.1 Import Packages\nIn a supervised learning problem we seek to predict a label from a set of features. If the label we are predicting is a continuous, we call it regression problem. If the label is discrete, we call it a classification problem.\nWe now turn our attention to unsupervised learning. The goal of unsupervised learning is not to predict a label from features. Rather, it is to discover interesting or useful things about the features themselves. In comparison to supervised learning, unsupervised learning is more ambiguous and therefore more difficult. Determining what is interesting about your features is quite subjective, and it is hard to assess how successfully you have discovered those interesting things.\nUnsupervised learning is often used in the exploratory data analysis process, or as a data preprocessing step to a supervised learning problem.\nThis chapter is an intuitive introduction to principal components analysis (PCA), which is an technique used to reduced the dimensionality of a set of features. In particular, we apply this technique to VIX term structure data.\nLet’s begin by loading the packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/26a_pca/pca.html#reading-in-data",
    "href": "chapters/26a_pca/pca.html#reading-in-data",
    "title": "27  Principal Components Analysis",
    "section": "27.2 Reading-In Data",
    "text": "27.2 Reading-In Data\nNext, let’s read-in our data; this is the same data set that we used in the k-nearest-neighbors chapter.\n\ndf_vix = pd.read_csv('vix_knn.csv')\ndf_vix = df_vix[df_vix.trade_date &gt; '2011-01-03'] #removing the first row of NaNs\ndf_vix.drop(['spy_ret'], axis=1, inplace=True) # dropping the returns (label) column\ndf_vix.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\n\n\n\n\n1\n2011-01-04\n0.02\n-0.23\n-0.01\n-0.21\n\n\n2\n2011-01-05\n-0.49\n-0.36\n-0.56\n-0.41\n\n\n3\n2011-01-06\n0.14\n0.38\n0.30\n0.09\n\n\n4\n2011-01-07\n-0.70\n-0.26\n-0.06\n0.05\n\n\n5\n2011-01-10\n0.80\n0.40\n0.19\n0.01\n\n\n\n\n\n\n\nRecall that this data set is the daily changes for various points along the implied volatility term structure. We’ll also be interested in the absolute levels of the VIX term structure, so let’s import that data now.\n\ndf_vix_ts = pd.read_csv('vix_term_structure.csv')\ndf_vix_ts.drop(['spy_ret'], axis=1, inplace=True) # dropping returns column\ndf_vix_ts.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\n\n\n\n\n0\n2011-01-03\n16.04\n17.61\n20.62\n23.40\n\n\n1\n2011-01-04\n16.06\n17.38\n20.61\n23.19\n\n\n2\n2011-01-05\n15.57\n17.02\n20.05\n22.78\n\n\n3\n2011-01-06\n15.71\n17.40\n20.35\n22.87\n\n\n4\n2011-01-07\n15.01\n17.14\n20.29\n22.92",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/26a_pca/pca.html#typical-term-structure",
    "href": "chapters/26a_pca/pca.html#typical-term-structure",
    "title": "27  Principal Components Analysis",
    "section": "27.3 Typical Term Structure",
    "text": "27.3 Typical Term Structure\nIt is useful to think of both df_vix and df_vix_ts as living in Euclidean space. However, they mostly live in certain specific subspaces of Euclidean space, and it’s the goal of PCA to give this observation a more specific meaning.\nBefore we dive into the specifics of PCA, let’s develop an intuition for what a typical VIX term structure looks like, and what kind of daily changes are typical in the term structure.\nThe first stylized fact about the VIX term structure is that it is usually monotonically increasing. We can check this with the following code.\n\ncond1 = (df_vix_ts.vix_009 &lt;= df_vix_ts.vix_030)\ncond2 = (df_vix_ts.vix_030 &lt;= df_vix_ts.vix_090)\ncond3 = (df_vix_ts.vix_090 &lt;= df_vix_ts.vix_180)\n\ndf_vix_ts[cond1 & cond2 & cond3].shape[0] / df_vix_ts.shape[0]\n\n0.6923459244532804\n\n\nNotice that about 70% of days in our data set have an unpwardly sloping term struture.\n\nCode Challenge: Find the percentage of days in which the 9-day VIX is greater than the 180-day VIX (we’ll refer to this as an inverted VIX term structure).\n\n\nSolution\ncond = (df_vix_ts.vix_009 &gt; df_vix_ts.vix_180)\ndf_vix_ts[cond].shape[0] / df_vix_ts.shape[0]\n\n\n0.11083499005964215",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/26a_pca/pca.html#typical-term-structure-changes",
    "href": "chapters/26a_pca/pca.html#typical-term-structure-changes",
    "title": "27  Principal Components Analysis",
    "section": "27.4 Typical Term-Structure Changes",
    "text": "27.4 Typical Term-Structure Changes\nNext we perform a similar analysis on the daily VIX changes, which are stored in df_vix. The following code shows that on 75% of days, all the points on the VIX term structure either rise or fall together.\n\n# decreasing\ncond1 = (df_vix.vix_009 &lt;= 0) \ncond2 = (df_vix.vix_030 &lt;= 0)\ncond3 = (df_vix.vix_090 &lt;= 0)\ncond4 = (df_vix.vix_180 &lt;= 0)\n\n# increasing\ncond5 = (df_vix.vix_009 &gt;= 0) \ncond6 = (df_vix.vix_030 &gt;= 0)\ncond7 = (df_vix.vix_090 &gt;= 0)\ncond8 = (df_vix.vix_180 &gt;= 0)\n\n\ncond = (cond1 & cond2 & cond3 & cond4) | (cond5 & cond6 & cond7 & cond8)\n\n\ndf_vix[cond].shape[0] / df_vix.shape[0]\n\n0.7454002983590253\n\n\nThis is also reflected in the strong positive correlations that we can observe in the following seaborn pair plot.\n\nsns.pairplot(df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]);",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/26a_pca/pca.html#summarizing-our-observations",
    "href": "chapters/26a_pca/pca.html#summarizing-our-observations",
    "title": "27  Principal Components Analysis",
    "section": "27.5 Summarizing Our Observations",
    "text": "27.5 Summarizing Our Observations\nLet’s summarize our observations from above:\n\nAbout 70% of the time the VIX term structure is monotonically increasing.\nAbout 10% of the time the VIX term structure is inverted with 9-day greater than 180-day.\nOn 75% of days, all points on the term structure either rise or fall together.\n\n\nDiscussion Question: If you were to construct a forecasting model for the VIX, would your model assert that all points of the term structure move completely independently of one another?\n\n\nSolution\n# Clearly no.  Most of the time there is a parallel shift, \n# so any model should account for that.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/26a_pca/pca.html#pca-overview",
    "href": "chapters/26a_pca/pca.html#pca-overview",
    "title": "27  Principal Components Analysis",
    "section": "27.6 PCA Overview",
    "text": "27.6 PCA Overview\nThe purpose of this chapter is to give an intuitive introduction to PCA and how to implement it in sklearn. We are not going to get into the mathematical specifics, which involve a fair amount of linear algebra.\nInstead, here is an overview of the essential ideas:\n\nIn PCA we are trying to understand a \\(n\\) sized set of \\(p\\)-dimensional features. Each of the \\(n\\) feature observations is a vector of \\(p\\) numbers. It is usually best to scale the features so that each component has mean zero and standard deviation 1.\nWe can think of our feature set as a collection of points living in \\(p\\)-dimensional euclidean space (\\(R^{p}\\)), with the points being centered around the origin.\nIt is often the case that there is some \\(q\\) that is smaller than \\(p\\), such that most of the variation of the features can be described by a \\(q\\) dimensional linear subspace of \\({R}^p\\).\nPCA is a technique for finding the basis of this \\(q\\) dimensional subspace.\nThe first element of this basis is the vector (which defines a line that goes through the origin) that is collectively closest to all of the feature observations. This is conceptually similar to fitting a regression line. This vector is called the first principal component. It has the property that if we project our feature observation onto that vector, the sample variance of the projections is maximized.\nThe second element of this basis is called the second principal component. It is found by considering all vectors which are perpendicular to the first principle component. Among these perpendicular vectors, the second component is the one which bests fits the the feature observations. It has the property that if we project our feature observations onto that vector, then we maximize the sample variance of the projections (among all vectors that are perpendicular to the first principle component).\nWe repeat this algorithm until we arrive at \\(q\\) principle components.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/26a_pca/pca.html#performing-a-pca-on-the-vix-changes",
    "href": "chapters/26a_pca/pca.html#performing-a-pca-on-the-vix-changes",
    "title": "27  Principal Components Analysis",
    "section": "27.7 Performing a PCA on the VIX Changes",
    "text": "27.7 Performing a PCA on the VIX Changes\nLet’s now perform a PCA on our df_vix data set using sklearn.\nWe’ll begin by normaling our features so that they have mean zero and unity standard deviation.\n\nfrom sklearn.preprocessing import scale\n\n# feature selection\nX = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\n\n# scaling fetures\nXs = scale(X)\n\nNext, we’ll instantiate our pca model and then fit it for \\(q = 3\\) components.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca.fit(Xs)\n\nPCA(n_components=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=3)\n\n\nWe can view the actual principal components by printing the .components property our model.\n\nprint(pca.components_)\n\n[[ 0.48854882  0.50863285  0.5063361   0.49622216]\n [ 0.73980156  0.15854235 -0.30757946 -0.57702067]\n [-0.40057447  0.53375678  0.4395292  -0.60121367]]\n\n\nTo see what percentage of the overall variation is explained by each of the components, we print the .explained_variance_ratio property.\n\nprint(pca.explained_variance_ratio_)\n\n[0.94602979 0.04134279 0.00840214]\n\n\nNotice that 95% the total variance is explained by the first component.\n\nCode Challenge: Try refitting the PCA with n_components=4.\n\nAre the first three components the same as before?\nWhat is the total amount of variance explained by the four componets?\nWhat happens when you set n_components=5 and try to refit?\n\n\n\nSolution\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=4)\npca.fit(Xs)\nprint(\"Components:\")\nprint(pca.components_)\nprint(\" \")\nprint(\"Variance Explaned:\")\nprint(pca.explained_variance_ratio_)\n\n\nComponents:\n[[ 0.48854882  0.50863285  0.5063361   0.49622216]\n [ 0.73980156  0.15854235 -0.30757946 -0.57702067]\n [-0.40057447  0.53375678  0.4395292  -0.60121367]\n [-0.23141694  0.65670438 -0.67515385  0.24362438]]\n \nVariance Explaned:\n[0.94602979 0.04134279 0.00840214 0.00422528]",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/26a_pca/pca.html#understanding-the-first-three-principal-components-level-slope-curvature",
    "href": "chapters/26a_pca/pca.html#understanding-the-first-three-principal-components-level-slope-curvature",
    "title": "27  Principal Components Analysis",
    "section": "27.8 Understanding the First Three Principal Components: Level, Slope, Curvature",
    "text": "27.8 Understanding the First Three Principal Components: Level, Slope, Curvature\nLet’s again look at the first three principal components.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca.fit(Xs)\nprint(\"Components:\")\nprint(pca.components_)\nprint(\" \")\nprint(\"Variance Explaned:\")\nprint(pca.explained_variance_ratio_)\n\nComponents:\n[[ 0.48854882  0.50863285  0.5063361   0.49622216]\n [ 0.73980156  0.15854235 -0.30757946 -0.57702067]\n [-0.40057447  0.53375678  0.4395292  -0.60121367]]\n \nVariance Explaned:\n[0.94602979 0.04134279 0.00840214]\n\n\nThese components have a very intuitive interpration (which is not always the case in PCA):\n\nLevel: the first component represents parallel shifts in the VIX term structure - all points increasing or all decreasing.\n\n95% of total variance explained\nthis means that all the points of the term structure are strongly positively correlated\nwe can see this in our pair-plots above, as well as the fact that 75% of days had all points going up or all points going down\n\nSlope: the second component represents the front of the curve going up, and the back of the curve going down.\n\nexplains 4% of total variance\nrecall that about 70% of days had an upward sloping term structure, and 10% of all days had a downward sloping term structure\nin order for the term structure to go from upward sloping to downward sloping, the curve would need this kind of a move (most likely in conjunction with a parallel shift)\n\nCurvature: the third component represnts changes in curvature - moving from more concave to more convex.\n\n1% of total variance",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/26b_principal_components_regression/principal_components_regression.html",
    "href": "chapters/26b_principal_components_regression/principal_components_regression.html",
    "title": "28  Principal Components Regression",
    "section": "",
    "text": "28.1 Importing Packages\nIn this chapter we will use principal components regression PCR to predict SPY returns from changes in various VIX indexes.\nThe underlying idea behind PCR is that often a small number of principal components can sufficiently explain most of the variability in the data, as well as the predictors’ relationship with the response. When utilizing PCR, we are tacitly assuming that the directions in which the predictors show the most variation are the directions that are most associated with the label. While this assumption is not guaranteed, it is reasonable enough that PCR can sometimes yield performance improvements over plain linear regression, or regularized regressions like lasso and ridge.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Principal Components Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26b_principal_components_regression/principal_components_regression.html#importing-packages",
    "href": "chapters/26b_principal_components_regression/principal_components_regression.html#importing-packages",
    "title": "28  Principal Components Regression",
    "section": "",
    "text": "28.1.1 Various VIX Indices\nNext, we give a brief discussion of the data we will be analyzing.\nThe VIX volatility index is published by the CBOE and is a measure of 30-day implied volatility for the S&P 500 index. Using that same methodology, the CBOE publishes other volatility measures on other stock indices and ETFs, such as the Russell 2000 and EWZ. Most of the CBOE volatility measures have a 30-day tenor, meaning they are calculated using options that have approximately 30 days to maturity.\nThere are, however, several CBOE volatility indices with different tenors. For the S&P 500, in addition to the standard 30-day VIX, there are indices with the following tenors: 9-day, 3-month, 6-month, and 1-year. The analysis in this chapter is going to involve four of these different S&P 500 VIX tenors.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Principal Components Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26b_principal_components_regression/principal_components_regression.html#reading-in-data",
    "href": "chapters/26b_principal_components_regression/principal_components_regression.html#reading-in-data",
    "title": "28  Principal Components Regression",
    "section": "28.2 Reading-In Data",
    "text": "28.2 Reading-In Data\nLet’s read-in our data set into a variable called df_vix. The daily SPY return is given in the column called spy_ret.\n\ndf_vix = pd.read_csv('vix_knn.csv')\ndf_vix.dropna(inplace=True) #removing the first row of NaNs\ndf_vix.head()\n\n\n\n\n\n\n\n\ntrade_date\nvix_009\nvix_030\nvix_090\nvix_180\nspy_ret\n\n\n\n\n1\n2011-01-04\n0.02\n-0.23\n-0.01\n-0.21\n-0.000551\n\n\n2\n2011-01-05\n-0.49\n-0.36\n-0.56\n-0.41\n0.005198\n\n\n3\n2011-01-06\n0.14\n0.38\n0.30\n0.09\n-0.001959\n\n\n4\n2011-01-07\n-0.70\n-0.26\n-0.06\n0.05\n-0.001962\n\n\n5\n2011-01-10\n0.80\n0.40\n0.19\n0.01\n-0.001259\n\n\n\n\n\n\n\nAnd now let’s separate our feature and our labels in preparation for fitting our regressions.\n\ndf_X = df_vix[['vix_009', 'vix_030', 'vix_090', 'vix_180']]\ndf_y = df_vix['spy_ret']",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Principal Components Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26b_principal_components_regression/principal_components_regression.html#a-visualization-aside-pairplot-with-seaborn",
    "href": "chapters/26b_principal_components_regression/principal_components_regression.html#a-visualization-aside-pairplot-with-seaborn",
    "title": "28  Principal Components Regression",
    "section": "28.3 A Visualization Aside: pairplot() with seaborn",
    "text": "28.3 A Visualization Aside: pairplot() with seaborn\nBefore jumping into our PCA regressions, let’s try the pairplot() function in the seaborn package. This function is useful for simultaneously visualizing pairwise relationships for several variables.\nLet’s apply this function to the various VIX indices in our data set.\n\nsns.pairplot(df_X, height=1.5, aspect=1);\n\n\n\n\n\n\n\n\nAs you can see, changes in the VIX indices are highly correlated with one another.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Principal Components Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26b_principal_components_regression/principal_components_regression.html#principal-components",
    "href": "chapters/26b_principal_components_regression/principal_components_regression.html#principal-components",
    "title": "28  Principal Components Regression",
    "section": "28.4 Principal Components",
    "text": "28.4 Principal Components\nWe now turn our attention to actually calculating the loading vectors (.components_) and the scores of the principal components. We begin by importing the constructor function PCA() and instantiating a model object which we will call pca.\n\nfrom sklearn.decomposition import PCA\npca = PCA()\n\nRunning the pca.fit_transform() method of our model does two things: 1. Fits the PCA which calculates the loading vectors and the component scores. 2. Returns an array that contains the component scores.\n\npc_scores = pca.fit_transform(df_X)\npd.DataFrame(pc_scores).head(10).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n-0.146930\n-0.819557\n0.393444\n-0.700098\n0.884020\n-1.168302\n-0.829475\n0.130422\n-1.310466\n1.820841\n\n\n1\n0.211505\n0.359739\n-0.286767\n-0.276346\n0.174504\n0.062142\n1.050975\n0.019997\n0.754812\n0.993667\n\n\n2\n-0.047278\n0.229324\n0.109897\n-0.077636\n0.085444\n-0.089474\n0.263591\n0.116801\n0.015106\n-0.047431\n\n\n3\n0.174600\n-0.087298\n0.087328\n-0.036313\n0.065709\n0.006939\n-0.048300\n-0.032953\n-0.062067\n-0.013547\n\n\n\n\n\n\n\nNow that our pca object is fit, we can examine how much variance is explained by each component. As you can see, 96% of the variance is explained by the first component.\n\npca.explained_variance_ratio_\n\narray([0.96160832, 0.03215176, 0.00460694, 0.00163298])",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Principal Components Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26b_principal_components_regression/principal_components_regression.html#fitting-linear-regressions-with-increasing-numbers-of-components",
    "href": "chapters/26b_principal_components_regression/principal_components_regression.html#fitting-linear-regressions-with-increasing-numbers-of-components",
    "title": "28  Principal Components Regression",
    "section": "28.5 Fitting Linear Regressions with Increasing Numbers of Components",
    "text": "28.5 Fitting Linear Regressions with Increasing Numbers of Components\nFinally, we’ll fit linear regressions with increasing numbers of principal components and analyze how the performance of the model changes. We’ll use 10-fold cross-validation root-mean-squared-error as our performance metric.\nLet’s begin by importing the functions that we need from sklearn and instantiating our model object that we will call lin_reg.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nlin_reg = LinearRegression()\n\nThe following for-loop iterates through the scores of the principal components and fits a linear regression to cumulative subsets of them. The first iteration uses the first principal component. The second iteration uses the first two principal components. The last iteration uses all the principal components.\n\n# creating lists to hold results\ncomponents_used = []\nrmse = []\n\n# performing 10-fold cross validation on and increasing number of principal components\nfor ix_component in range(1, 5):\n\n    # 10-fold cross-validation\n    cv_scores = cross_val_score(estimator=lin_reg,\n                                X=pc_scores[:, 0:ix_component],\n                                y=df_y,\n                                cv=10,\n                                scoring='neg_root_mean_squared_error')\n\n    # calculating average rmse\n    cv_rmse = -cv_scores.mean()\n\n    # appending to results list\n    components_used.append(ix_component)\n    rmse.append(cv_rmse)\n\nLet’s visualize how our linear regressions perform as we successively add principal components. Using three principal components performs the best, but just barely.\n\npd.DataFrame({\n    'num_components':components_used,\n    'rmse':rmse,\n}).plot(x='num_components', y='rmse', grid=True, xticks=range(1, 5));",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Principal Components Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26b_principal_components_regression/principal_components_regression.html#references",
    "href": "chapters/26b_principal_components_regression/principal_components_regression.html#references",
    "title": "28  Principal Components Regression",
    "section": "28.6 References",
    "text": "28.6 References\nhttps://ethanwicker.com/2021-03-14-principal-components-regression-001/",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Principal Components Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html",
    "href": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html",
    "title": "29  Principal Components Logistic Regression",
    "section": "",
    "text": "29.1 Importing Data\nNumerai is a hedge fund that crowdsources their market predictions. They disseminate data that is anonymized so that the data scientists who are working on the forecasting are not even aware of what features or labels they are working with. The prediction problem is reduced to a classification of predicting a gain or loss.\nIn this chapter we will fit principal components logistic regressions to Numerai data.\nThe underlying idea behind PCR is that often a small number of principal components can sufficiently explain most of the variability in the data, as well as the predictors’ relationship with the response. When utilizing PCR, we are tacitly assuming that the directions in which the predictors show the most variation are the directions that are most associated with the label. While this assumption is not guaranteed, it is reasonable enough that PCR can sometimes yield performance improvements over plain logistic regression.\nLet’s begin by importing the data and organizing our features and labels.\nimport pandas as pd\nimport numpy as np\ndf_data = pd.read_csv('../data/numerai_training_data.csv')\ndf_data.head().T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nfeature1\n0.499664\n0.099515\n0.671993\n0.578177\n0.474311\n\n\nfeature2\n0.951271\n0.682824\n0.383901\n0.872357\n0.639613\n\n\nfeature3\n0.127110\n0.867939\n0.533011\n0.679625\n0.563562\n\n\nfeature4\n0.469706\n0.943828\n0.690863\n0.108961\n0.169508\n\n\nfeature5\n0.188336\n0.505526\n0.176539\n0.945910\n0.456858\n\n\nfeature6\n0.113830\n0.886766\n0.600196\n0.571062\n0.580710\n\n\nfeature7\n0.917618\n0.530862\n0.381543\n0.891958\n0.969811\n\n\nfeature8\n0.398412\n0.531002\n0.648849\n0.916592\n0.357417\n\n\nfeature9\n0.418910\n0.980002\n0.831643\n0.141508\n0.157594\n\n\nfeature10\n0.452983\n0.941859\n0.861746\n0.258504\n0.251147\n\n\nfeature11\n0.167463\n0.311850\n0.712546\n0.108918\n0.344988\n\n\nfeature12\n0.430536\n0.133233\n0.406130\n0.477112\n0.289470\n\n\nfeature13\n0.137192\n0.642640\n0.520068\n0.037959\n0.038095\n\n\nfeature14\n0.201437\n0.533367\n0.660924\n0.604539\n0.770200\n\n\nfeature15\n0.507708\n0.616879\n0.538882\n0.974103\n0.697395\n\n\nfeature16\n0.919475\n0.697038\n0.160117\n0.187519\n0.792327\n\n\nfeature17\n0.978169\n0.741461\n0.765317\n0.938254\n0.711650\n\n\nfeature18\n0.177080\n0.086690\n0.301772\n0.560129\n0.177080\n\n\nfeature19\n0.101372\n0.109533\n0.352097\n0.136483\n0.247403\n\n\nfeature20\n0.722138\n0.324666\n0.638205\n0.284507\n0.666598\n\n\nfeature21\n0.832319\n0.552276\n0.383552\n0.199446\n0.755557\n\n\ntarget\n0.000000\n1.000000\n0.000000\n1.000000\n0.000000\ndf_X = df_data.drop(columns='target').copy()\ndf_y = df_data['target'].copy()",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Principal Components Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#the-data-is-clean",
    "href": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#the-data-is-clean",
    "title": "29  Principal Components Logistic Regression",
    "section": "29.2 The Data is Clean",
    "text": "29.2 The Data is Clean\nOne nice thing about working with the Numerai data is that it is clean and normalized:\n\nAll the features are in the range of \\([0, 1]\\).\nAll the features have a mean of 0.50 and a standard deviation of 0.28.\nThe occurrence of labels is balanced at 50% gains and 50% losses.\n\n\ndf_X.mean()\n\nfeature1     0.511372\nfeature2     0.492770\nfeature3     0.492105\nfeature4     0.499420\nfeature5     0.502291\nfeature6     0.493039\nfeature7     0.480280\nfeature8     0.494526\nfeature9     0.492926\nfeature10    0.489265\nfeature11    0.495725\nfeature12    0.510969\nfeature13    0.489852\nfeature14    0.509350\nfeature15    0.487469\nfeature16    0.509012\nfeature17    0.488944\nfeature18    0.484929\nfeature19    0.491757\nfeature20    0.509223\nfeature21    0.498371\ndtype: float64\n\n\n\ndf_X.std()\n\nfeature1     0.282260\nfeature2     0.287446\nfeature3     0.282481\nfeature4     0.284493\nfeature5     0.289867\nfeature6     0.287061\nfeature7     0.287526\nfeature8     0.288087\nfeature9     0.293945\nfeature10    0.287046\nfeature11    0.290922\nfeature12    0.285451\nfeature13    0.291276\nfeature14    0.290140\nfeature15    0.286997\nfeature16    0.289279\nfeature17    0.284790\nfeature18    0.290445\nfeature19    0.283742\nfeature20    0.291001\nfeature21    0.289637\ndtype: float64\n\n\nNotice that a guess of increase for all assets would yield an accuracy of 50.5%.\n\ndf_y.mean()\n\n0.5051702657807309",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Principal Components Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#analyzing-correlations",
    "href": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#analyzing-correlations",
    "title": "29  Principal Components Logistic Regression",
    "section": "29.3 Analyzing Correlations",
    "text": "29.3 Analyzing Correlations\nPCA regression tends to work best when there are a large number of features, and many of the features are highly correlated (i.e. lots of multi-colinearity). So let’s analyze the correlation structure of our features. We begin by using the DataFrame.corr() method in pandas and then applying a heat-map with the seaborn package.\n\nimport seaborn as sns\nsns.heatmap(df_X.corr());\n\n\n\n\n\n\n\n\nI find the heat-map a little hard to read, so let’s try a different approach. In particular, we’ll create a histogram of all the correlations.\nI begin by creating a the correlation matrix using numpy and then I fill the diagonal with np.nans because I only want to focus on the pairwise correlations (why??).\n\ncorr = np.corrcoef(df_X, rowvar=False) # calculating correlation matrix\nnp.fill_diagonal(corr, np.nan) # filling the diagonal with nans\nsns.histplot(corr.flatten(), legend=False); # plotting histogram \n\n\n\n\n\n\n\n\nAs we can see, all our correlations are in the range of -60% to +85% and there aren’t that many on the high range - most are in the range of -40% to +40%. So I wouldn’t suspect that PCA regression is going to help performance that much.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Principal Components Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#principal-components",
    "href": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#principal-components",
    "title": "29  Principal Components Logistic Regression",
    "section": "29.4 Principal Components",
    "text": "29.4 Principal Components\nLet’s now turn our attention to actually calculating the loading vectors (.components_) and the scores of the principal components. We begin by importing the constructor function PCA() and instantiating a model object which we will call pca.\n\nfrom sklearn.decomposition import PCA\npca = PCA()\n\nRunning the pca.fit_transform() method of our model does two things: 1. Fits the PCA which calculates the loading vectors and the component scores. 2. Returns an array that contains the component scores.\n\npc_scores = pca.fit_transform(df_X)\npd.DataFrame(pc_scores).head(10).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n0.108582\n0.047151\n-0.090215\n1.353407\n0.543936\n-1.337931\n-1.065158\n-0.897421\n-0.009373\n-0.126169\n\n\n1\n-0.574424\n-0.716723\n-0.211012\n0.273582\n-0.291230\n-0.512642\n-0.327780\n-0.489651\n-0.315189\n-0.422283\n\n\n2\n0.537864\n0.494604\n-0.096291\n0.451725\n0.491375\n-0.065085\n-0.547790\n0.823222\n-0.750135\n-0.353828\n\n\n3\n0.498702\n0.406678\n0.226609\n0.129886\n0.045200\n-0.113888\n-0.041263\n-0.300901\n-0.038114\n-0.574432\n\n\n4\n-0.587933\n0.489419\n0.412921\n0.066838\n-0.686664\n0.213407\n-0.238949\n0.207119\n-0.039683\n0.336750\n\n\n5\n0.456096\n-0.508008\n-0.250760\n0.185151\n-0.092568\n0.031041\n0.643533\n0.040054\n-0.214722\n0.228547\n\n\n6\n0.279154\n-0.373310\n-0.040299\n0.118741\n0.321192\n-0.059057\n-0.296018\n0.175180\n-0.067271\n0.003632\n\n\n7\n0.546058\n-0.117602\n-0.015733\n0.150985\n-0.087823\n0.108120\n0.124292\n-0.246969\n-0.200124\n0.058870\n\n\n8\n0.082506\n0.295906\n-0.389367\n-0.077537\n-0.096548\n0.052603\n0.090922\n-0.152764\n-0.127075\n-0.094810\n\n\n9\n-0.132753\n-0.058394\n-0.161979\n0.054263\n-0.186621\n0.073997\n0.006181\n-0.038866\n-0.227574\n-0.058603\n\n\n10\n0.020061\n0.114448\n0.208979\n0.008523\n-0.003582\n0.036240\n-0.057146\n-0.239290\n-0.062074\n-0.006718\n\n\n11\n-0.124765\n0.108493\n-0.050232\n-0.048477\n-0.192685\n0.196224\n-0.006818\n-0.047599\n-0.033631\n0.055577\n\n\n12\n-0.061971\n0.066483\n0.032047\n0.079159\n-0.038498\n-0.239024\n-0.000349\n0.158037\n0.056380\n-0.020836\n\n\n13\n0.155951\n0.183960\n-0.075008\n0.042270\n0.186666\n0.083251\n0.130578\n0.035460\n-0.113674\n0.079207\n\n\n14\n0.094436\n0.015976\n0.352847\n0.000650\n-0.058494\n0.106945\n-0.081955\n-0.043160\n0.039765\n-0.097706\n\n\n15\n-0.029417\n0.205648\n0.200973\n0.012170\n-0.003544\n-0.006003\n-0.208144\n-0.020886\n0.116896\n-0.113330\n\n\n16\n-0.142508\n0.097671\n-0.014369\n0.027975\n0.029881\n0.169603\n-0.022507\n-0.093937\n-0.100857\n0.129521\n\n\n17\n0.163273\n0.092256\n0.297362\n-0.023787\n-0.001466\n-0.138810\n-0.126334\n0.051337\n-0.110244\n-0.009919\n\n\n18\n0.017856\n-0.085099\n0.085173\n0.122379\n-0.019062\n0.035983\n-0.016196\n0.112267\n0.080343\n0.126598\n\n\n19\n0.059728\n0.002044\n0.077430\n-0.020641\n0.004201\n-0.007187\n0.131228\n0.003170\n0.002344\n-0.008848\n\n\n20\n-0.032296\n-0.069026\n0.010650\n-0.042135\n0.002415\n-0.002586\n-0.026838\n-0.093424\n0.090696\n-0.026539\n\n\n\n\n\n\n\nNow that our pca object is fit, we can examine how much variance is explained by each component. As you can see, 31% of the variance is explained by the first component.\n\npca.explained_variance_ratio_\n\narray([0.31066213, 0.15675227, 0.12543453, 0.07790119, 0.06755613,\n       0.06360026, 0.03925333, 0.03386603, 0.01977618, 0.01712178,\n       0.01380059, 0.01331539, 0.01199815, 0.00946309, 0.00812939,\n       0.00801006, 0.00682141, 0.00512116, 0.00459589, 0.00373127,\n       0.00308978])\n\n\nIt’s a little more enlightening to consider how much variance is explained by the first \\(n\\) components cumulatively. We can visualize this easily with the following code.\n\npd.DataFrame({\n    'num_components': range(1, 22),\n    'explained_variance':pca.explained_variance_ratio_.cumsum(),\n}).plot(x='num_components', y='explained_variance', grid=True, xticks=range(22));\n\n\n\n\n\n\n\n\nNotice that the first 9 (out of 21) components account for 90% of the variability in the features.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Principal Components Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#fitting-logistic-regressions-with-increasing-numbers-of-components",
    "href": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#fitting-logistic-regressions-with-increasing-numbers-of-components",
    "title": "29  Principal Components Logistic Regression",
    "section": "29.5 Fitting Logistic Regressions with Increasing Numbers of Components",
    "text": "29.5 Fitting Logistic Regressions with Increasing Numbers of Components\nFinally, we’ll fit logistic regressions with increasing numbers of principal components and analyze how the performance of the model changes. We’ll use 10-fold cross-validation accuracy as our performance metric.\nLet’s begin by importing the functions that we need from sklearn and instantiating our model object that we will call log_reg.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nlog_reg = LogisticRegression()\n\nThe following for-loop iterates through the scores of the principal components and fits a logistic regression to cumulative subsets of them. The first iteration uses the first principal component. The second iteration uses the first two principal components. The last iteration uses all the principal components.\n\n# creating lists to hold results\ncomponents_used = []\naccuracy = []\n\n# performing 10-fold cross validation on and increasing number of principal components\nfor ix_component in range(1, 21):\n\n    # 10-fold cross-validation\n    cv_scores = cross_val_score(estimator=log_reg,\n                                X=pc_scores[:, 0:ix_component],\n                                y=df_y,\n                                cv=10,\n                                scoring=\"accuracy\")\n\n    # calculating average accuracy\n    cv_accuracy = cv_scores.mean()\n\n    # appending to results list\n    components_used.append(ix_component)\n    accuracy.append(cv_accuracy)\n\nLet’s visualize how our logistic regressions perform as we successively add principal components. Using all principal components has the highest accuracy, but using about 12 seems to preform relatively well.\n\npd.DataFrame({\n    'num_components':components_used,\n    'accuracy':accuracy,\n}).plot(x='num_components', y='accuracy', grid=True, xticks=range(22));",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Principal Components Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#references",
    "href": "chapters/26c_principal_components_logistic_regression/principal_components_logistic_regression.html#references",
    "title": "29  Principal Components Logistic Regression",
    "section": "29.6 References",
    "text": "29.6 References\nhttps://ethanwicker.com/2021-03-14-principal-components-regression-001/",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Principal Components Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html",
    "title": "30  K-Means Clustering",
    "section": "",
    "text": "30.1 Importing Packages\nK-means clustering is an unsupervised learning technique that groups collections of feature observations together into a pre-specified number of groups.\nIn this chapter we will perform k-means clustering to backtest data of a volatility trading strategy applied to various ETF underlyings. In particular, we will cluster the underlyings based on beta/pnl. In the related homework assignment, you will be asked to perform clustering by beta/stdev as well as stdev/pnl.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>K-Means Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#reading-in-data",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#reading-in-data",
    "title": "30  K-Means Clustering",
    "section": "30.2 Reading-In Data",
    "text": "30.2 Reading-In Data\nNext let’s read-in our data, which consists of performance statistics of the various underlyings over the backtest period. In particular, we will be interested in:\n\npnl - daily average pnl (multplied by 100)\nbeta - the beta to the strategy applied to SPY (not SPY itself)\nstdev - the standard deviation of the daily PNLs.\n\n\ndf_cluster = pd.read_csv(\"strategy.csv\")\ndf_cluster.head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\n\n\n\n\n0\nDIA\n0.010383\n0.014772\n0.824586\n0.579557\n0.613557\n\n\n1\nEEM\n0.025753\n0.014772\n0.550046\n0.958898\n0.457594\n\n\n2\nEFA\n0.021988\n0.014772\n0.608328\n0.905449\n-0.582501\n\n\n3\nEMB\n0.005283\n0.014772\n0.185949\n0.066495\n-0.117455\n\n\n4\nEWH\n0.037914\n0.014772\n0.134310\n0.344708\n0.036128\n\n\n\n\n\n\n\nWe will also import some additional information about the universe of underlyings which we will find useful later in our analysis.\n\ndf_universe = df_cluster[['underlying']]\ndf = pd.read_csv('universe.csv')\ndf_universe = df_universe.merge(df, how='inner', on='underlying')\ndf_universe.head()\n\n\n\n\n\n\n\n\nunderlying\nname\n\n\n\n\n0\nDIA\nSPDR DOW JONES INDL AVERAGE ET UT SER 1\n\n\n1\nEEM\nISHARES TR MSCI EMG MKT ETF\n\n\n2\nEFA\nISHARES TR MSCI EAFE ETF\n\n\n3\nEMB\nISHARES TR JPMORGAN USD EMG\n\n\n4\nEWH\nISHARES INC MSCI HONG KG ETF",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>K-Means Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#checking-for-outliers",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#checking-for-outliers",
    "title": "30  K-Means Clustering",
    "section": "30.3 Checking for Outliers",
    "text": "30.3 Checking for Outliers\nLet’s plot our data quickly in order to check for potential outliers.\n\nwith sns.axes_style('whitegrid'):\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , color = 'black'\n            , alpha = 0.75\n            , height=7 \n            , aspect=1.3\n            , s=70\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>K-Means Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#wrangling-removing-outliers",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#wrangling-removing-outliers",
    "title": "30  K-Means Clustering",
    "section": "30.4 Wrangling: Removing Outliers",
    "text": "30.4 Wrangling: Removing Outliers\nThere seem to be a few extreme data point, which is likely the result of bad data. Let’s write a query to isolate these.\n\ndf_cluster.query(\"pnl &lt; -8 or pnl &gt; 5\")\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\n\n\n\n\n13\nGDX\n0.061813\n0.014772\n0.197046\n0.824502\n-8.315433\n\n\n19\nSLV\n0.048230\n0.014772\n0.221409\n0.722864\n5.975203\n\n\n23\nUNG\n0.082083\n0.014772\n0.041565\n0.230951\n-8.397742\n\n\n\n\n\n\n\nWe can see that the offenders are GDX, SLV, and UNG. Let’s remove these manually now.\n\ndf_cluster = df_cluster[~df_cluster['underlying'].isin(['UNG', 'SLV', 'GDX'])].reset_index(drop=True)\ndf_cluster.head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\n\n\n\n\n0\nDIA\n0.010383\n0.014772\n0.824586\n0.579557\n0.613557\n\n\n1\nEEM\n0.025753\n0.014772\n0.550046\n0.958898\n0.457594\n\n\n2\nEFA\n0.021988\n0.014772\n0.608328\n0.905449\n-0.582501\n\n\n3\nEMB\n0.005283\n0.014772\n0.185949\n0.066495\n-0.117455\n\n\n4\nEWH\n0.037914\n0.014772\n0.134310\n0.344708\n0.036128\n\n\n\n\n\n\n\nLet’s regraph our cleaned data.\n\nwith sns.axes_style('whitegrid'):\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , color = 'black'\n            , alpha = 0.75\n            , height=7 \n            , aspect=1.3\n            , s=70\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');\n\n\n\n\n\n\n\n\n\nDiscussion Question: Inspect the graph above and see if you can predict a K-means clustering based on four clusters.\n\n\nSolution\n#",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>K-Means Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#k-means-clustering---first-try",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#k-means-clustering---first-try",
    "title": "30  K-Means Clustering",
    "section": "30.5 K-Means Clustering - First Try",
    "text": "30.5 K-Means Clustering - First Try\nWe are now ready to fit our clustering model. We begin by importing the KMeans constructor.\n\nfrom sklearn.cluster import KMeans\n\nNext, we will isolate the features we want to use for grouping.\n\nX = df_cluster[['beta', 'pnl']]\nX.head()\n\n\n\n\n\n\n\n\nbeta\npnl\n\n\n\n\n0\n0.579557\n0.613557\n\n\n1\n0.958898\n0.457594\n\n\n2\n0.905449\n-0.582501\n\n\n3\n0.066495\n-0.117455\n\n\n4\n0.344708\n0.036128\n\n\n\n\n\n\n\nWe are now ready to fit our clustering model. I chose to use 4 groups in the hope that the groupings would align with the following categories:\n\nhigh beta, high pnl\nhigh beta, low pnl\nlow beta, high pnl\nlow beta, low pnl\n\n\nkmeans = KMeans(n_clusters = 4, n_init=100, random_state=0)\nkmeans.fit(X)\n\nKMeans(n_clusters=4, n_init=100, random_state=0)\n\n\nLet’s add a column to our original data frame that includes the group number.\n\ndf_cluster['group'] = kmeans.labels_\ndf_cluster['group'] = df_cluster['group'].apply(str)\ndf_cluster.sort_values('group').head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\ngroup\n\n\n\n\n0\nDIA\n0.010383\n0.014772\n0.824586\n0.579557\n0.613557\n0\n\n\n31\nXLU\n0.011834\n0.014772\n0.236587\n0.189522\n-0.695763\n0\n\n\n30\nXLP\n0.009352\n0.014772\n0.408279\n0.258476\n-0.653215\n0\n\n\n28\nXLI\n0.018132\n0.014772\n0.605048\n0.742666\n-0.576125\n0\n\n\n27\nXLF\n0.031330\n0.014772\n0.444355\n0.942399\n-0.571652\n0\n\n\n\n\n\n\n\nAnd now let’s graph our clustering with colors to show the grouping.\n\nwith sns.axes_style('whitegrid'):\n    sns.color_palette(\"Paired\")\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , height=7 \n            , aspect=1.3\n            , hue='group'\n            , palette=[\"b\", \"r\", 'k', 'y']\n            , s=100\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');\n\n\n\n\n\n\n\n\n\nDiscussion Question: What are your observations about this clustering?\n\n\nSolution\n# The PNL feature is dominating the clustering because of scale.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>K-Means Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/27_k_means_clustering/k_means_clustering.html#k-means-clustering---second-try",
    "href": "chapters/27_k_means_clustering/k_means_clustering.html#k-means-clustering---second-try",
    "title": "30  K-Means Clustering",
    "section": "30.6 K-Means Clustering - Second Try",
    "text": "30.6 K-Means Clustering - Second Try\nAs we can see from the previous section, unless we perform scaling, the pnl feature will dominate because its values are an order of magnitude bigger than the beta feature.\nSo let’s scale and perform our analysis again.\n\nfrom sklearn.preprocessing import scale\nXs = scale(X)\nkmeans = KMeans(n_clusters = 4, n_init=100, random_state=0)\nkmeans.fit(Xs)\n\nKMeans(n_clusters=4, n_init=100, random_state=0)\n\n\n\ndf_cluster['group'] = kmeans.labels_\ndf_cluster['group'] = df_cluster['group'].apply(str)\ndf_cluster = df_cluster.sort_values(['group', 'underlying']).merge(df_universe)\ndf_cluster.head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\ngroup\nname\n\n\n\n\n0\nEEM\n0.025753\n0.014772\n0.550046\n0.958898\n0.457594\n0\nISHARES TR MSCI EMG MKT ETF\n\n\n1\nEFA\n0.021988\n0.014772\n0.608328\n0.905449\n-0.582501\n0\nISHARES TR MSCI EAFE ETF\n\n\n2\nEWJ\n0.025301\n0.014772\n0.343123\n0.587681\n-2.597758\n0\nISHARES INC MSCI JPN ETF NEW\n\n\n3\nEWU\n0.050743\n0.014772\n0.268563\n0.922505\n-1.406626\n0\nISHARES TR MSCI UK ETF NEW\n\n\n4\nEWW\n0.026251\n0.014772\n0.372520\n0.661983\n-0.608147\n0\nISHARES INC MSCI MEXICO ETF\n\n\n\n\n\n\n\nAnd let’s graph our new grouping.\n\nwith sns.axes_style('whitegrid'):\n    sns.color_palette(\"Paired\")\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , height=7 \n            , aspect=1.3\n            , hue='group'\n            , palette=[\"b\", \"r\", 'k', 'y']\n            , s=100\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');\n\n\n\n\n\n\n\n\n\nDiscussion Question: What are your observations about this grouping?\n\n\nSolution\n# group 0: high beta, zeroish pnl\n# group 1: low beta, zeroish pnl\n# group 2: extremely low performers\n# group 3: high beta, high performers\n\n# group 3 consists of: QQQ, SPY, IWM, USO, EWZ, FXI -- three of these are major stock indices, which is what I would expect\n# group 0 contains 7 of the 11 SPDR sector select funds (X--), which makes sense based on what I know of the strategy\n# group 0 also contains 5 emerging market related funds (E--), I don't have an opinion on this \n\n\n\nCode Challenge: Redo the clustering for 3 groups.\n\n\nSolution\n# fitting\nX = df_cluster[['beta', 'pnl']]\nXs = scale(X)\nkmeans = KMeans(n_clusters = 3, n_init=100, random_state=0)\nkmeans.fit(Xs)\n# adding labels to df_cluster\ndf_cluster['group'] = kmeans.labels_\ndf_cluster['group'] = df_cluster['group'].apply(str)\ndf_cluster.sort_values('group')\n# graphing\nwith sns.axes_style('whitegrid'):\n    sns.color_palette(\"Paired\")\n    g = sns.relplot(\n            x='beta'\n            , y='pnl'\n            , data=df_cluster\n            , height=7 \n            , aspect=1.3\n            , hue='group'\n            , palette=[\"b\", \"r\", 'k']\n            , s=100\n        );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('beta vs pnl');",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>K-Means Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html",
    "href": "chapters/28_pca_visualization/pca_visualization.html",
    "title": "31  Visualizing Principal Components Analysis",
    "section": "",
    "text": "31.1 Importing Packages\nIn this chapter we will explore some visualizations of principal components analysis applied to weekly PNL data from an options trading backtest applied to various underlyings. This is the same data set we analyzed in the K-means Clustering chapter.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Visualizing Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#reading-in-data",
    "href": "chapters/28_pca_visualization/pca_visualization.html#reading-in-data",
    "title": "31  Visualizing Principal Components Analysis",
    "section": "31.2 Reading-In Data",
    "text": "31.2 Reading-In Data\nNext, let’s read-in the data that we will analyze. This data consists of weekly PNLs of an options trading strategy that is applied to various underlyings.\n\ndf_strategy = pd.read_csv('strategy_pca.csv')\ndf_strategy\n\n\n\n\n\n\n\n\ntrade_date\nDIA\nEEM\nEFA\nEWJ\nEWW\nEWZ\nFXE\nFXI\nFXY\n...\nXLF\nXLI\nXLK\nXLP\nXLU\nXLV\nXLY\nXME\nXOP\nXRT\n\n\n\n\n0\n2010-06-11\n0.007174\n-0.017362\n-0.010725\n-0.026382\n-0.025379\n-0.013350\n-0.002375\n-0.009553\n-0.003428\n...\n-0.023686\n-0.024645\n-0.019757\n-0.012389\n-0.008406\n-0.012576\n-0.020438\n-0.032221\n-0.051404\n-0.019148\n\n\n1\n2010-06-14\n0.010173\n0.035633\n0.014950\n0.014282\n0.033116\n0.033594\n-0.000735\n0.019362\n0.005734\n...\n0.003849\n0.044187\n0.049763\n0.015300\n0.011585\n0.019370\n0.040846\n0.060549\n0.047449\n0.030016\n\n\n2\n2010-06-15\n-0.002297\n-0.013010\n-0.019534\n-0.005922\n0.000404\n-0.004744\n-0.001553\n-0.000144\n0.002334\n...\n-0.000972\n-0.010887\n-0.027390\n0.010241\n0.006655\n0.014511\n-0.014392\n0.037345\n0.016062\n0.032434\n\n\n3\n2010-06-16\n0.010865\n0.023527\n0.020398\n0.009137\n0.016342\n0.032344\n0.003108\n0.027393\n0.004025\n...\n0.024156\n0.021204\n0.016975\n-0.000245\n-0.002232\n0.006271\n0.030290\n0.024339\n0.040401\n-0.004622\n\n\n4\n2010-06-17\n0.007654\n0.021416\n0.013009\n0.019784\n0.012869\n0.020282\n0.002741\n0.005559\n0.003677\n...\n0.029602\n-0.001968\n-0.006714\n0.003149\n0.011234\n-0.000091\n0.004604\n0.027596\n0.021323\n0.030434\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n749\n2018-12-21\n-0.010358\n-0.008775\n-0.025460\n-0.058596\n-0.009828\n0.004381\n-0.002566\n0.011680\n-0.001237\n...\n-0.052536\n-0.026648\n-0.070161\n-0.017729\n0.102341\n-0.017573\n-0.008129\n-0.101895\n0.012906\n-0.089217\n\n\n750\n2018-12-24\n-0.037064\n0.004238\n-0.017792\n-0.007173\n0.021813\n0.007675\n-0.000743\n0.002134\n-0.006349\n...\n0.000666\n-0.027368\n-0.101256\n-0.080759\n-0.141095\n-0.043487\n-0.168685\n0.164792\n-0.190494\n-0.006617\n\n\n751\n2018-12-26\n-0.074702\n0.023863\n0.015465\n0.014819\n0.047261\n0.035732\n0.000935\n0.021011\n0.001450\n...\n-0.033270\n-0.105362\n-0.053457\n-0.002229\n0.016203\n-0.033066\n-0.079741\n-0.138356\n-0.422292\n-0.074324\n\n\n752\n2018-12-27\n0.015387\n0.016627\n0.013537\n0.013345\n0.028583\n0.016973\n-0.002762\n0.007095\n0.001280\n...\n0.025021\n0.013005\n0.033152\n0.012723\n0.011532\n0.012957\n0.020950\n0.073538\n0.067077\n0.016728\n\n\n753\n2018-12-28\n0.025541\n0.011025\n0.007857\n0.014097\n0.020565\n0.020779\n0.001501\n0.020864\n-0.003356\n...\n0.024544\n0.033024\n0.025723\n0.009940\n0.020296\n0.030015\n0.030467\n0.024561\n0.059169\n0.031644\n\n\n\n\n754 rows × 35 columns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Visualizing Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#fitting-pca",
    "href": "chapters/28_pca_visualization/pca_visualization.html#fitting-pca",
    "title": "31  Visualizing Principal Components Analysis",
    "section": "31.3 Fitting PCA",
    "text": "31.3 Fitting PCA\nWe are now ready to fit a PCA to this PNL data; let’s start by scaling.\n\nfrom sklearn.preprocessing import scale\nXs = scale(df_strategy.loc[:, df_strategy.columns != 'trade_date'])\nXs\n\narray([[ 0.55349196, -0.7868123 , -0.40985713, ..., -0.53168657,\n        -1.11237419, -1.01039618],\n       [ 0.77882186,  1.61951344,  0.71837458, ...,  1.2393787 ,\n         1.01447345,  1.72395022],\n       [-0.15822341, -0.58920351, -0.796928  , ...,  0.79640469,\n         0.33917233,  1.85846243],\n       ...,\n       [-5.59881083,  1.08506556,  0.74096939, ..., -2.55791817,\n        -9.09217992, -4.07915548],\n       [ 1.17060449,  0.75653352,  0.65627559, ...,  1.48735182,\n         1.43678679,  0.98492788],\n       [ 1.93360316,  0.50215785,  0.40666156, ...,  0.55234503,\n         1.26664176,  1.81448399]])\n\n\nNext we’ll fit the first five principal components.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=5)\npca.fit(Xs)\n\nPCA(n_components=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=5)\n\n\n\nCode Challenge: What is the percent of variance explained of the first principal component?\n\n\nSolution\npca.explained_variance_ratio_\n\n\narray([0.40549205, 0.07136019, 0.04928076, 0.04223706, 0.03744758])",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Visualizing Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#visualizing-loading-vectors",
    "href": "chapters/28_pca_visualization/pca_visualization.html#visualizing-loading-vectors",
    "title": "31  Visualizing Principal Components Analysis",
    "section": "31.4 Visualizing Loading Vectors",
    "text": "31.4 Visualizing Loading Vectors\nNext, we will visualize the loading vectors. In order to do this, let’s begin by creating a DataFrame to hold them.\n\ndf_components = \\\n    pd.DataFrame({\n        'underlying':df_strategy.drop(columns=['trade_date']).columns,\n        'PCA1':-pca.components_[0],\n        'PCA2':-pca.components_[1],\n        'PCA3':-pca.components_[2],\n        'PCA4':-pca.components_[3],\n        'PCA5':-pca.components_[4],\n    })\ndf_components.head()\n\n\n\n\n\n\n\n\nunderlying\nPCA1\nPCA2\nPCA3\nPCA4\nPCA5\n\n\n\n\n0\nDIA\n0.239596\n-0.169226\n0.090560\n0.015176\n0.005542\n\n\n1\nEEM\n0.222856\n0.125537\n-0.084749\n0.123141\n-0.089064\n\n\n2\nEFA\n0.204699\n0.084143\n-0.030489\n0.260802\n-0.137933\n\n\n3\nEWJ\n0.141398\n0.016348\n-0.005798\n0.202220\n0.008194\n\n\n4\nEWW\n0.146514\n0.136649\n-0.138207\n0.125954\n-0.217323\n\n\n\n\n\n\n\nLet’s also add in the grouping from the cluster analysis we performed in the previous tutorial. We being by reading in a CSV that contains the grouping.\n\ndf_cluster = pd.read_csv('cluster_analysis.csv')\ndf_cluster.head()\n\n\n\n\n\n\n\n\nunderlying\nstdev\nspy_stdev\ncorr_spy\nbeta\npnl\ngroup\nname\n\n\n\n\n0\nEEM\n0.025753\n0.014772\n0.550046\n0.958898\n0.457594\n0\nISHARES TR MSCI EMG MKT ETF\n\n\n1\nEWZ\n0.045733\n0.014772\n0.388278\n1.202046\n4.057360\n0\nISHARES INC MSCI BRAZIL ETF\n\n\n2\nFXI\n0.030682\n0.014772\n0.433179\n0.899703\n1.700224\n0\nISHARES TR CHINA LG-CAP ETF\n\n\n3\nIWM\n0.019681\n0.014772\n0.756170\n1.007407\n2.827085\n0\nISHARES TR RUSSELL 2000 ETF\n\n\n4\nQQQ\n0.016196\n0.014772\n0.737255\n0.808304\n1.240073\n0\nINVESCO QQQ TR UNIT SER 1\n\n\n\n\n\n\n\nNow we can join in the groupings to df_components.\n\ndf_components = \\\n    (\n    df_components\n        .merge(df_cluster[['underlying','group']], how='inner', on='underlying')\n    )\ndf_components.head()\n\n\n\n\n\n\n\n\nunderlying\nPCA1\nPCA2\nPCA3\nPCA4\nPCA5\ngroup\n\n\n\n\n0\nDIA\n0.239596\n-0.169226\n0.090560\n0.015176\n0.005542\n1\n\n\n1\nEEM\n0.222856\n0.125537\n-0.084749\n0.123141\n-0.089064\n0\n\n\n2\nEFA\n0.204699\n0.084143\n-0.030489\n0.260802\n-0.137933\n2\n\n\n3\nEWJ\n0.141398\n0.016348\n-0.005798\n0.202220\n0.008194\n2\n\n\n4\nEWW\n0.146514\n0.136649\n-0.138207\n0.125954\n-0.217323\n2\n\n\n\n\n\n\n\nAnd finally we can use seaborn to visualize the first component.\n\nwith sns.axes_style('darkgrid'):\n    g = sns.catplot(\n        x='underlying',\n        y='PCA1',\n        kind='bar',\n        color='black',\n        alpha=0.75,\n        height=5,\n        aspect = 3,\n        data=df_components.sort_values(['group', 'underlying']),\n        hue='group',\n        palette=[\"b\", \"r\", 'k', 'y'],\n        dodge=False,\n    );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('First Principal Loading Vector');\n\n\n\n\n\n\n\n\n\nDiscussion Question: Give an interpretation of the first principal component.\n\n\nSolution\n# all underlyings tend to make/lose money together.\n\n\n\nCode Challenge: Graph the other principal components. Do you see any interesting patterns?\n\n\nSolution\n# just change the graph above to point to the different components by altering the y arguement.\nwith sns.axes_style('darkgrid'):\n    g = sns.catplot(\n        x='underlying',\n        y='PCA2',\n        kind='bar',\n        color='black',\n        alpha=0.75,\n        height=5,\n        aspect = 3,\n        data=df_components.sort_values(['group', 'underlying']),\n        hue='group',\n        palette=[\"b\", \"r\", 'k', 'y'],\n        dodge=False,\n    );\n    plt.subplots_adjust(top=0.93);\n    g.fig.suptitle('Second Principal Loading Vector');",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Visualizing Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/28_pca_visualization/pca_visualization.html#visualizing-scatter-plots-of-the-scores",
    "href": "chapters/28_pca_visualization/pca_visualization.html#visualizing-scatter-plots-of-the-scores",
    "title": "31  Visualizing Principal Components Analysis",
    "section": "31.5 Visualizing Scatter Plots of the Scores",
    "text": "31.5 Visualizing Scatter Plots of the Scores\nLet’s now graph the scores of the principal components. We can access these with the .transform attribute.\n\ntransform = pca.transform(Xs)\ntransform\n\narray([[ 3.89770162,  1.05257476, -0.89223461, -0.35589021, -0.35466412],\n       [-6.57546892, -0.40130749, -0.62645217,  1.17417457, -0.0447888 ],\n       [-1.24527477, -0.22133038, -0.47610851,  1.01747872, -0.95672622],\n       ...,\n       [11.11022384, -8.76996808, -2.62431904, -8.75691498,  2.29959156],\n       [-4.13229712,  0.8258197 ,  0.22816105,  0.82739011, -0.21214192],\n       [-6.48726319,  1.57657387,  0.08028833,  1.82019844, -0.81119418]])\n\n\nFinally, we construct a DataFrame with the scores and create a seaborn pair-plot.\n\ndf = \\\n    pd.DataFrame({\n        'PC1':transform[:,0],\n        'PC2':transform[:,1],\n        'PC3':transform[:,2],\n        'PC4':transform[:,3],\n    })\n\nsns.pairplot(df);\n\n\n\n\n\n\n\n\n\nDiscussion Question: Are you able to extract any insights from these plots?\n\n\nSolution\n# I can see that the variance of the first principal scores is much higher than the others.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Visualizing Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html",
    "title": "32  Student Loan: Overfitting",
    "section": "",
    "text": "32.1 Loading Packages\nAsset-backed Securities (ABS) are fixed income instruments that securitize the cashflows from various kinds of loans such as auto-loans, credit card balances, and students loans. Many of the loans backing ABS have fixed payment schedules that fully amortize the loan amount. These loans usually also give the borrower the option to repay larger amounts (up to the full loan balance) at any time without penalty, which is referred to as prepayment. For a given ABS, the rate at which the underlying loans prepay affects the timing of principle repayments as well as the amount of interest that the ABS owner earns - both of these affect overall investment performance. Thus, predicting prepayment speeds is of interest to ABS investors.\nIn this chapter we make our first attempt at predicting student loan prepayments. Our focus will be on familiarizing ourselves with the data and getting some initial models fit with sklearn. Along the way we will see model overfitting in action.\nLet’s begin by loading the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport sklearn\npd.options.display.max_rows = 10",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Student Loan: Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#reading-in-data",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#reading-in-data",
    "title": "32  Student Loan: Overfitting",
    "section": "32.2 Reading-In Data",
    "text": "32.2 Reading-In Data\nNext, let’s read-in our data set.\n\ndf_train = pd.read_csv('../data/student_loan.csv')\ndf_train\n\n\n\n\n\n\n\n\nload_id\ndeal_name\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\npaid_label\n\n\n\n\n0\n765579\n2014_b\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n0\n\n\n1\n765580\n2014_b\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n0\n\n\n2\n765581\n2014_b\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n0\n\n\n3\n765582\n2014_b\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n0\n\n\n4\n765583\n2014_b\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n1808885\n2019_c\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n0\n\n\n1043307\n1808886\n2019_c\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n0\n\n\n1043308\n1808887\n2019_c\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n0\n\n\n1043309\n1808888\n2019_c\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n0\n\n\n1043310\n1808889\n2019_c\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n0\n\n\n\n\n1043311 rows × 13 columns\n\n\n\nWe can inspect the columns of our data set with the DataFrame.info() method.\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1043311 entries, 0 to 1043310\nData columns (total 13 columns):\n #   Column           Non-Null Count    Dtype  \n---  ------           --------------    -----  \n 0   load_id          1043311 non-null  int64  \n 1   deal_name        1043311 non-null  object \n 2   loan_age         1043311 non-null  int64  \n 3   cosign           1043311 non-null  int64  \n 4   income_annual    1043311 non-null  float64\n 5   upb              1043311 non-null  float64\n 6   monthly_payment  1043311 non-null  float64\n 7   fico             1043311 non-null  int64  \n 8   origbalance      1043311 non-null  float64\n 9   mos_to_repay     1043311 non-null  int64  \n 10  repay_status     1043311 non-null  int64  \n 11  mos_to_balln     1043311 non-null  int64  \n 12  paid_label       1043311 non-null  int64  \ndtypes: float64(4), int64(8), object(1)\nmemory usage: 103.5+ MB",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Student Loan: Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#organizing-features-and-labels",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#organizing-features-and-labels",
    "title": "32  Student Loan: Overfitting",
    "section": "32.3 Organizing Features and Labels",
    "text": "32.3 Organizing Features and Labels\nNow that we have our data in memory, we can separate the features and labels in preparation for model fitting. We begin with the features.\n\nlst_features = \\\n    ['loan_age','cosign','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','repay_status','mos_to_balln',]    \ndf_X = df_train[lst_features]\ndf_X\n\n\n\n\n\n\n\n\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\n\n\n\n\n0\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n\n\n1\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n\n\n2\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n\n\n3\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n\n\n4\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n\n\n1043307\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n\n\n1043308\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n\n\n1043309\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n\n\n1043310\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n\n\n\n\n1043311 rows × 10 columns\n\n\n\nAnd next we do the same for the labels. Note that in our encoding a 1 stands for prepayment, while a 0 stands for non-prepayment.\n\ndf_y = df_train['paid_label']\ndf_y\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1043306    0\n1043307    0\n1043308    0\n1043309    0\n1043310    0\nName: paid_label, Length: 1043311, dtype: int64",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Student Loan: Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#logistic-regression",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#logistic-regression",
    "title": "32  Student Loan: Overfitting",
    "section": "32.4 Logistic Regression",
    "text": "32.4 Logistic Regression\nThe first classification model that we fit is called logistic regression. The name is a poor choice of words because despite being called a regression, it is actually used for classification. Although logistic regression can be used to predict a label with more than two outcomes, it is most effective when used to predict binary outcomes.\nAs with any modeling task, we begin by importing the constructor function for our model.\n\nfrom sklearn.linear_model import LogisticRegression\n\nNext, we instantiate our model.\n\nmdl_logit = LogisticRegression(random_state = 0)\n\nNow we can go ahead and fit our model, which will take a few seconds.\n\nmdl_logit.fit(df_X, df_y)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nSo how well does our model predict the training data? The standard metric for determining goodness of fit in a classification setting is accuracy, which is simply the the ratio of correct predictions to total predictions. This is the default metric that is used by the .score() method of classification models.\n\nmdl_logit.score(df_X, df_y)\n\n0.9835887860858363\n\n\n\nDiscussion Question: This accuracy looks great. So is our work done? Why might this accuracy be misleading?\n\n\nSolution\n##&gt; The data set is highly imbalanced, meaning there are very few \n##&gt; prepayments. Even a degenerate model that always predicts \n##&gt; non-prepayment would have a high accuracy.\n\n\n\nCode Challenge: Calculate the probability of prepayment in our data set.\n\n\nSolution\ndf_y.mean()\n\n\n0.01621472408514815\n\n\n\nAs we can see from the code challenge, our student loan data is highly imbalanced, meaning there are far more loans that don’t prepay than those that do prepay. Predicting rare outcomes via classification can be challenging. We will address the imbalance issue in future chapters.\nIt is often useful consider other model performance metrics when performing classification. In order to invoke these methods, we will need to be able to grab the predictions from our model as follows.\n\nmdl_logit.predict(df_X)\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\n\nCode Challenge: Calculate the probability of prepayment as predicted by our logistic regression model.\n\n\nSolution\nmdl_logit.predict(df_X).mean()\n\n\n0.0005837185652216837\n\n\n\nAn alternative goodness of fit metric is called precision, which is the percentage of prepayment predictions that were correct. The code below demonstrates that 33% of the prediction prepayments were correct\n\nsklearn.metrics.precision_score(df_y, mdl_logit.predict(df_X))\n\n0.33169129720853857\n\n\nAnother metric that we will consider is recall, which is the percentage of actual prepayments that were predicted correctly. The code below demonstrates that 1% of the prepayments were identified correctly.\n\nsklearn.metrics.recall_score(df_y, mdl_logit.predict(df_X))\n\n0.01194065141573565\n\n\nWhen performing classification, we strive for a model that has both high precision and high recall. Thus, it makes sense to combine these two metrics into a single metric. The standard combined metric that is used is called F1 and is defined as follows:\nF1 = 2 * (precision * recall) / (precision + recall)\nThe following code calculates F1.\n\nprecision = sklearn.metrics.precision_score(df_y, mdl_logit.predict(df_X))\nrecall = sklearn.metrics.recall_score(df_y, mdl_logit.predict(df_X))\n\n2 * (precision * recall) / (precision + recall)\n\n0.023051466392787857\n\n\nThe sklearn has F1 built into the the metrics module.\n\nsklearn.metrics.f1_score(df_y, mdl_logit.predict(df_X))\n\n0.023051466392787857",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Student Loan: Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#decision-tree",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#decision-tree",
    "title": "32  Student Loan: Overfitting",
    "section": "32.5 Decision Tree",
    "text": "32.5 Decision Tree\nThe next model we are going to fit to our student loan data is a decision tree classifier. As with any model, our steps are as follows:\n\nimport the constructor\ninstantiate the model\nfit model to data\n\nLet’s do all three steps in the following code cell.\n\nfrom sklearn.tree import DecisionTreeClassifier\nmdl_tree = DecisionTreeClassifier(random_state = 0)\nmdl_tree.fit(df_X, df_y)\n\nDecisionTreeClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\n\nCode Challenge: Calculate the accuracy and F1 for our fitted decision tree model. Can we pat ourselves on the back and call it quits?\n\n\nSolution\nprint(mdl_tree.score(df_X, df_y))\nprint(sklearn.metrics.f1_score(df_y,mdl_tree.predict(df_X)))\n\n\n1.0\n1.0\n\n\n\nDecision trees often overfit the data, which is what we are observing in code challenge above. Thus, while mdl_tree looks great with the training data, it won’t look nearly so good in the wild. One way to get a sense for this is to use a holdout set, which can conveniently do with the train_test_split() function in sklearn.\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df_X, df_y, random_state = 0)\n\nLet’s instantiate a new decision tree model and fit it to only X_train and y_train.\n\nmdl_holdout = DecisionTreeClassifier(random_state = 0)\nmdl_holdout.fit(X_train, y_train)\n\nDecisionTreeClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\nAnd let’s see how our hold out model performs on the test data X_test and y_test.\n\nsklearn.metrics.f1_score(y_test, mdl_holdout.predict(X_test))\n\n0.36954662104362707\n\n\nOne of the byproducts of fitting a decision tree is that it assigns an importance to the features. This can be accessed with the feature_importances_ attribute.\n\nmdl_tree.feature_importances_\n\narray([9.34238470e-02, 6.94084895e-04, 8.93879170e-02, 2.51588832e-01,\n       1.03062456e-01, 6.62052798e-02, 9.42629043e-02, 5.99678238e-04,\n       2.34529524e-04, 3.00540471e-01])\n\n\nLet’s make this output more readable by putting it in a DataFrame.\n\ndct_cols = {'feature':df_X.columns.values, 'importance':mdl_tree.feature_importances_}\npd.DataFrame(dct_cols).sort_values('importance', ascending = False)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n9\nmos_to_balln\n0.300540\n\n\n3\nupb\n0.251589\n\n\n4\nmonthly_payment\n0.103062\n\n\n6\norigbalance\n0.094263\n\n\n0\nloan_age\n0.093424\n\n\n2\nincome_annual\n0.089388\n\n\n5\nfico\n0.066205\n\n\n1\ncosign\n0.000694\n\n\n7\nmos_to_repay\n0.000600\n\n\n8\nrepay_status\n0.000235",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Student Loan: Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#random-forest",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#random-forest",
    "title": "32  Student Loan: Overfitting",
    "section": "32.6 Random Forest",
    "text": "32.6 Random Forest\nThe final classifier we will consider is a random forest. A random forest is gotten by fitting several decision trees to random subsets of the features, and then averaging the results. Random forest are ensemble methods, meaning they aggregate the results of a number of submodels.\nAs usual, we begin by instantiating and fitting the model. The n_estimators input controls the number of sub-decision-trees that will be aggregated.\n\nfrom sklearn.ensemble import RandomForestClassifier\nmdl_forest = RandomForestClassifier(n_estimators = 10, random_state = 0)\nmdl_forest.fit(df_X, df_y)\n\nRandomForestClassifier(n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=10, random_state=0)\n\n\nLet’s take a look at the in-sample F1 score.\n\nsklearn.metrics.f1_score(df_y, mdl_forest.predict(df_X))\n\n0.9082128714465085\n\n\nNext, let’s fit our model to the holdout training set that we defined above.\n\nmdl_holdout_forest = RandomForestClassifier(n_estimators = 10, random_state = 0)\nmdl_holdout_forest.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=10, random_state=0)\n\n\nFinally, let’s check the F1 score on our holdout test set.\n\nsklearn.metrics.f1_score(y_test, mdl_holdout_forest.predict(X_test))\n\n0.493071000855432",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Student Loan: Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#further-reading",
    "href": "chapters/29_student_loan_overfitting/student_loan_overfitting.html#further-reading",
    "title": "32  Student Loan: Overfitting",
    "section": "32.7 Further Reading",
    "text": "32.7 Further Reading\nSklearn User Guides\nhttps://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees\nhttps://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\nhttps://scikit-learn.org/stable/modules/tree.html\nSklearn API Documentation\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Student Loan: Overfitting</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "",
    "text": "33.1 Importing Packages\nIn this chapter we continue our exploration of the problem of predicting prepayments for student loans. In particular, we consider an alternative goodness-of-fit metric and also demonstrate how to perform cross-validation for several metrics simultaneously.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport sklearn\npd.options.display.max_rows = 10",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#reading-in-data",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#reading-in-data",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.2 Reading-In Data",
    "text": "33.2 Reading-In Data\nNext, let’s read-in our data set.\n\ndf_train = pd.read_csv('../data/student_loan.csv')\ndf_train\n\n\n\n\n\n\n\n\nload_id\ndeal_name\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\npaid_label\n\n\n\n\n0\n765579\n2014_b\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n0\n\n\n1\n765580\n2014_b\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n0\n\n\n2\n765581\n2014_b\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n0\n\n\n3\n765582\n2014_b\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n0\n\n\n4\n765583\n2014_b\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n1808885\n2019_c\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n0\n\n\n1043307\n1808886\n2019_c\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n0\n\n\n1043308\n1808887\n2019_c\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n0\n\n\n1043309\n1808888\n2019_c\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n0\n\n\n1043310\n1808889\n2019_c\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n0\n\n\n\n\n1043311 rows × 13 columns\n\n\n\nWe can inspect the columns of our data set with the DataFrame.info() method.\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1043311 entries, 0 to 1043310\nData columns (total 13 columns):\n #   Column           Non-Null Count    Dtype  \n---  ------           --------------    -----  \n 0   load_id          1043311 non-null  int64  \n 1   deal_name        1043311 non-null  object \n 2   loan_age         1043311 non-null  int64  \n 3   cosign           1043311 non-null  int64  \n 4   income_annual    1043311 non-null  float64\n 5   upb              1043311 non-null  float64\n 6   monthly_payment  1043311 non-null  float64\n 7   fico             1043311 non-null  int64  \n 8   origbalance      1043311 non-null  float64\n 9   mos_to_repay     1043311 non-null  int64  \n 10  repay_status     1043311 non-null  int64  \n 11  mos_to_balln     1043311 non-null  int64  \n 12  paid_label       1043311 non-null  int64  \ndtypes: float64(4), int64(8), object(1)\nmemory usage: 103.5+ MB",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#organizing-our-features-and-labels",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#organizing-our-features-and-labels",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.3 Organizing Our Features and Labels",
    "text": "33.3 Organizing Our Features and Labels\nNow that we have our data in memory, we can separate the features and labels in preparation for model fitting. We begin with the features.\n\nlst_features = \\\n    ['loan_age','cosign','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','repay_status','mos_to_balln',]    \ndf_X = df_train[lst_features]\ndf_X.head()\n\n\n\n\n\n\n\n\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\n\n\n\n\n0\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n\n\n1\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n\n\n2\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n\n\n3\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n\n\n4\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n\n\n\n\n\n\n\nNext we do the same for the labels. Note that in our encoding a 1 stands for prepayment, while a 0 stands for non-prepayment.\n\ndf_y = df_train['paid_label']\ndf_y\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1043306    0\n1043307    0\n1043308    0\n1043309    0\n1043310    0\nName: paid_label, Length: 1043311, dtype: int64",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#creating-a-holdout-set-with-train_test_split",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#creating-a-holdout-set-with-train_test_split",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.4 Creating a Holdout Set with train_test_split()",
    "text": "33.4 Creating a Holdout Set with train_test_split()\nIn subsequent sections we will require a holdout set to measure the out-of-sample performance of our models, so let’s create that now.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_X, df_y, random_state = 0, test_size = 0.1)\n\n\nCode Challenge: Explore X_train and X_test and verify that the test_size parameter controls the size of the test set.\n\n\nSolution\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(938979, 10)\n(104332, 10)",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#logistic-regression---accuracy-precision-reall-f1",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#logistic-regression---accuracy-precision-reall-f1",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.5 Logistic Regression - Accuracy, Precision, Reall, F1",
    "text": "33.5 Logistic Regression - Accuracy, Precision, Reall, F1\nIn this section we’ll review the traditional goodness-of-fit metrics: accuracy, precision, recall, and F1. We’ll do this in the context of logistic regression.\nLet’s begin by fitting a logistic regression to the entirety of our training data.\n\nfrom sklearn.linear_model import LogisticRegression\nmdl_logit = LogisticRegression(random_state = 0, solver = 'lbfgs')\nmdl_logit.fit(df_X, df_y)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nWe can use the .predict() method of our model to generate the predictions of our model.\n\narr_pred_logit = mdl_logit.predict(df_X)\narr_pred_logit\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\nLet’s take a look at various in-sample accuracy measures of our model.\n\nprint(\"Accuracy:  \", np.round(mdl_logit.score(df_X, df_y), 3))\nprint(\"Precision: \", np.round(sklearn.metrics.precision_score(df_y, arr_pred_logit), 3))\nprint(\"Recall:    \", np.round(sklearn.metrics.recall_score(df_y, arr_pred_logit), 3))\n\nAccuracy:   0.984\nPrecision:  0.332\nRecall:     0.012\n\n\n\nCode Challenge: Use the built-in function in sklearn.metrics to calculate the F1 score.\n\n\nSolution\nprint(np.round(sklearn.metrics.f1_score(df_y, arr_pred_logit), 3))      \n\n\n0.023\n\n\n\nAs we know, in-sample goodness-of-fit metrics are usually too optimistic about model performance. Using a holdout set is a simple way to get a sense for how the model will perform in the wild.\nThe following code fits a logistic regression model to the training set that we created above.\n\nmdl_logit_holdout = LogisticRegression(random_state = 0)\nmdl_logit_holdout.fit(X_train, y_train)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nHere is code that calculates the out-of-sample goodness of fit metrics on the test-set.\n\narr_pred_logit_holdout = mdl_logit_holdout.predict(X_test)\n\nprint(\"Accuracy:  \", np.round(mdl_logit_holdout.score(X_test, y_test), 3))\nprint(\"Precision: \", np.round(sklearn.metrics.precision_score(y_test, arr_pred_logit_holdout), 3))\nprint(\"Recall:    \", np.round(sklearn.metrics.recall_score(y_test, arr_pred_logit_holdout), 3))\nprint(\"F1:        \", np.round(sklearn.metrics.f1_score(y_test, arr_pred_logit_holdout), 3))\n\nAccuracy:   0.984\nPrecision:  0.304\nRecall:     0.01\nF1:         0.02",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#balances-of-loans-that-prepaid",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#balances-of-loans-that-prepaid",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.6 Balances of Loans that Prepaid",
    "text": "33.6 Balances of Loans that Prepaid\nThus far our all of our goodness-of-fit measures have focused on tallying the accuracy of individual predictions. However, ABS investors are not interested in which particular loans prepayed, but rather the total UPB that prepayed.\nThe following code calculates the total UPB of the loans that actually prepayed in the training data.\n\nupb_prepay_actual = \\\n    (\n    df_train[['upb', 'paid_label']]\n        .assign(prepay_upb = lambda df: df.upb * df.paid_label)\n        ['prepay_upb'].sum()\n    )\nupb_prepay_actual\n\n683871848.0400001",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#balances-of-predicted-prepays",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#balances-of-predicted-prepays",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.7 Balances of Predicted Prepays",
    "text": "33.7 Balances of Predicted Prepays\nLet’s now calculate the balance of the loans that our logistic regression model predicts will prepay.\n\nupb_prepay_logit = \\\n    (\n    df_train\n        .assign(pred_logit = mdl_logit.predict(df_X))\n        .assign(prepay_upb_logit = lambda df: df.pred_logit * df.upb)\n        ['prepay_upb_logit'].sum()\n    )\n\nupb_prepay_logit\n\n28332218.820000004\n\n\nAs you can see, the logistic regression UPB prepay predictions are only 4% of what actually occurred.\n\nupb_prepay_logit / upb_prepay_actual\n\n0.04142913456841527",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#expected-value-of-balance-of-loan-prepayment-in-sample",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#expected-value-of-balance-of-loan-prepayment-in-sample",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.8 Expected Value of Balance of Loan Prepayment (In-Sample)",
    "text": "33.8 Expected Value of Balance of Loan Prepayment (In-Sample)\nUnder the hood, most classification algorithms calculate a probability for each class. The specific prediction is then simply the class with the highest probability.\nIn sklearn we can view these probabilities with the .predict_proba() method. Let’s do this with mdl_logit.\n\nmdl_logit.predict_proba(df_X)\n\narray([[0.99119454, 0.00880546],\n       [0.98607272, 0.01392728],\n       [0.98927091, 0.01072909],\n       ...,\n       [0.98597615, 0.01402385],\n       [0.99228992, 0.00771008],\n       [0.98784426, 0.01215574]])\n\n\nIn our example, the probability of prepayment is in the second column, which we can isolate as follows.\n\nmdl_logit.predict_proba(df_X)[:, 1]\n\narray([0.00880546, 0.01392728, 0.01072909, ..., 0.01402385, 0.00771008,\n       0.01215574])\n\n\nUsing these probabilities, let’s calculate an expected value for the total UPB that will be prepaid.\n\nev_logit = \\\n    (\n    df_train\n        .assign(pred_logit = mdl_logit.predict_proba(df_X)[:,1])\n        .assign(prepay_upb_logit = lambda df: df.pred_logit * df.upb)\n        ['prepay_upb_logit'].sum()\n    )\n\nev_logit\n\n683873044.6387616\n\n\nAs you can see, the in-sample expected value calculation is almost exactly in-line with the actual prepayments.\n\nev_logit / upb_prepay_actual\n\n1.0000017497412197",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#expected-value-of-balance-of-loan-prepayments-out-of-sample",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#expected-value-of-balance-of-loan-prepayments-out-of-sample",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.9 Expected Value of Balance of Loan Prepayments (Out-of-Sample)",
    "text": "33.9 Expected Value of Balance of Loan Prepayments (Out-of-Sample)\nAs we can see above, from an expected UPB standpoint, our model seems to be working quite well. However, the above calculation was done in-sample. Let’s try an out-of-sample accuracy measure calculation with our holdout set.\nWe begin by fitting a model to the training data.\n\nmdl_logit_holdout = LogisticRegression(random_state = 0)\nmdl_logit_holdout.fit(X_train, y_train)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nNext, let’s calculated the actual prepayments in the test-set.\n\nprepay_test = \\\n    (\n    X_test\n        .merge(y_test, left_index=True, right_index = True)\n        .assign(upb_prepay = lambda df: df.upb * df.paid_label)\n        ['upb_prepay'].sum()    \n    )\n\nprepay_test\n\n68602482.71000001\n\n\nThe following code returns the out-of-sample prediction probabilities for the test set.\n\nmdl_logit_holdout.predict_proba(X_test)\n\narray([[0.98717972, 0.01282028],\n       [0.99661209, 0.00338791],\n       [0.99589556, 0.00410444],\n       ...,\n       [0.98467554, 0.01532446],\n       [0.99119456, 0.00880544],\n       [0.97107693, 0.02892307]])\n\n\n\nCode Challenge: Calculate the out-of-sample expected value of prepaid UPB for the hold-out test set; also, find it’s proportion relative to the actual prepayments.\n\n\nSolution\nprepay_holdout = \\\n    (\n    X_test\n        .assign(pred_holdout = mdl_logit_holdout.predict_proba(X_test)[:, 1])\n        .assign(upb_prepay_holdout = lambda df: df.upb * df.pred_holdout)\n        ['upb_prepay_holdout'].sum()\n    )\n\nprint(prepay_holdout)\nprint(prepay_holdout / prepay_test)\n\n\n67477986.49727169\n0.9836085201539738",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#cross-validation-for-precision-recall-and-f1-score",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#cross-validation-for-precision-recall-and-f1-score",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.10 Cross-Validation for Precision, Recall, and F1 Score",
    "text": "33.10 Cross-Validation for Precision, Recall, and F1 Score\nThe holdout set methodology can be generalized to \\(n\\)-fold cross validation. The set of goodness-of-fit measures that result from cross-validation are, in aggregate, more robust than a metric calculated on a single holdout test set.\nIn this final section, we’ll see what the code looks like to generate these cross-validation metrics for a decision tree classifier.\nLet’s begin by instantiating a decision tree model.\n\nfrom sklearn.tree import DecisionTreeClassifier\nmdl_tree = DecisionTreeClassifier(random_state = 0)\n\nThe following code generates F1, precision, and recall via cross-validation.\n\ndct_cv = sklearn.model_selection.cross_validate(mdl_tree, df_X, df_y, scoring = ['f1', 'precision', 'recall'], cv = 5)\ndct_cv\n\n{'fit_time': array([12.7782793 , 13.40303349, 11.756989  , 11.9663868 , 11.73292732]),\n 'score_time': array([0.17642283, 0.17362189, 0.17155647, 0.17140222, 0.17079616]),\n 'test_f1': array([0.22146021, 0.35535158, 0.37082628, 0.38977097, 0.41728045]),\n 'test_precision': array([0.21438451, 0.39882266, 0.38671374, 0.37145153, 0.40070729]),\n 'test_recall': array([0.22901891, 0.32042566, 0.35619273, 0.40999113, 0.43528369])}\n\n\n\nCode Challenge: Calculate the average F1 score in our cross-validation scheme.\n\n\nSolution\ndct_cv['test_f1'].mean()\n\n\n0.35093789908958994",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#cross-validation-on-the-alternative-metric",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#cross-validation-on-the-alternative-metric",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.11 Cross-Validation on the Alternative Metric",
    "text": "33.11 Cross-Validation on the Alternative Metric\nThe goodness of fit metric that will be most useful to us will be the expected value of prepaid balance. Unfortunately, this does not fit neatly into the .cross_validate() method in the previous section. Thus, in order to use our expected value of prepaid balance metric in a cross-validation context, we will have to write some of the boiler-plate code. Luckily, the sklearn.model_selection module does much of the heavy lifting for us. This is what we will do in the next chapter.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#further-reading",
    "href": "chapters/30_student_loan_alternative_metric/student_loan_alternative_metric.html#further-reading",
    "title": "33  Student Loan: Alternative Metric & Cross-Validation",
    "section": "33.12 Further Reading",
    "text": "33.12 Further Reading\nSklearn User Guides\nhttps://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\nhttps://scikit-learn.org/stable/modules/tree.html\nhttps://scikit-learn.org/stable/modules/cross_validation.html\nSklearn API Documentation\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Student Loan: Alternative Metric & Cross-Validation</span>"
    ]
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html",
    "title": "34  Student Loan: Hyperparameter Tuning",
    "section": "",
    "text": "34.1 Loading Packages\nThe hyperparameters of a machine learning algorithm control its flexibility. Greater flexibility can be desirable because it allows the model to react to nuances of the data. However, too much model flexibility can also lead to overfitting. Therefore, it is sometimes preferable to restrict the flexibility of a model to reduce overfitting.\nThe trade-off between allowing for flexibility while at the same avoiding overfitting is referred to as the variance-bias tradeoff.\nLet’s begin by loading the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport time\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Student Loan: Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#reading-in-data",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#reading-in-data",
    "title": "34  Student Loan: Hyperparameter Tuning",
    "section": "34.2 Reading-In Data",
    "text": "34.2 Reading-In Data\nNext, let’s read-in our data set.\n\ndf_train = pd.read_csv('../data/student_loan.csv')\ndf_train\n\n\n\n\n\n\n\n\nload_id\ndeal_name\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\npaid_label\n\n\n\n\n0\n765579\n2014_b\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n0\n\n\n1\n765580\n2014_b\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n0\n\n\n2\n765581\n2014_b\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n0\n\n\n3\n765582\n2014_b\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n0\n\n\n4\n765583\n2014_b\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n1808885\n2019_c\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n0\n\n\n1043307\n1808886\n2019_c\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n0\n\n\n1043308\n1808887\n2019_c\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n0\n\n\n1043309\n1808888\n2019_c\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n0\n\n\n1043310\n1808889\n2019_c\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n0\n\n\n\n\n1043311 rows × 13 columns\n\n\n\nWe can inspect the columns of our data set with the DataFrame.info() method.\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1043311 entries, 0 to 1043310\nData columns (total 13 columns):\n #   Column           Non-Null Count    Dtype  \n---  ------           --------------    -----  \n 0   load_id          1043311 non-null  int64  \n 1   deal_name        1043311 non-null  object \n 2   loan_age         1043311 non-null  int64  \n 3   cosign           1043311 non-null  int64  \n 4   income_annual    1043311 non-null  float64\n 5   upb              1043311 non-null  float64\n 6   monthly_payment  1043311 non-null  float64\n 7   fico             1043311 non-null  int64  \n 8   origbalance      1043311 non-null  float64\n 9   mos_to_repay     1043311 non-null  int64  \n 10  repay_status     1043311 non-null  int64  \n 11  mos_to_balln     1043311 non-null  int64  \n 12  paid_label       1043311 non-null  int64  \ndtypes: float64(4), int64(8), object(1)\nmemory usage: 103.5+ MB",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Student Loan: Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#organizing-our-features-and-labels",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#organizing-our-features-and-labels",
    "title": "34  Student Loan: Hyperparameter Tuning",
    "section": "34.3 Organizing Our Features and Labels",
    "text": "34.3 Organizing Our Features and Labels\nNow that we have our data in memory, we can separate the features and labels in preparation for model fitting. We begin with the features.\n\nlst_features = \\\n    ['loan_age','cosign','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','repay_status','mos_to_balln',]    \ndf_X = df_train[lst_features]\ndf_y = df_train['paid_label']\n\nAnd next we do the same for the labels. Note that in our encoding a 1 stands for prepayment, while a 0 stands for non-prepayment.\n\ndf_y = df_train['paid_label']\ndf_y\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1043306    0\n1043307    0\n1043308    0\n1043309    0\n1043310    0\nName: paid_label, Length: 1043311, dtype: int64",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Student Loan: Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#sklearn.model_selection.kfold",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#sklearn.model_selection.kfold",
    "title": "34  Student Loan: Hyperparameter Tuning",
    "section": "34.4 sklearn.model_selection.KFold",
    "text": "34.4 sklearn.model_selection.KFold\nCross-validation techniques are a useful for estimating out-of-sample goodness of fit metrics for a model. Writing cross-validation code from scratch would involve a lot of boiler plate code, which basically amounts to lots of for-loops whose implementation yields very little insight.\nOne of great things about sklearn is that it contains a variety of convenience functions that take care a lot of this sort boiler-plate code for you.\nA great example of such a convenience function is KFold() which produces arrays of indexes that define a K-Fold cross validation.\nThe following code returns an object that when applied to a data set will yield the indexes for the training set and test set for each iteration of a 2-fold cross validation.\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits = 2, shuffle = True, random_state = 0)\n\nWe can use a for-loop to inspect these indexes:\n\nfor train_index, test_index in kf.split(df_X, df_y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nTRAIN: [      1       2       3 ... 1043303 1043307 1043309] TEST: [      0       5       6 ... 1043306 1043308 1043310]\nTRAIN: [      0       5       6 ... 1043306 1043308 1043310] TEST: [      1       2       3 ... 1043303 1043307 1043309]\n\n\n\nResearch Challenge: Google sklearn.model_selection.StratifiedKFold and describe the difference between it and KFold().\n\n\nSolution\n# The folds are made by preserving the percentage of samples for each class.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Student Loan: Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#choosing-optimal-max_depth-for-decisiontreeclassifier",
    "href": "chapters/31_student_loan_hyperparameter_tuning/student_loan_hyperparameter_tuning.html#choosing-optimal-max_depth-for-decisiontreeclassifier",
    "title": "34  Student Loan: Hyperparameter Tuning",
    "section": "34.5 Choosing Optimal max_depth for DecisionTreeClassifier",
    "text": "34.5 Choosing Optimal max_depth for DecisionTreeClassifier\nRecall that the decision tree algorithm is a process that iteratively partitions the feature space. Specifically, for each iteration of the algorithm, a split is made along one particular dimension of the feature space. The process is repeated until some kind of stopping criteria is met.\nThe hyperparameters of a decision tree model control various criteria for stopping this iterative splitting process. The more strict the stopping criteria (i.e. stopping faster) the less flexible the model. The more lenient the stopping criteria (i.e. stopping slower) the more flexible the model.\nRestricting the flexibility of a model by changing hyperparameters is also referred to as regularization.\nIn this section we demonstrate choosing an optimal max_depth value for a decision tree classifier on our student loan data. Towards this end we will use cross-validation.\nLet’s import the functions that we will need.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score\n\nWe will use 10-fold cross-validation, so let’s invoke the required indexes with KFold.\n\nkf = KFold(n_splits = 10, shuffle = True, random_state = 0)\n\nThe following code calculates a 10-fold CV F1-score, expected-balance ratio, and average fit-time for various levels of max_depth.\n\n# various levels of max_depth we will try\nlst_depth = [1, 2, 3, 4, 5, 10, 15, 20, 25]\n\n# lists for storing average statistics \nlst_f1_avg = []\nlst_eb_ratio_avg = []\nlst_fit_time_avg = []\nfor ix_depth in lst_depth:\n\n    # list for storing statitics for each fold of cross-validation\n    lst_f1 = []\n    lst_eb_ratio = []\n    lst_fit_time = []\n    for train_index, test_index in kf.split(df_X, df_y):    \n        # creating training set \n        X_train = df_X.iloc[train_index]\n        y_train = df_y.iloc[train_index]\n\n        # creating test set\n        X_test = df_X.iloc[test_index]\n        y_test = df_y.iloc[test_index]\n\n        # intantiating model\n        mdl = DecisionTreeClassifier(max_depth = ix_depth, random_state = 0)\n\n        # fit the model\n        start = time.time()\n        mdl.fit(X_train, y_train)\n        arr_pred = mdl.predict(X_test)\n        end = time.time()\n        \n        # fit time\n        dbl_fit_time = end - start\n        lst_fit_time.append(dbl_fit_time)\n\n        # f1-score\n        dbl_f1 = f1_score(y_test, arr_pred)\n        lst_f1.append(dbl_f1) \n\n        # expected-balance ratio\n        arr_pred_proba = mdl.predict_proba(X_test)[:,1]\n        dbl_eb_ratio = (arr_pred_proba * X_test['upb']).sum() / (y_test * X_test['upb']).sum()\n        lst_eb_ratio.append(dbl_eb_ratio)\n\n    # calculating and storing average metrics\n    fit_time_avg = np.round(np.mean(lst_fit_time), 1)\n    lst_fit_time_avg.append(fit_time_avg)\n    f1_avg = np.round(np.mean(lst_f1), 3)\n    lst_f1_avg.append(f1_avg)\n    eb_ratio_avg = np.round(np.mean(lst_eb_ratio), 3)\n    lst_eb_ratio_avg.append(eb_ratio_avg)\n    \n    # printing some output so I know my code is running\n    print(ix_depth)\n\n1\n2\n3\n4\n5\n10\n15\n20\n25\n\n\nLet’s put our results into a DataFrame, and then graph them.\n\ndf_results = pd.DataFrame({'max_depth':lst_depth, 'f1':lst_f1_avg, 'eb_ratio':lst_eb_ratio_avg, 'fit_time':lst_fit_time_avg})\ndf_results\n\n\n\n\n\n\n\n\nmax_depth\nf1\neb_ratio\nfit_time\n\n\n\n\n0\n1\n0.374\n1.261\n0.7\n\n\n1\n2\n0.464\n1.128\n1.1\n\n\n2\n3\n0.444\n1.048\n1.8\n\n\n3\n4\n0.459\n1.054\n2.3\n\n\n4\n5\n0.470\n0.984\n2.9\n\n\n5\n10\n0.486\n1.043\n5.8\n\n\n6\n15\n0.478\n1.016\n8.4\n\n\n7\n20\n0.463\n0.997\n10.5\n\n\n8\n25\n0.441\n0.978\n12.2\n\n\n\n\n\n\n\n\n34.5.1 Graph of F1\n\n%matplotlib inline\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(\n            x = 'max_depth',\n            y = 'f1',\n            data = df_results,\n            alpha = 0.75,\n            height = 5, \n            aspect = 1.5,\n        );\n    plt.subplots_adjust(top = 0.93);\n    g.fig.suptitle('F1 for Various Levels of Max-Depth');\n\n\n\n\n\n\n\n\n\n\n34.5.2 Graph of Expected-Balance Ratio\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(\n            x = 'max_depth',\n            y = 'eb_ratio',\n            data = df_results,\n            alpha = 0.75,\n            height = 5, \n            aspect = 1.5,\n        );\n    plt.subplots_adjust(top = 0.93);\n    g.fig.suptitle('Expected-Balance Ratio for Various Levels of Max-Depth');\n\n\n\n\n\n\n\n\n\n\n34.5.3 Graph of Fit Time\n\nwith sns.axes_style('darkgrid'):\n    g = sns.relplot(\n            x = 'max_depth',\n            y = 'fit_time',\n            data = df_results,\n            alpha = 0.75,\n            height = 5, \n            aspect = 1.5,\n        );\n    plt.subplots_adjust(top = 0.93);\n    g.fig.suptitle('Fit-Time for Various Levels of Max-Depth');\n\n\n\n\n\n\n\n\n\nDiscussion Question: Based on these results, which what would you choose for max_depth?\n\n\nSolution\n# I would probaby choose something in the 5-10 range.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Student Loan: Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html",
    "title": "35  Student Loan: xgboost",
    "section": "",
    "text": "35.1 Import Packages\nIn this chapter we will apply the XGBoost classifier to our student loan data.\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport xgboost\npd.options.display.max_rows = 10",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Student Loan: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#read-in-data",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#read-in-data",
    "title": "35  Student Loan: xgboost",
    "section": "35.2 Read-In Data",
    "text": "35.2 Read-In Data\n\ndf_train = pd.read_csv('../data/student_loan.csv')\ndf_train\n\n\n\n\n\n\n\n\nload_id\ndeal_name\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\npaid_label\n\n\n\n\n0\n765579\n2014_b\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n0\n\n\n1\n765580\n2014_b\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n0\n\n\n2\n765581\n2014_b\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n0\n\n\n3\n765582\n2014_b\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n0\n\n\n4\n765583\n2014_b\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n1808885\n2019_c\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n0\n\n\n1043307\n1808886\n2019_c\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n0\n\n\n1043308\n1808887\n2019_c\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n0\n\n\n1043309\n1808888\n2019_c\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n0\n\n\n1043310\n1808889\n2019_c\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n0\n\n\n\n\n1043311 rows × 13 columns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Student Loan: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#feature-selection",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#feature-selection",
    "title": "35  Student Loan: xgboost",
    "section": "35.3 Feature Selection",
    "text": "35.3 Feature Selection\n\nlst_features = \\\n    ['loan_age','cosign','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','repay_status','mos_to_balln',]    \ndf_X = df_train[lst_features]\ndf_X\n\n\n\n\n\n\n\n\nloan_age\ncosign\nincome_annual\nupb\nmonthly_payment\nfico\norigbalance\nmos_to_repay\nrepay_status\nmos_to_balln\n\n\n\n\n0\n56\n0\n113401.60\n36011.11\n397.91\n814\n51453.60\n0\n0\n124\n\n\n1\n56\n1\n100742.34\n101683.38\n1172.10\n711\n130271.33\n0\n0\n124\n\n\n2\n56\n0\n46000.24\n49249.37\n593.57\n772\n62918.96\n0\n0\n124\n\n\n3\n56\n0\n428958.96\n36554.85\n404.63\n849\n48238.73\n0\n0\n125\n\n\n4\n56\n0\n491649.96\n7022.30\n1967.46\n815\n106124.68\n0\n0\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1043306\n2\n0\n152885.00\n115363.12\n1212.22\n798\n116834.64\n0\n0\n118\n\n\n1043307\n2\n0\n116480.00\n77500.70\n831.13\n826\n79566.03\n0\n0\n118\n\n\n1043308\n2\n0\n96800.00\n16156.76\n232.34\n781\n16472.50\n0\n0\n82\n\n\n1043309\n2\n0\n78400.14\n77197.03\n833.57\n777\n78135.54\n0\n0\n118\n\n\n1043310\n65\n0\n50447.28\n65667.85\n767.10\n765\n82602.38\n0\n0\n119\n\n\n\n\n1043311 rows × 10 columns\n\n\n\n\ndf_y = df_train['paid_label']\ndf_y\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1043306    0\n1043307    0\n1043308    0\n1043309    0\n1043310    0\nName: paid_label, Length: 1043311, dtype: int64",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Student Loan: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#creating-holdout-sets",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#creating-holdout-sets",
    "title": "35  Student Loan: xgboost",
    "section": "35.4 Creating Holdout Sets",
    "text": "35.4 Creating Holdout Sets\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df_X, df_y, random_state = 0)\n\n\nX_train = X_train.copy()\nX_test = X_test.copy()\ny_train = y_train.copy()\ny_test = y_test.copy()",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Student Loan: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#initial-fit",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#initial-fit",
    "title": "35  Student Loan: xgboost",
    "section": "35.5 Initial Fit",
    "text": "35.5 Initial Fit\n\nfrom xgboost import XGBClassifier\nmodel = XGBClassifier(eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict(X_test)\n\n\nprint('Actual:    ', y_test.sum())\nprint('Predicted: ', y_pred.sum())\n\nActual:     4227\nPredicted:  1647\n\n\n\nprint('Actual:   ', sum(X_test['upb'] * y_test))\nprint('Predicted: ', sum(X_test['upb'] * y_pred))\nprint('Ratio:     ', sum(X_test['upb'] * y_pred) / sum(X_test['upb'] * y_test))\n\nActual:    166234148.19000015\nPredicted:  32604191.500000022\nRatio:      0.19613413883370406\n\n\n\nsklearn.metrics.f1_score(y_test, y_pred)\n\n0.5110657133129044",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Student Loan: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#modifying-scale_pos_weight",
    "href": "chapters/32_student_loan_xgboost/student_loan_xgboost.html#modifying-scale_pos_weight",
    "title": "35  Student Loan: xgboost",
    "section": "35.6 Modifying scale_pos_weight",
    "text": "35.6 Modifying scale_pos_weight\n\n# fit model no training data\nmodel = XGBClassifier(eval_metric='logloss', scale_pos_weight=25)\nmodel.fit(X_train, y_train)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=None, ...)\n\n\n\ny_pred = model.predict(X_test)\n\n\nprint('Actual:    ', y_test.sum())\nprint('Predicted: ', y_pred.sum())\n\nActual:     4227\nPredicted:  6905\n\n\n\nprint('Actual:   ', sum(X_test['upb'] * y_test))\nprint('Predicted: ', sum(X_test['upb'] * y_pred))\nprint('Ratio:     ', sum(X_test['upb'] * y_pred) / sum(X_test['upb'] * y_test))\n\nActual:    166234148.19000015\nPredicted:  128185269.83000004\nRatio:      0.7711127420311289\n\n\n\nsklearn.metrics.f1_score(y_test, y_pred)\n\n0.3645346748113546",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Student Loan: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html",
    "title": "36  Option Volume: Introduction",
    "section": "",
    "text": "36.1 Importing Packages\nThe function of an option market-maker is to provide liquidity on option exchanges. At any given time, they are willing to buy an option for a price slightly lower than fair value, and sell that same option for a price slightly higher than the fair value. The more trades a market-maker executes, the more money they make. They also have limited computing power, and thus it makes sense for them to direct their limited resources to underlyings which have the most volume. For this reason, it is useful for a market-maker to predict option volume rankings for underlyings.\nThe purpose of this chapter is to try to predict option volume rankings. In particular, we will:\nAdditionally, we will define the top-\\(n\\) metric that we will use to evaluate various prediction models that we will explore in future chapters.\nLet’s begin by importing the packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport sklearn",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#reading-in-data",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#reading-in-data",
    "title": "36  Option Volume: Introduction",
    "section": "36.2 Reading-In Data",
    "text": "36.2 Reading-In Data\nNext let’s read-in the data that we will need.\n\ndf_stats = pd.read_csv('../data/option_stats.csv')\ndf_stock = pd.read_csv('../data/option_stock_quote.csv')\n\nThe data in df_stats consists of volume, open-interest, and implied volatility statistics for all available underlyings.\n\ndf_stats\n\n\n\n\n\n\n\n\nsymbol\nquotedate\nimplied_vol\ntotalvol\ntotaloi\n\n\n\n\n0\nA\n2016-01-04\n0.3289\n1330\n116961\n\n\n1\nAA\n2016-01-04\n0.4843\n38615\n1152498\n\n\n2\nAAC\n2016-01-04\n0.8606\n3\n1386\n\n\n3\nAAL\n2016-01-04\n0.4096\n56386\n875178\n\n\n4\nAAN\n2016-01-04\n0.4471\n23\n5480\n\n\n...\n...\n...\n...\n...\n...\n\n\n2202060\nZTO\n2018-01-31\n0.4140\n53\n15086\n\n\n2202061\nZTS\n2018-01-31\n0.2419\n1333\n29890\n\n\n2202062\nZUMZ\n2018-01-31\n0.5936\n21\n4121\n\n\n2202063\nZX\n2018-01-31\n0.0000\n0\n21\n\n\n2202064\nZYNE\n2018-01-31\n0.7505\n253\n10646\n\n\n\n\n2202065 rows × 5 columns\n\n\n\nThe data in df_stock consists of end-of-day prices for each of the underlyings.\n\ndf_stock\n\n\n\n\n\n\n\n\nsymbol\nquotedate\npx_close\nvolume\n\n\n\n\n0\nA\n2016-01-04\n40.69000\n3287300\n\n\n1\nAA\n2016-01-04\n9.70999\n12639700\n\n\n2\nAAC\n2016-01-04\n18.52000\n119400\n\n\n3\nAAL\n2016-01-04\n40.91000\n12037200\n\n\n4\nAAN\n2016-01-04\n22.68000\n698000\n\n\n...\n...\n...\n...\n...\n\n\n2201883\nZTO\n2018-01-31\n15.81000\n1121202\n\n\n2201884\nZTS\n2018-01-31\n76.77000\n3690956\n\n\n2201885\nZUMZ\n2018-01-31\n20.75000\n361661\n\n\n2201886\nZX\n2018-01-31\n1.31000\n17279\n\n\n2201887\nZYNE\n2018-01-31\n12.09000\n263042\n\n\n\n\n2201888 rows × 4 columns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#wrangling-selecting-the-universe",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#wrangling-selecting-the-universe",
    "title": "36  Option Volume: Introduction",
    "section": "36.3 Wrangling: Selecting the Universe",
    "text": "36.3 Wrangling: Selecting the Universe\nWe will use data from 2016 to choose the universe for our analysis. The universe we will ultimately arrive at is the 301-500 most liquid underlyings for which there is data for all of 2017 (training period) and January of 2018 (backtest period).\nLet’s begin by isolating the 2016 data from df_stats.\n\ndf_stats_2016 = df_stats.query('quotedate &gt;= \"2016-01-01\"').query('quotedate &lt;= \"2016-12-31\"')\n\nNext, we calculate the daily volume volume ranks for all the symbols in 2016.\n\ndf_volume_rank = \\\n    (\n    df_stats_2016\n        .groupby(['symbol'])[['totalvol']].mean()\n        .reset_index()\n        .rename(columns={'totalvol':'average_daily_volume'})\n        .assign(average_daily_volume = lambda df: np.round(df['average_daily_volume']))\n        .assign(volume_rank = lambda df: df['average_daily_volume'].rank(ascending=False))\n    )\ndf_volume_rank\n\n\n\n\n\n\n\n\nsymbol\naverage_daily_volume\nvolume_rank\n\n\n\n\n0\nA\n905.0\n895.5\n\n\n1\nAA\n39852.0\n55.0\n\n\n2\nAAC\n108.0\n2118.0\n\n\n3\nAAL\n37165.0\n60.0\n\n\n4\nAAN\n402.0\n1308.0\n\n\n...\n...\n...\n...\n\n\n4742\nZTO\n358.0\n1381.0\n\n\n4743\nZTS\n1945.0\n589.5\n\n\n4744\nZUMZ\n206.0\n1693.0\n\n\n4745\nZX\n0.0\n4703.5\n\n\n4746\nZYNE\n91.0\n2235.0\n\n\n\n\n4747 rows × 3 columns\n\n\n\nLet’s initially create df_raw_universe that has more symbols than we will need because not all symbols have data for all days. In particular, we will first grab the 301-700th most liquid underlyings. In the next steps we will select only symbols that have data for all days.\n\ndf_universe_raw = \\\n    (\n    df_volume_rank\n        .query('volume_rank &gt;= 301 & volume_rank &lt;= 700')\n        .sort_values(['volume_rank'])\n        [['symbol', 'average_daily_volume', 'volume_rank']]\n    )\ndf_universe_raw\n\n\n\n\n\n\n\n\nsymbol\naverage_daily_volume\nvolume_rank\n\n\n\n\n624\nBUD\n5623.0\n301.0\n\n\n2159\nIILG\n5619.0\n302.0\n\n\n4032\nTAP\n5611.0\n303.0\n\n\n325\nASHR\n5606.0\n304.0\n\n\n3301\nPOM\n5565.0\n305.0\n\n\n...\n...\n...\n...\n\n\n2812\nMTW\n1441.0\n696.0\n\n\n957\nCRC\n1438.0\n697.0\n\n\n3296\nPNRA\n1432.0\n698.5\n\n\n1653\nFNSR\n1432.0\n698.5\n\n\n4433\nVIXY\n1428.0\n700.0\n\n\n\n\n400 rows × 3 columns\n\n\n\nNext, let’s grab the data for both the training period (2017) and the backtest period (January 2018).\n\ndf_stats_analysis = df_stats.query('quotedate &gt;= \"2017-01-01\"').query('quotedate &lt;= \"2018-01-31\"')\n\nAnd finally we are ready to select our universe.\n\ndf_universe = \\\n    (\n    df_universe_raw                                                   # start with big universe\n        .merge(df_stats_analysis, how='left', on='symbol')            # join volume and volatility stats\n        .merge(df_stock, how='left', on=['symbol', 'quotedate'])      # join stock price data\n        .dropna()\n        .groupby(['symbol', 'volume_rank'])[['quotedate']].count()    # count the number of rows of data that exist\n        .reset_index()\n        .sort_values(['volume_rank'])\n        .query('quotedate == 272')                                    # grab the symbols that have all 272 days worth of data - this is hardcoded\n        .assign(rerank = lambda df: df['volume_rank'].rank())         # rerank this smaller universe\n    ).query('rerank &lt;= 300')                                          # grab the 300 most liquid underlyings in restricted universe\ndf_universe\n\n\n\n\n\n\n\n\nsymbol\nvolume_rank\nquotedate\nrerank\n\n\n\n\n44\nBUD\n301.0\n272\n1.0\n\n\n321\nTAP\n303.0\n272\n2.0\n\n\n25\nASHR\n304.0\n272\n3.0\n\n\n19\nAMJ\n306.0\n272\n4.0\n\n\n69\nCOF\n307.0\n272\n5.0\n\n\n...\n...\n...\n...\n...\n\n\n9\nAEP\n647.0\n272\n296.0\n\n\n259\nPII\n648.0\n272\n297.0\n\n\n27\nAU\n649.0\n272\n298.0\n\n\n320\nSYY\n650.0\n272\n299.0\n\n\n266\nPSTG\n651.0\n272\n300.0\n\n\n\n\n300 rows × 4 columns",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#wrangling-handling-zero-implied-vols",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#wrangling-handling-zero-implied-vols",
    "title": "36  Option Volume: Introduction",
    "section": "36.4 Wrangling: Handling Zero Implied Vols",
    "text": "36.4 Wrangling: Handling Zero Implied Vols\nThere are a number of rows in df_stats_analysis that have zero implied vols, which will affect our analysis down the road.\n\ndf_stats_analysis.query('implied_vol &lt;= 0')\n\n\n\n\n\n\n\n\nsymbol\nquotedate\nimplied_vol\ntotalvol\ntotaloi\n\n\n\n\n1066997\nAAU\n2017-01-03\n0.0\n103\n3217\n\n\n1067042\nACUR\n2017-01-03\n0.0\n0\n647\n\n\n1067084\nAFMD\n2017-01-03\n0.0\n50\n1655\n\n\n1067140\nALIM\n2017-01-03\n0.0\n0\n2454\n\n\n1067150\nALQA\n2017-01-03\n0.0\n0\n16\n\n\n...\n...\n...\n...\n...\n...\n\n\n2201849\nVVUS\n2018-01-31\n0.0\n0\n5085\n\n\n2201895\nWG\n2018-01-31\n0.0\n0\n1598\n\n\n2201967\nXCOOQ\n2018-01-31\n0.0\n0\n10573\n\n\n2202017\nXSPA\n2018-01-31\n0.0\n3\n3031\n\n\n2202063\nZX\n2018-01-31\n0.0\n0\n21\n\n\n\n\n45968 rows × 5 columns\n\n\n\nLet’s take care of these now. We will set these implied vols to the average non-zero implied vol in the data set. There are probably more sophisticated ways to do this that are more accurate, but using this crude approach will not affect the analysis.\n\nnon_zero_mean = df_stats_analysis.query('implied_vol &gt; 0')['implied_vol'].mean()\ndf_stats_analysis.loc[df_stats_analysis.implied_vol == 0, \"implied_vol\"] = non_zero_mean\ndf_stats_analysis.query('implied_vol &lt;= 0')\n\n\n\n\n\n\n\n\nsymbol\nquotedate\nimplied_vol\ntotalvol\ntotaloi",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#feature-construction-of-training-set-2017-and-backtest-set-2018",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#feature-construction-of-training-set-2017-and-backtest-set-2018",
    "title": "36  Option Volume: Introduction",
    "section": "36.5 Feature Construction of Training Set (2017) and Backtest Set (2018)",
    "text": "36.5 Feature Construction of Training Set (2017) and Backtest Set (2018)\nLet’s define the training set, and create the features that we may want to use.\n\ndf_train = \\\n    (\n    df_universe[['symbol','rerank']]\n        .rename(columns={'rerank':'volume_rank'})\n        .merge(df_stats_analysis, how='left', on=['symbol'])\n        .query('quotedate &lt;= \"2017-12-31\"')\n        .assign(iv_one_lag = lambda df: df.groupby(['symbol'])['implied_vol'].shift())\n        .assign(iv_change_one_lag = lambda df: df.groupby(['symbol'])['implied_vol'].pct_change().shift())\n        .assign(iv_change_two_lag = lambda df: df.groupby(['symbol'])['implied_vol'].pct_change().shift(2))\n        .assign(daily_volume_rank = lambda df: df.groupby(['quotedate'])['totalvol'].rank(ascending=False))\n        .sort_values(['symbol', 'quotedate'])\n        .merge(df_stock[['symbol', 'quotedate', 'px_close']], how='left', on=['symbol', 'quotedate'])\n        .assign(daily_return = lambda df: df.groupby(['symbol'])['px_close'].pct_change())\n        .assign(scaled_return = lambda df: df.daily_return / (df.iv_one_lag / np.sqrt(252)))\n        .assign(scaled_return_one_lag = lambda df: df['scaled_return'].shift())\n        .assign(scaled_return_two_lag = lambda df: df['scaled_return'].shift(2))\n        .assign(rank_one_lag = lambda df: df.groupby(['symbol'])['daily_volume_rank'].shift())\n        .assign(rank_two_lag = lambda df: df.groupby(['symbol'])['daily_volume_rank'].shift(2))\n        .assign(rank_change = lambda df: df.groupby(['symbol'])['daily_volume_rank'].diff())\n        .assign(rank_change_one_lag = lambda df: df.groupby(['symbol'])['rank_change'].shift())\n        .assign(rank_change_two_lag = lambda df: df.groupby(['symbol'])['rank_change'].shift(2))\n        .dropna()\n     )\ndf_train\n\n\n\n\n\n\n\n\nsymbol\nvolume_rank\nquotedate\nimplied_vol\ntotalvol\ntotaloi\niv_one_lag\niv_change_one_lag\niv_change_two_lag\ndaily_volume_rank\npx_close\ndaily_return\nscaled_return\nscaled_return_one_lag\nscaled_return_two_lag\nrank_one_lag\nrank_two_lag\nrank_change\nrank_change_one_lag\nrank_change_two_lag\n\n\n\n\n3\nACAD\n57.0\n2017-01-06\n0.8471\n4184\n120009\n0.7940\n-0.075992\n0.097165\n65.5\n32.64\n0.030303\n0.605851\n-0.150381\n2.233300\n73.0\n7.0\n-7.5\n66.0\n-103.0\n\n\n4\nACAD\n57.0\n2017-01-09\n0.7463\n2541\n118250\n0.8471\n0.066877\n-0.075992\n97.0\n32.69\n0.001532\n0.028707\n0.605851\n-0.150381\n65.5\n73.0\n31.5\n-7.5\n66.0\n\n\n5\nACAD\n57.0\n2017-01-10\n0.8257\n2145\n119307\n0.7463\n-0.118994\n0.066877\n116.0\n31.47\n-0.037320\n-0.793838\n0.028707\n0.605851\n97.0\n65.5\n19.0\n31.5\n-7.5\n\n\n6\nACAD\n57.0\n2017-01-11\n0.7765\n3366\n120644\n0.8257\n0.106392\n-0.118994\n62.0\n29.87\n-0.050842\n-0.977465\n-0.793838\n0.028707\n116.0\n97.0\n-54.0\n19.0\n31.5\n\n\n7\nACAD\n57.0\n2017-01-12\n0.7953\n5229\n122426\n0.7765\n-0.059586\n0.106392\n32.0\n31.78\n0.063944\n1.307245\n-0.977465\n-0.793838\n62.0\n116.0\n-30.0\n-54.0\n19.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n75295\nZTS\n249.0\n2017-12-22\n0.1581\n2843\n89209\n0.1865\n0.085565\n0.216714\n79.0\n71.99\n-0.004012\n-0.341508\n-0.546451\n-0.138988\n29.0\n19.0\n50.0\n10.0\n-200.0\n\n\n75296\nZTS\n249.0\n2017-12-26\n0.1485\n773\n89968\n0.1581\n-0.152279\n0.085565\n202.0\n72.34\n0.004862\n0.488162\n-0.341508\n-0.546451\n79.0\n29.0\n123.0\n50.0\n10.0\n\n\n75297\nZTS\n249.0\n2017-12-27\n0.1490\n780\n90270\n0.1485\n-0.060721\n-0.152279\n192.0\n72.45\n0.001521\n0.162550\n0.488162\n-0.341508\n202.0\n79.0\n-10.0\n123.0\n50.0\n\n\n75298\nZTS\n249.0\n2017-12-28\n0.1484\n718\n90161\n0.1490\n0.003367\n-0.060721\n199.5\n72.39\n-0.000828\n-0.088232\n0.162550\n0.488162\n192.0\n202.0\n7.5\n-10.0\n123.0\n\n\n75299\nZTS\n249.0\n2017-12-29\n0.1491\n808\n89855\n0.1484\n-0.004027\n0.003367\n212.0\n72.04\n-0.004835\n-0.517197\n-0.088232\n0.162550\n199.5\n192.0\n12.5\n7.5\n-10.0\n\n\n\n\n74400 rows × 20 columns\n\n\n\nWe will do the same feature construction for the backtest period, but the filtering will have different dates.\n\ndf_test = \\\n    (\n    df_universe[['symbol','rerank']]\n        .rename(columns={'rerank':'volume_rank'})\n        .merge(df_stats_analysis, how='left', on=['symbol'])\n        .query('quotedate &gt; \"2017-12-31\"')\n        .assign(iv_one_lag = lambda df: df.groupby(['symbol'])['implied_vol'].shift())\n        .assign(iv_change_one_lag = lambda df: df.groupby(['symbol'])['implied_vol'].pct_change().shift())\n        .assign(iv_change_two_lag = lambda df: df.groupby(['symbol'])['implied_vol'].pct_change().shift(2))\n        .assign(daily_volume_rank = lambda df: df.groupby(['quotedate'])['totalvol'].rank(ascending=False))\n        .sort_values(['symbol', 'quotedate'])\n        .merge(df_stock[['symbol', 'quotedate', 'px_close']], how='left', on=['symbol', 'quotedate'])\n        .assign(daily_return = lambda df: df.groupby(['symbol'])['px_close'].pct_change())\n        .assign(scaled_return = lambda df: df.daily_return / (df.iv_one_lag / np.sqrt(252)))\n        .assign(scaled_return_one_lag = lambda df: df['scaled_return'].shift())\n        .assign(scaled_return_two_lag = lambda df: df['scaled_return'].shift(2))\n        .assign(rank_one_lag = lambda df: df.groupby(['symbol'])['daily_volume_rank'].shift())\n        .assign(rank_two_lag = lambda df: df.groupby(['symbol'])['daily_volume_rank'].shift(2))\n        .assign(rank_change = lambda df: df.groupby(['symbol'])['daily_volume_rank'].diff())\n        .assign(rank_change_one_lag = lambda df: df.groupby(['symbol'])['rank_change'].shift())\n        .assign(rank_change_two_lag = lambda df: df.groupby(['symbol'])['rank_change'].shift(2))\n        .dropna()\n     )\ndf_test\n\n\n\n\n\n\n\n\nsymbol\nvolume_rank\nquotedate\nimplied_vol\ntotalvol\ntotaloi\niv_one_lag\niv_change_one_lag\niv_change_two_lag\ndaily_volume_rank\npx_close\ndaily_return\nscaled_return\nscaled_return_one_lag\nscaled_return_two_lag\nrank_one_lag\nrank_two_lag\nrank_change\nrank_change_one_lag\nrank_change_two_lag\n\n\n\n\n3\nACAD\n57.0\n2018-01-05\n0.5870\n650\n59158\n0.5443\n0.073147\n0.047717\n255.0\n28.90\n0.004170\n0.121605\n-0.774238\n-1.022725\n60.0\n142.0\n195.0\n-82.0\n121.0\n\n\n4\nACAD\n57.0\n2018-01-08\n0.6228\n15210\n58518\n0.5870\n0.078449\n0.073147\n7.0\n27.11\n-0.061938\n-1.675010\n0.121605\n-0.774238\n255.0\n60.0\n-248.0\n195.0\n-82.0\n\n\n5\nACAD\n57.0\n2018-01-09\n0.6016\n2152\n68583\n0.6228\n0.060988\n0.078449\n153.0\n28.29\n0.043526\n1.109441\n-1.675010\n0.121605\n7.0\n255.0\n146.0\n-248.0\n195.0\n\n\n6\nACAD\n57.0\n2018-01-10\n0.5761\n798\n65129\n0.6016\n-0.034040\n0.060988\n237.0\n28.63\n0.012018\n0.317131\n1.109441\n-1.675010\n153.0\n7.0\n84.0\n146.0\n-248.0\n\n\n7\nACAD\n57.0\n2018-01-11\n0.5355\n1081\n65813\n0.5761\n-0.042387\n-0.034040\n236.0\n27.90\n-0.025498\n-0.702593\n0.317131\n1.109441\n237.0\n153.0\n-1.0\n84.0\n146.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6295\nZTS\n249.0\n2018-01-25\n0.2039\n1662\n24055\n0.2025\n0.006962\n0.021331\n177.0\n79.25\n0.011745\n0.920736\n0.752860\n0.114461\n203.0\n227.0\n-26.0\n-24.0\n-5.5\n\n\n6296\nZTS\n249.0\n2018-01-26\n0.2046\n4137\n25213\n0.2039\n0.006914\n0.006962\n85.0\n80.09\n0.010599\n0.825207\n0.920736\n0.752860\n177.0\n203.0\n-92.0\n-26.0\n-24.0\n\n\n6297\nZTS\n249.0\n2018-01-29\n0.2426\n3662\n26275\n0.2046\n0.003433\n0.006914\n96.0\n79.18\n-0.011362\n-0.881572\n0.825207\n0.920736\n85.0\n177.0\n11.0\n-92.0\n-26.0\n\n\n6298\nZTS\n249.0\n2018-01-30\n0.2604\n365\n29722\n0.2426\n0.185728\n0.003433\n276.0\n78.35\n-0.010482\n-0.685918\n-0.881572\n0.825207\n96.0\n85.0\n180.0\n11.0\n-92.0\n\n\n6299\nZTS\n249.0\n2018-01-31\n0.2419\n1333\n29890\n0.2604\n0.073372\n0.185728\n184.0\n76.77\n-0.020166\n-1.229355\n-0.685918\n-0.881572\n276.0\n96.0\n-92.0\n180.0\n11.0\n\n\n\n\n5400 rows × 20 columns\n\n\n\nLet’s export both the training data and backtest data to CSVs for future use.\n\ndf_train.to_csv('../data/option_train_2017.csv', index=False)\ndf_test.to_csv('../data/option_test_2018.csv', index=False)",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#exploratory-data-analysis-of-training-data",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#exploratory-data-analysis-of-training-data",
    "title": "36  Option Volume: Introduction",
    "section": "36.6 Exploratory Data Analysis of Training Data",
    "text": "36.6 Exploratory Data Analysis of Training Data\nLet’s perform some exploratory data analysis on our training data.\nOur first observation is that there is a high correlation between an underlying’s previous day rank and its current rank, which makes intuitive sense.\n\ndf_train[['rank_one_lag', 'daily_volume_rank']].corr()\n\n\n\n\n\n\n\n\nrank_one_lag\ndaily_volume_rank\n\n\n\n\nrank_one_lag\n1.000000\n0.559952\n\n\ndaily_volume_rank\n0.559952\n1.000000\n\n\n\n\n\n\n\nNext, we can see that there is a fair amount of mean reversion in ranks, which means if the rank increased yesterday, it is likely to decrease today, and vice versa.\n\ndf_train.plot(kind='scatter', x='rank_change_one_lag', y='rank_change');\n\n\n\n\n\n\n\n\n\ndf_train[['rank_change_one_lag', 'rank_change']].corr()\n\n\n\n\n\n\n\n\nrank_change_one_lag\nrank_change\n\n\n\n\nrank_change_one_lag\n1.000000\n-0.434256\n\n\nrank_change\n-0.434256\n1.000000\n\n\n\n\n\n\n\nSince the correlation is stronger for rank than it is for rank-change, so we will perform predictive modeling on rank.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#top-n-ratio",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#top-n-ratio",
    "title": "36  Option Volume: Introduction",
    "section": "36.7 Top-\\(n\\) Ratio",
    "text": "36.7 Top-\\(n\\) Ratio\nUltimately, we don’t care if we get the volume rank predictions exactly right. For example, if we correctly guess the top 10 underlyings, we don’t care if we get the ordering right.\nFor this reason, let’s consider the top-\\(n\\) ratio metric, which compares the ratio of our predicted top-\\(n\\) ranked volume, and the actual top-\\(n\\) ranked volume. Note that this ratio is always less that or equal to 1.\nWe’ll begin by creating a function that calculates the top-\\(n\\) volume for all the days in our backtest period.\n\ndef top_n_volume(n):\n    df_test = pd.read_csv(\"../data/option_test_2018.csv\")\n    df_top_n_volume = \\\n    (\n    df_test\n        .query('daily_volume_rank &lt;= @n')\n        .groupby(['quotedate'])[['totalvol']].sum()\n        .reset_index()\n        .rename(columns={'totalvol':'top_' + str(n) + '_volume'})\n    )\n    return(df_top_n_volume)\n\nLet’s examine the output of this function.\n\ntop_n_volume(25)\n\n\n\n\n\n\n\n\nquotedate\ntop_25_volume\n\n\n\n\n0\n2018-01-05\n659260\n\n\n1\n2018-01-08\n516046\n\n\n2\n2018-01-09\n543219\n\n\n3\n2018-01-10\n598589\n\n\n4\n2018-01-11\n559668\n\n\n5\n2018-01-12\n634138\n\n\n6\n2018-01-16\n484750\n\n\n7\n2018-01-17\n711695\n\n\n8\n2018-01-18\n510736\n\n\n9\n2018-01-19\n735554\n\n\n10\n2018-01-22\n508607\n\n\n11\n2018-01-23\n580468\n\n\n12\n2018-01-24\n601835\n\n\n13\n2018-01-25\n524369\n\n\n14\n2018-01-26\n547504\n\n\n15\n2018-01-29\n511379\n\n\n16\n2018-01-30\n493253\n\n\n17\n2018-01-31\n499669",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#a-simple-rule-based-predictor",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#a-simple-rule-based-predictor",
    "title": "36  Option Volume: Introduction",
    "section": "36.8 A Simple Rule-Based Predictor",
    "text": "36.8 A Simple Rule-Based Predictor\nRecall that there is a high degree of correlation between yesterday’s volume rank and today’s volume rank. This observation leads to the following rules based predictor: tomorrow’s volume rank is equal to today’s. In a later section we will compare the performance of this simple rule-based predictor with linear regression on two features.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#user-defined-functions",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#user-defined-functions",
    "title": "36  Option Volume: Introduction",
    "section": "36.9 User Defined Functions",
    "text": "36.9 User Defined Functions\nIn order to evaluate predictors with top-\\(n\\) ratio we will need the following two function functions.\nThe calc_top_n_ratio() function calculates the top-\\(n\\) ratio for a given fitted model, for a given trade_date.\n\ndef calc_top_n_ratio(n, trade_date, df_test, model=None, features=[]):\n    \n    # grabbing top-n volume for each day in backtest\n    df_top_n = top_n_volume(n)\n    \n    # grabbing feature observations for trade_date\n    df_prediction = df_test.query('quotedate == @trade_date').copy()\n    \n    # selecting features from df_X\n    df_X = df_prediction[features]\n    \n    # calculating model predictions\n    if model is not None:\n        df_prediction['prediction'] = model.predict(df_X) # predictions base on model\n    else:\n        df_prediction['prediction'] = df_prediction['rank_one_lag'] # simple-rule based\n    \n    # sorting by predicted rank\n    df_prediction = df_prediction.sort_values(['prediction'])\n    # calculating predicted top-n volume\n    predicted_top_n_volume = df_prediction.head(n)['totalvol'].sum()\n    # querying for actual top-n volume\n    actual_top_n_volume = df_top_n.query('quotedate == @trade_date')['top_' + str(n) + '_volume'].values[0]\n    \n    # return the top-n-ratio\n    return(predicted_top_n_volume / actual_top_n_volume)\n\nThe backtest() function takes a fitted model and loops through all the trade_dates in the backtest period and uses the calc_top_n_ratio() function to calculate the all the top-\\(n\\) ratios.\n\ndef backtest(n, df_test, model=None, features=[]):\n    # all trade dates in backtest period\n    trade_dates = df_test['quotedate'].unique().tolist()\n    \n    # calculating all top-n ratios\n    top_n_ratios = []\n    for ix_trade_date in trade_dates:\n        top_n_ratios.append(calc_top_n_ratio(n, ix_trade_date, df_test, model, features))\n\n    # creating a dataframe of daily top-n ratios\n    df_daily = pd.DataFrame({\n        'trade_date':trade_dates,\n        'top_'+str(n)+'_volume': np.round(top_n_ratios, 3),\n    })\n\n    # calculating summary statsics of top-n ratios during backtest period\n    df_stats = pd.DataFrame({\n        'model':[str(model)],\n        'average':[np.mean(top_n_ratios).round(3)],\n        'std_dev':[np.std(top_n_ratios).round(3)],\n        'minimum':[np.min(top_n_ratios).round(3)],\n        'maximum':[np.max(top_n_ratios).round(3)],\n    })\n\n    return([df_daily, df_stats])",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#calculating-performance-of-simple-rule-based-predictor",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#calculating-performance-of-simple-rule-based-predictor",
    "title": "36  Option Volume: Introduction",
    "section": "36.10 Calculating Performance of Simple Rule-Based Predictor",
    "text": "36.10 Calculating Performance of Simple Rule-Based Predictor\nWe can now use the functions defined above to calculate the performance of the simple rule-based predictor.\nAs we can see, a simple rule-based scheme of simply guessing that volume rank remains unchanged has an average top-25 ratio of 59%.\n\nbacktest(25, df_test)\n\n[    trade_date  top_25_volume\n 0   2018-01-05          0.768\n 1   2018-01-08          0.556\n 2   2018-01-09          0.624\n 3   2018-01-10          0.467\n 4   2018-01-11          0.678\n 5   2018-01-12          0.504\n 6   2018-01-16          0.591\n 7   2018-01-17          0.516\n 8   2018-01-18          0.419\n 9   2018-01-19          0.610\n 10  2018-01-22          0.675\n 11  2018-01-23          0.562\n 12  2018-01-24          0.550\n 13  2018-01-25          0.563\n 14  2018-01-26          0.722\n 15  2018-01-29          0.592\n 16  2018-01-30          0.525\n 17  2018-01-31          0.753,\n   model  average  std_dev  minimum  maximum\n 0  None    0.593    0.094    0.419    0.768]",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/33_option_volume_introduction/option_volume_introduction.html#simple-linear-regression",
    "href": "chapters/33_option_volume_introduction/option_volume_introduction.html#simple-linear-regression",
    "title": "36  Option Volume: Introduction",
    "section": "36.11 Simple Linear Regression",
    "text": "36.11 Simple Linear Regression\nLet’s fit a linear regression with rank_one_lag and rank_two_lag and then see if this model does better.\n\nfeatures = ['rank_one_lag', 'rank_two_lag']\nfrom sklearn.linear_model import LinearRegression\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nlinear_regression = LinearRegression()\nlinear_regression.fit(df_features, np.ravel(df_label.values))\nlinear_regression.score(df_features, df_label)\n\n0.3652412348607029\n\n\n\nbacktest(25, df_test, linear_regression, features)\n\n[    trade_date  top_25_volume\n 0   2018-01-05          0.816\n 1   2018-01-08          0.634\n 2   2018-01-09          0.651\n 3   2018-01-10          0.531\n 4   2018-01-11          0.734\n 5   2018-01-12          0.500\n 6   2018-01-16          0.620\n 7   2018-01-17          0.461\n 8   2018-01-18          0.478\n 9   2018-01-19          0.610\n 10  2018-01-22          0.696\n 11  2018-01-23          0.580\n 12  2018-01-24          0.584\n 13  2018-01-25          0.620\n 14  2018-01-26          0.709\n 15  2018-01-29          0.605\n 16  2018-01-30          0.736\n 17  2018-01-31          0.667,\n                 model  average  std_dev  minimum  maximum\n 0  LinearRegression()    0.624    0.092    0.461    0.816]\n\n\nClearly the improvement is only modest. The average top-25 ratio of the linear regression is 3-points higher. Additionally, the standard deviation is slightly lower, and the min and max are both closer to 100%.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Option Volume: Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "",
    "text": "37.1 Import Packages\nIn this chapter we continue our prediction of option volume rank with the data that we wrangled in the previous chapter. In particular, we will:\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#reading-in-data",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#reading-in-data",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "37.2 Reading-In Data",
    "text": "37.2 Reading-In Data\nNext, let’s read-in our training data and backtest data.\n\ndf_train = pd.read_csv('../data/option_train_2017.csv')\ndf_test = pd.read_csv('../data/option_test_2018.csv')",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#starting-features",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#starting-features",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "37.3 Starting Features",
    "text": "37.3 Starting Features\nWe will start with the following four features.\n\nfeatures = ['iv_change_one_lag', 'scaled_return_one_lag', 'rank_one_lag', 'rank_change_one_lag']",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#linear-regression",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#linear-regression",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "37.4 Linear Regression",
    "text": "37.4 Linear Regression\nWe can now run a linear regression with these features.\n\nfrom sklearn.linear_model import LinearRegression\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nlinear_regression = LinearRegression()\nlinear_regression.fit(df_features, np.ravel(df_label.values))\nlinear_regression.score(df_features, df_label)\n\n0.3654074977254579\n\n\nLet’s check the parameters and see if they make intuitive sense.\n\ndf_linear_regression_coefficients = \\\n    pd.DataFrame({\n        'feature':features,\n        'coefficient':linear_regression.coef_\n    })\ndf_linear_regression_coefficients\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n0\niv_change_one_lag\n0.010543\n\n\n1\nscaled_return_one_lag\n-1.208765\n\n\n2\nrank_one_lag\n0.680732\n\n\n3\nrank_change_one_lag\n-0.274649\n\n\n\n\n\n\n\nInterpretation: 1. iv_change - a positive change in implied vol could be caused by supply/demand effects of increased option buying, which could carry through to the following day 1. scaled_return - when a stock goes down, long positions in the stock get fearful (or greedy) and option buying increases 1. rank_one_lag - if an underlying has high rank one day, it will likely have high rank the next day 1. rank_change_one_lag - if an underlying has a jump in volume one day, it will usually revert back to previous levels the next day",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#feature-selection-using-lasso",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#feature-selection-using-lasso",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "37.5 Feature Selection Using Lasso",
    "text": "37.5 Feature Selection Using Lasso\nLasso regression is a linear regression technique that minimizes an objective function that involves residual-sum-of-squares and also the magnitude of the regression coefficients.\nIn particular, it penalizes the objective for the collective magnitude of the regression coefficients. This has the effect of making the coefficients of the non-predictive features equal to zero.\nThus, lasso regression can be a way of weeding out non-predictive coefficients.\nLet’s next fit a lasso regression to our four features.\n\nfrom sklearn.linear_model import Lasso\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nlasso = Lasso(alpha=0.10)\nlasso.fit(df_features, np.ravel(df_label.values))\nlasso.score(df_features, df_label)\n\n0.3654059332222376\n\n\nWe can now examine the coefficients. Notice that iv_change_one_lag has a value of 0, and thus it is not that predictive.\n\ndf_lasso_coefficients = \\\n    pd.DataFrame({\n        'feature':features,\n        'coefficient':lasso.coef_\n    })\ndf_lasso_coefficients\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n0\niv_change_one_lag\n0.000000\n\n\n1\nscaled_return_one_lag\n-1.091635\n\n\n2\nrank_one_lag\n0.680723\n\n\n3\nrank_change_one_lag\n-0.274577\n\n\n\n\n\n\n\nThe alpha hyperparameter controls how much the coefficient sizes are penalized. We can use cross-validation to choose the optimal level of alpha.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Lasso\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nalphas = np.linspace(0.1, 1, 10)\nfor ix_alpha in alphas:\n   lasso = Lasso(alpha=ix_alpha)\n   cvs = cross_val_score(lasso, df_features, df_label, cv = 10)\n   print(np.round(ix_alpha, 2), cvs.mean())\n\n0.1 0.3565912774339949\n0.2 0.35658669987577574\n0.3 0.35657893041868144\n0.4 0.35656800276703626\n0.5 0.35655390432426176\n0.6 0.35653663509035816\n0.7 0.35651619506532517\n0.8 0.3564925842491631\n0.9 0.3564673297611635\n1.0 0.3564481756197352\n\n\nIn our case, the value of alpha doesn’t seem to matter that much. So we’ll leave it as is.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#selecting-predictive-features",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#selecting-predictive-features",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "37.6 Selecting Predictive Features",
    "text": "37.6 Selecting Predictive Features\nWe can remove iv_change_one_lag as our lasso regression showed that it has low predictive power.\n\nfeatures = ['scaled_return_one_lag', 'rank_one_lag', 'rank_change_one_lag']",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#user-defined-functions",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#user-defined-functions",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "37.7 User Defined Functions",
    "text": "37.7 User Defined Functions\nLet’s create the user defined functions we will need to use our top-\\(n\\) metric in our backtest. These functions were introduced in the previous chapter.\n\ndef top_n_volume(n):\n    df_test = pd.read_csv(\"../data/option_test_2018.csv\")\n    df_top_n_volume = \\\n    (\n    df_test\n        .query('daily_volume_rank &lt;= @n')\n        .groupby(['quotedate'])[['totalvol']].sum()\n        .reset_index()\n        .rename(columns={'totalvol':'top_' + str(n) + '_volume'})\n    )\n    return(df_top_n_volume)\n\n\ndef calc_top_n_ratio(n, trade_date, df_test, model=None, features=[]):\n    \n    # grabbing top-n volume for each day in backtest\n    df_top_n = top_n_volume(n)\n    \n    # grabbing feature observations for trade_date\n    df_prediction = df_test.query('quotedate == @trade_date').copy()\n    \n    # selecting features from df_X\n    df_X = df_prediction[features]\n    \n    # calculating model predictions\n    if model is not None:\n        df_prediction['prediction'] = model.predict(df_X) # predictions based on model\n    else:\n        df_prediction['prediction'] = df_prediction['rank_one_lag'] # simple-rule based predictor\n    \n    # sorting by predicted rank\n    df_prediction = df_prediction.sort_values(['prediction'])\n    # calculating predicted top-n volume\n    predicted_top_n_volume = df_prediction.head(n)['totalvol'].sum()\n    # querying for actual top-n volume\n    actual_top_n_volume = df_top_n.query('quotedate == @trade_date')['top_' + str(n) + '_volume'].values[0]\n    \n    # return the top-n-ratio\n    return(predicted_top_n_volume / actual_top_n_volume)\n\n\ndef backtest(n, df_test, model=None, features=[]):\n    # all trade dates in backtest period\n    trade_dates = df_test['quotedate'].unique().tolist()\n    \n    # calculating all top-n ratios\n    top_n_ratios = []\n    for ix_trade_date in trade_dates:\n        top_n_ratios.append(calc_top_n_ratio(n, ix_trade_date, df_test, model, features))\n\n    # creating a dataframe of daily top-n ratios\n    df_daily = pd.DataFrame({\n        'trade_date':trade_dates,\n        'top_'+str(n)+'_volume': np.round(top_n_ratios, 3),\n    })\n\n    # calculating summary statsics of top-n ratios during backtest period\n    df_stats = pd.DataFrame({\n        'model':[str(model)],\n        'average':[np.mean(top_n_ratios).round(3)],\n        'std_dev':[np.std(top_n_ratios).round(3)],\n        'minimum':[np.min(top_n_ratios).round(3)],\n        'maximum':[np.max(top_n_ratios).round(3)],\n    })\n\n    return([df_daily, df_stats])",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#k-nearest-neighbors",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#k-nearest-neighbors",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "37.8 K Nearest Neighbors",
    "text": "37.8 K Nearest Neighbors\nIn this section we’ll fit a KNeighborsRegressor to our training data and see how it performs during the backtest period.\nFirst, let’s use a 10-fold cross validation (using \\(R^2\\) as metric) to determine optimal value of n_neighbors.\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nk = range(100, 1100, 100)\nfor ix_k in k:\n    knn = KNeighborsRegressor(n_neighbors=ix_k)\n    cvs = cross_val_score(knn, df_features, df_label, cv = 10)\n    print(ix_k, cvs.mean())\n\n100 0.36101494817841573\n200 0.36386843260646606\n300 0.3646175979349766\n400 0.36480461178161694\n500 0.3647867229860612\n600 0.3646215082035253\n700 0.3644970083233438\n800 0.36423480801137564\n900 0.36403894419896105\n1000 0.3638359335614717\n\n\nThe model doesn’t seem particularly sensitive to the value of n_neighbors, so let’s just use 400 because it had the highest \\(R^2\\) and the run-time seems reasonable.\nNext, let’s fit a KNeighborsRegressor to the entirety of our training data use n_neighbors=400.\n\nfrom sklearn.neighbors import KNeighborsRegressor\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nknn = KNeighborsRegressor(n_neighbors=400)\nknn.fit(df_features, np.ravel(df_label.values))\nknn.score(df_features, df_label)\n\n0.37700389902243137\n\n\nWe can now use our fitted model to perform our backtest using top-25 ratio as our metric for success.\n\nbacktest(10, df_test, knn, features)\n\n[    trade_date  top_10_volume\n 0   2018-01-05          0.831\n 1   2018-01-08          0.577\n 2   2018-01-09          0.659\n 3   2018-01-10          0.421\n 4   2018-01-11          0.532\n 5   2018-01-12          0.309\n 6   2018-01-16          0.599\n 7   2018-01-17          0.467\n 8   2018-01-18          0.398\n 9   2018-01-19          0.625\n 10  2018-01-22          0.708\n 11  2018-01-23          0.657\n 12  2018-01-24          0.496\n 13  2018-01-25          0.616\n 14  2018-01-26          0.763\n 15  2018-01-29          0.608\n 16  2018-01-30          0.471\n 17  2018-01-31          0.690,\n                                   model  average  std_dev  minimum  maximum\n 0  KNeighborsRegressor(n_neighbors=400)    0.579    0.131    0.309    0.831]\n\n\nAs we can see, our KNN model actually performs worse than the simple rule-based predictor that we introduced in the previous chapter.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#random-forest",
    "href": "chapters/34_option_volume_feature_selection/option_volume_feature_selection.html#random-forest",
    "title": "37  Option Volume: Feature Selection and Hyperparameter Tuning",
    "section": "37.9 Random Forest",
    "text": "37.9 Random Forest\nIn this section we’ll run our backtest using a RandomForestRegressor. I’ve already run a cross-validation analysis that n_estimators=10 has a good trade off of performance and run time.\nLet’s find an optimal value of max_depth using a 10-fold cross validation.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nd = range(1, 21, 1)\nfor ix_d in d:\n    random_forest = RandomForestRegressor(n_estimators=10, max_depth=ix_d)\n    cvs = cross_val_score(random_forest, df_features, np.ravel(df_label.values), cv = 10)\n    print(ix_d, cvs.mean())\n\n1 0.21323542515349816\n2 0.28667668385608386\n3 0.32823705965447214\n4 0.350204904212642\n5 0.3604337950155293\n6 0.3643107573903573\n7 0.36517895534208716\n8 0.3638805625055838\n9 0.3627851611360925\n10 0.3582779076013644\n11 0.3539930070013009\n12 0.34701968731147276\n13 0.34101687956906235\n14 0.3326469340556921\n15 0.3248038486930812\n16 0.31443952345817633\n17 0.3059982711930008\n18 0.29436642171242966\n19 0.2866621073083838\n20 0.27945630028663293\n\n\nBased on our cross-validation analysis above, let’s use max_depth=7 to train our model.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nrandom_forest = RandomForestRegressor(n_estimators = 10, max_depth=7)\nrandom_forest.fit(df_features, np.ravel(df_label.values))\nrandom_forest.score(df_features, df_label)\n\n0.387368629069925\n\n\nWe can now run our backtest using the top-25 metric for our measure of success.\n\nbacktest(10, df_test, random_forest, features)\n\n[    trade_date  top_10_volume\n 0   2018-01-05          0.831\n 1   2018-01-08          0.577\n 2   2018-01-09          0.694\n 3   2018-01-10          0.421\n 4   2018-01-11          0.532\n 5   2018-01-12          0.338\n 6   2018-01-16          0.599\n 7   2018-01-17          0.511\n 8   2018-01-18          0.398\n 9   2018-01-19          0.625\n 10  2018-01-22          0.728\n 11  2018-01-23          0.571\n 12  2018-01-24          0.496\n 13  2018-01-25          0.661\n 14  2018-01-26          0.763\n 15  2018-01-29          0.576\n 16  2018-01-30          0.471\n 17  2018-01-31          0.690,\n                                                model  average  std_dev   \n 0  RandomForestRegressor(max_depth=7, n_estimator...    0.582    0.128  \\\n \n    minimum  maximum  \n 0    0.338    0.831  ]\n\n\nAs we can see, our random forest model also under performs relative our simple rule based model.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Option Volume: Feature Selection and Hyperparameter Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html",
    "title": "38  Option Volume: xgboost",
    "section": "",
    "text": "38.1 Importing Packages\nXGBoost is a general framework for constructing gradient boosted trees; the Python implementation is a package called xgboost.\nThis chapter is a continuation of the option volume prediction work we have been doing in previous chapters. In particular, we show how to use xgboost in that context.\nLet’s begin by importing the packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Option Volume: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#reading-in-data",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#reading-in-data",
    "title": "38  Option Volume: xgboost",
    "section": "38.2 Reading-In Data",
    "text": "38.2 Reading-In Data\nNext, let’s read-in our training data and testing data.\n\ndf_train = pd.read_csv('../data/option_train_2017.csv')\ndf_test = pd.read_csv('../data/option_test_2018.csv')",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Option Volume: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#feature-selection",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#feature-selection",
    "title": "38  Option Volume: xgboost",
    "section": "38.3 Feature Selection",
    "text": "38.3 Feature Selection\nFor this exercise we will use all the features available in our training data set.\n\nfeatures = ['iv_change_one_lag', 'iv_change_two_lag', 'scaled_return_one_lag', \n            'scaled_return_two_lag', 'rank_one_lag', 'rank_two_lag',\n            'rank_change_one_lag', 'rank_change_two_lag',]",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Option Volume: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#user-defined-functions",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#user-defined-functions",
    "title": "38  Option Volume: xgboost",
    "section": "38.4 User Defined Functions",
    "text": "38.4 User Defined Functions\nIn this section we import the three custom functions that are needed to execute our backtest. These functions were introduced in a previous chapter.\n\ndef top_n_volume(n):\n    df_test = pd.read_csv(\"../data/option_test_2018.csv\")\n    df_top_n_volume = \\\n    (\n    df_test\n        .query('daily_volume_rank &lt;= @n')\n        .groupby(['quotedate'])[['totalvol']].sum()\n        .reset_index()\n        .rename(columns={'totalvol':'top_' + str(n) + '_volume'})\n    )\n    return(df_top_n_volume)\n\n\ndef calc_top_n_ratio(n, trade_date, df_test, model=None, features=[]):\n    \n    # grabbing top-n volume for each day in backtest\n    df_top_n = top_n_volume(n)\n    \n    # grabbing feature observations for trade_date\n    df_prediction = df_test.query('quotedate == @trade_date').copy()\n    \n    # selecting features from df_X\n    df_X = df_prediction[features]\n    \n    # calculating model predictions\n    if model is not None:\n        df_prediction['prediction'] = model.predict(df_X) # predictions based on model\n    else:\n        df_prediction['prediction'] = df_prediction['rank_one_lag'] # simple-rule based predictor\n    \n    # sorting by predicted rank\n    df_prediction = df_prediction.sort_values(['prediction'])\n    # calculating predicted top-n volume\n    predicted_top_n_volume = df_prediction.head(n)['totalvol'].sum()\n    # querying for actual top-n volume\n    actual_top_n_volume = df_top_n.query('quotedate == @trade_date')['top_' + str(n) + '_volume'].values[0]\n    \n    # return the top-n-ratio\n    return(predicted_top_n_volume / actual_top_n_volume)\n\n\ndef backtest(n, df_test, model=None, features=[]):\n    # all trade dates in backtest period\n    trade_dates = df_test['quotedate'].unique().tolist()\n    \n    # calculating all top-n ratios\n    top_n_ratios = []\n    for ix_trade_date in trade_dates:\n        top_n_ratios.append(calc_top_n_ratio(n, ix_trade_date, df_test, model, features))\n\n    # creating a dataframe of daily top-n ratios\n    df_daily = pd.DataFrame({\n        'trade_date':trade_dates,\n        'top_'+str(n)+'_volume': np.round(top_n_ratios, 3),\n    })\n\n    # calculating summary statistics of top-n ratios during backtest period\n    df_stats = pd.DataFrame({\n        'model':[str(model)],\n        'average':[np.mean(top_n_ratios).round(3)],\n        'std_dev':[np.std(top_n_ratios).round(3)],\n        'minimum':[np.min(top_n_ratios).round(3)],\n        'maximum':[np.max(top_n_ratios).round(3)],\n    })\n\n    return([df_daily, df_stats])",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Option Volume: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#hyperparameter-tuning",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#hyperparameter-tuning",
    "title": "38  Option Volume: xgboost",
    "section": "38.5 Hyperparameter Tuning",
    "text": "38.5 Hyperparameter Tuning\nThe learning_rate is rate at which successive trees are boosted; a lower learning_rate amounts to slower learning.\nHere we use a 5-fold cross-validation to select the optimal learning_rate. We will use \\(R^2\\) as our goodness of fit metric.\n\nfrom sklearn.model_selection import cross_val_score\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nalphas = np.linspace(0.1, 1, 10)\nfor ix_alpha in alphas:\n   xgb_model = XGBRegressor(n_estimators=25, max_depth=3, learning_rate=ix_alpha, random_state=0)\n   cvs = cross_val_score(xgb_model, df_features, df_label, cv = 5)\n   print(np.round(ix_alpha, 2), cvs.mean())\n\n0.1 0.36246615782763447\n0.2 0.38969475875156323\n0.3 0.3910816017808643\n0.4 0.38967754013651945\n0.5 0.38808813540767895\n0.6 0.38692635332836656\n0.7 0.38474370336623864\n0.8 0.3841157442722449\n0.9 0.38194061118381856\n1.0 0.37935841896562783\n\n\nAs we can see, learning_rate = 0.3 yields the highest \\(R^2\\).",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Option Volume: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#fitting-model",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#fitting-model",
    "title": "38  Option Volume: xgboost",
    "section": "38.6 Fitting Model",
    "text": "38.6 Fitting Model\nNow we are ready to fit the our model with learning_rate = 0.3. Notice that we are increasing n_estimators=500.\n\ndf_features = df_train[features]\ndf_label = df_train[['daily_volume_rank']]\nxg_model = XGBRegressor(n_estimators=500, max_depth=3, learning_rate=0.3, random_state=0)\nxg_model.fit(df_features, df_label)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=0, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.3, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=0, ...)\n\n\n\nsklearn.metrics.r2_score(df_label, xg_model.predict(df_features))\n\n0.4621096942085411",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Option Volume: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#backtest",
    "href": "chapters/35_option_volume_xgboost/option_volume_xgboost.html#backtest",
    "title": "38  Option Volume: xgboost",
    "section": "38.7 Backtest",
    "text": "38.7 Backtest\nLet’s run our backtest with the fit model.\n\nbacktest(25, df_test, xg_model, features)\n\n[    trade_date  top_25_volume\n 0   2018-01-05          0.827\n 1   2018-01-08          0.574\n 2   2018-01-09          0.694\n 3   2018-01-10          0.535\n 4   2018-01-11          0.780\n 5   2018-01-12          0.562\n 6   2018-01-16          0.584\n 7   2018-01-17          0.552\n 8   2018-01-18          0.475\n 9   2018-01-19          0.642\n 10  2018-01-22          0.717\n 11  2018-01-23          0.563\n 12  2018-01-24          0.599\n 13  2018-01-25          0.588\n 14  2018-01-26          0.868\n 15  2018-01-29          0.577\n 16  2018-01-30          0.546\n 17  2018-01-31          0.679,\n                                                model  average  std_dev   \n 0  XGBRegressor(base_score=None, booster=None, ca...    0.631    0.105  \\\n \n    minimum  maximum  \n 0    0.475    0.868  ]\n\n\nAnd we can compare our results to the simple rules based strategy.\n\nbacktest(25, df_test)\n\n[    trade_date  top_25_volume\n 0   2018-01-05          0.768\n 1   2018-01-08          0.556\n 2   2018-01-09          0.624\n 3   2018-01-10          0.467\n 4   2018-01-11          0.678\n 5   2018-01-12          0.504\n 6   2018-01-16          0.591\n 7   2018-01-17          0.516\n 8   2018-01-18          0.419\n 9   2018-01-19          0.610\n 10  2018-01-22          0.675\n 11  2018-01-23          0.562\n 12  2018-01-24          0.550\n 13  2018-01-25          0.563\n 14  2018-01-26          0.722\n 15  2018-01-29          0.592\n 16  2018-01-30          0.525\n 17  2018-01-31          0.753,\n   model  average  std_dev  minimum  maximum\n 0  None    0.593    0.094    0.419    0.768]\n\n\n\nCode Challenge: Search the documentation and find a model hyper-parameter to tune. Then see how the new model performs with that hyper-parameter set to the optimal value that you found.",
    "crumbs": [
      "Classical Machine Learning",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Option Volume: **xgboost**</span>"
    ]
  },
  {
    "objectID": "chapters/36a_mnist_digit_recognition/mnist_digit_recognition.html",
    "href": "chapters/36a_mnist_digit_recognition/mnist_digit_recognition.html",
    "title": "39  MNIST Digit Recognition",
    "section": "",
    "text": "39.1 Reading-In Data\nThis chapter contains the code samples found in Chapter 2, Section 1 of Deep Learning with Python.\nThe problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been around for almost as long as the field itself and has been very intensively studied. It’s a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of “solving” MNIST as the “Hello World” of deep learning – it’s what you do to verify that your algorithms are working as expected. As you become a machine learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on.\nThe MNIST dataset comes pre-loaded in keras, in the form of a set of four numpy arrays.\nfrom keras.datasets import mnist\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n2023-08-31 15:55:47.729038: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-08-31 15:55:47.777390: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-08-31 15:55:47.779107: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-08-31 15:55:48.608385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nLet’s examine the structure of of these arrays.\ntrain_images\n\narray([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)\ntrain_labels\n\narray([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\ntrain_images and train_labels form the training set, the data that the model will learn from. The model will then be tested on the “test set”, test_images and test_labels. Our images are encoded as numpy arrays, and the labels are simply an array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.\nLet’s have a look at the training data.\ntrain_images.shape\n\n(60000, 28, 28)\ntrain_labels\n\narray([5, 0, 4, ..., 5, 6, 8], dtype=uint8)\nLet’s have a look at the test data.\ntest_images.shape\n\n(10000, 28, 28)\ntest_labels\n\narray([7, 2, 1, ..., 4, 5, 6], dtype=uint8)\nOur workflow will be as follow: first we will present our neural network with the training data, train_images and train_labels. The network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for test_images, and we will verify if these predictions match the labels from test_labels.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>MNIST Digit Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/36a_mnist_digit_recognition/mnist_digit_recognition.html#data-wrangling",
    "href": "chapters/36a_mnist_digit_recognition/mnist_digit_recognition.html#data-wrangling",
    "title": "39  MNIST Digit Recognition",
    "section": "39.2 Data Wrangling",
    "text": "39.2 Data Wrangling\nBefore training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in the [0, 1] interval. Previously, our training images for instance were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1.\n\ntrain_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype('float32') / 255\n\ntest_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype('float32') / 255\n\nWe also need to one-hot encode the labels.\n\nfrom keras.utils import to_categorical\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n\ntrain_labels\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)\n\n\n\ntest_labels\n\narray([[0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>MNIST Digit Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/36a_mnist_digit_recognition/mnist_digit_recognition.html#building-the-network",
    "href": "chapters/36a_mnist_digit_recognition/mnist_digit_recognition.html#building-the-network",
    "title": "39  MNIST Digit Recognition",
    "section": "39.3 Building the Network",
    "text": "39.3 Building the Network\nIn keras, a dense neural network is constructed by instantiating a Sequential model and then adding layers to it.\n\nfrom keras import models\nfrom keras import layers\n\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\nnetwork.add(layers.Dense(10, activation='softmax'))\n\nThe core building block of neural networks is the layer, a data-processing module which you can conceive as a “filter” for data. Some data comes in, and comes out in a more useful form. Precisely, layers extract representations out of the data fed into them – hopefully representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers which will implement a form of progressive “data distillation”. A deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters – the layers.\nHere our network consists of a sequence of two Dense layers, which are densely-connected (also called fully-connected) neural layers. The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\nTo make our network ready for training, we need to pick three more things, as part of the compilation step:\n\nA loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction.\nAn optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\nMetrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly classified).\n\n\nnetwork.compile(loss='categorical_crossentropy',\n                optimizer='rmsprop',\n                metrics=['accuracy'])",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>MNIST Digit Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/36a_mnist_digit_recognition/mnist_digit_recognition.html#fitting-and-testing-the-network",
    "href": "chapters/36a_mnist_digit_recognition/mnist_digit_recognition.html#fitting-and-testing-the-network",
    "title": "39  MNIST Digit Recognition",
    "section": "39.4 Fitting and Testing the Network",
    "text": "39.4 Fitting and Testing the Network\nWe are now ready to train our network, which in keras is done via a call to the .fit() method of the network: we “fit” the model to its training data.\n\nnetwork.fit(train_images, train_labels, epochs=5, batch_size=128)\n\nEpoch 1/5\n\n\n2023-08-31 15:55:50.006682: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n\n\n469/469 [==============================] - 3s 5ms/step - loss: 0.2671 - accuracy: 0.9230\nEpoch 2/5\n469/469 [==============================] - 2s 4ms/step - loss: 0.1090 - accuracy: 0.9679\nEpoch 3/5\n469/469 [==============================] - 2s 5ms/step - loss: 0.0709 - accuracy: 0.9783\nEpoch 4/5\n469/469 [==============================] - 2s 4ms/step - loss: 0.0515 - accuracy: 0.9843\nEpoch 5/5\n469/469 [==============================] - 2s 4ms/step - loss: 0.0385 - accuracy: 0.9884\n\n\n&lt;keras.src.callbacks.History&gt;\n\n\nTwo quantities are being displayed during training: the loss of the network over the training data, and the accuracy of the network over the training data.\nWe quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now let’s check that our model performs well on the test set too.\n\ntest_loss, test_acc = network.evaluate(test_images, test_labels)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9791\n\n\n\nprint('test_acc:', test_acc)\n\ntest_acc: 0.9790999889373779\n\n\nOur test set accuracy turns out to be 97.8% – that’s quite a bit lower than the training set accuracy. This gap between training accuracy and test accuracy is an example of overfitting, the fact that machine learning models tend to perform worse on new data than on their training data.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>MNIST Digit Recognition</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html",
    "href": "chapters/36b_imdb_classification/imdb_classification.html",
    "title": "40  IMBD Movie Reviews",
    "section": "",
    "text": "40.1 Loading the Data Set\nIn this tutorial we will work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews.\nThe IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary. This enables us to focus on model building, training, and evaluation.\nThe following code will load the dataset (when you run it the first time, about 80 MB of data will be downloaded to your machine).\nThe argument num_words=10000 means you’ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows us to work with vector data of manageable size. If we didn’t set this limit, we’d be working with 88,585 unique words in the training data, which is unnecessarily large. Many of these words only occur in a single sample, and thus can’t be meaningfully used for classification.\nfrom tensorflow.keras.datasets import imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=10000)\n\n2023-11-07 13:46:55.134895: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-07 13:46:55.170619: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-11-07 13:46:55.170648: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2023-11-07 13:46:55.170677: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-11-07 13:46:55.176988: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-07 13:46:55.177807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-07 13:46:56.042119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nThe variables train_data and test_data are lists of reviews; each review is a list of word indices (encoding a sequence of words).\nprint(train_data[0])\n\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\ntrain_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive.\ntrain_labels[0]\n\n1\nBecause we’re restricting ourselves to the top 10,000 most frequent words, no word index will exceed 10,000.\nmax([max(sequence) for sequence in train_data])\n\n9999",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#decoding-reviews",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#decoding-reviews",
    "title": "40  IMBD Movie Reviews",
    "section": "40.2 Decoding Reviews",
    "text": "40.2 Decoding Reviews\nFor purposes of illustration, the following code decodes one of the reviews back to text.\n\nword_index = imdb.get_word_index()\nreverse_word_index = dict(\n    [(value, key) for (key, value) in word_index.items()]\n)\ndecoded_review = \" \".join(\n    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]]\n)\n\n\ndecoded_review\n\n\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\"",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#preparing-the-data",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#preparing-the-data",
    "title": "40  IMBD Movie Reviews",
    "section": "40.3 Preparing the Data",
    "text": "40.3 Preparing the Data\nYou can’t directly feed lists of integers into a neural network. They all have different lengths, but a neural network expects to process contiguous batches of data. You have to turn your lists into tensors. In particular we will use multi-hot encoding.\nThis would mean, for instance, turning the sequence [8, 5] into a 10,000-dimensional vector that would be all 0s except for indices 8 and 5, which would be 1s. Then you could use a Dense layer, capable of handling floating-point vector data, as the first layer in your model.\nThe following code performs the multi-hot encoding. Notice that we are flattening our text data so that we are losing all sense of ordering of the words.\n\nimport numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        for j in sequence:\n            results[i, j] = 1.\n    return results\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\nHere’s what the samples look like now:\n\nx_train[0]\n\narray([0., 1., 1., ..., 0., 0., 0.])\n\n\nLet’s also vectorize the labels.\n\ny_train = np.asarray(train_labels).astype(\"float32\")\ny_test = np.asarray(test_labels).astype(\"float32\")",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#building-the-model",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#building-the-model",
    "title": "40  IMBD Movie Reviews",
    "section": "40.4 Building the Model",
    "text": "40.4 Building the Model\nThe input data is vectors, and the labels are scalars (1s and 0s): this is one of the simplest problem setups you’ll ever encounter. A type of model that performs well on such a problem is a plain stack of densely connected (Dense) layers with relu activations.\nThere are two key architecture decisions to be made about such a stack of Dense layers:\n\nHow many layers to use\nHow many units to choose for each layer\n\nWe will use two hidden layers with 16 units each and a relu activation function. Since we are predicting a single outcome (positive or negative) our output layer will have a single unit and we will use a sigmoid activation because this is a binary classification problem.\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nFinally, you need to choose a loss function and an optimizer. Because you’re facing a binary classification problem and the output of your model is a probability (you end your model with a single-unit layer with a sigmoid activation), it’s best to use the binary_crossentropy loss. It isn’t the only viable choice: for instance, you could use mean_squared_error. But crossentropy is usually the best choice when you’re dealing with models that output probabilities.\nAs for the choice of the optimizer, we’ll go with rmsprop, which is a usually a good default choice for virtually any problem.\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#creating-a-validation-set",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#creating-a-validation-set",
    "title": "40  IMBD Movie Reviews",
    "section": "40.5 Creating a Validation Set",
    "text": "40.5 Creating a Validation Set\nA deep learning model should never be evaluated on its training data — it’s standard practice to use a validation set to monitor the accuracy of the model during training. Here, we’ll create a validation set by setting apart 10,000 samples from the original training data.\n\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#training-the-model",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#training-the-model",
    "title": "40  IMBD Movie Reviews",
    "section": "40.6 Training the Model",
    "text": "40.6 Training the Model\nWe will now train the model for 20 epochs (20 iterations over all samples in the training data) in mini-batches of 512 samples. At the same time, we will monitor loss and accuracy on the 10,000 samples that we set apart. We do so by passing the validation data as the validation_data argument.\n\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n\nEpoch 1/20\n\n\n2023-11-07 13:52:47.420717: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 600000000 exceeds 10% of free system memory.\n\n\n27/30 [==========================&gt;...] - ETA: 0s - loss: 0.5326 - accuracy: 0.7713\n\n\n2023-11-07 13:52:48.719414: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 400000000 exceeds 10% of free system memory.\n\n\n30/30 [==============================] - 1s 32ms/step - loss: 0.5202 - accuracy: 0.7799 - val_loss: 0.3975 - val_accuracy: 0.8579\nEpoch 2/20\n30/30 [==============================] - 0s 12ms/step - loss: 0.3163 - accuracy: 0.8985 - val_loss: 0.3150 - val_accuracy: 0.8773\nEpoch 3/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.2351 - accuracy: 0.9231 - val_loss: 0.2828 - val_accuracy: 0.8882\nEpoch 4/20\n30/30 [==============================] - 0s 14ms/step - loss: 0.1887 - accuracy: 0.9397 - val_loss: 0.2874 - val_accuracy: 0.8842\nEpoch 5/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.1590 - accuracy: 0.9471 - val_loss: 0.3151 - val_accuracy: 0.8760\nEpoch 6/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.1356 - accuracy: 0.9569 - val_loss: 0.2940 - val_accuracy: 0.8816\nEpoch 7/20\n30/30 [==============================] - 0s 14ms/step - loss: 0.1145 - accuracy: 0.9665 - val_loss: 0.3222 - val_accuracy: 0.8733\nEpoch 8/20\n30/30 [==============================] - 0s 12ms/step - loss: 0.1014 - accuracy: 0.9711 - val_loss: 0.3076 - val_accuracy: 0.8839\nEpoch 9/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.0847 - accuracy: 0.9769 - val_loss: 0.3288 - val_accuracy: 0.8783\nEpoch 10/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.0718 - accuracy: 0.9827 - val_loss: 0.3700 - val_accuracy: 0.8687\nEpoch 11/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.0600 - accuracy: 0.9861 - val_loss: 0.5107 - val_accuracy: 0.8414\nEpoch 12/20\n30/30 [==============================] - 0s 14ms/step - loss: 0.0553 - accuracy: 0.9861 - val_loss: 0.3913 - val_accuracy: 0.8753\nEpoch 13/20\n30/30 [==============================] - 0s 14ms/step - loss: 0.0437 - accuracy: 0.9915 - val_loss: 0.4087 - val_accuracy: 0.8738\nEpoch 14/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.0380 - accuracy: 0.9947 - val_loss: 0.4465 - val_accuracy: 0.8713\nEpoch 15/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.0341 - accuracy: 0.9937 - val_loss: 0.4392 - val_accuracy: 0.8749\nEpoch 16/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.0260 - accuracy: 0.9963 - val_loss: 0.5053 - val_accuracy: 0.8675\nEpoch 17/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.0223 - accuracy: 0.9973 - val_loss: 0.4829 - val_accuracy: 0.8720\nEpoch 18/20\n30/30 [==============================] - 0s 14ms/step - loss: 0.0177 - accuracy: 0.9990 - val_loss: 0.5195 - val_accuracy: 0.8669\nEpoch 19/20\n30/30 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 0.9970 - val_loss: 0.5246 - val_accuracy: 0.8703\nEpoch 20/20\n30/30 [==============================] - 0s 12ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.5822 - val_accuracy: 0.8604\n\n\nThe call to model.fit() returns a history object. This object has a member history, which is a dictionary containing data about everything that happened during training. Let’s look at it:\n\nhistory_dict = history.history\nhistory_dict.keys()\n\ndict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#plotting-the-training-and-validation-loss",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#plotting-the-training-and-validation-loss",
    "title": "40  IMBD Movie Reviews",
    "section": "40.7 Plotting the Training and Validation Loss",
    "text": "40.7 Plotting the Training and Validation Loss\nLet’s use matplotlib to plot the training and validation loss side by side as well as the training and validation accuracy. Note that your own results may vary slightly due to a different random initialization of your model.\n\nimport matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()\nacc = history_dict[\"accuracy\"]\nval_acc = history_dict[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training acc\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, the training loss decreases with every epoch, and the training accuracy increases with every epoch. That’s what you would expect when running gradient- descent optimization — the quantity you’re trying to minimize should be less with every iteration. But that isn’t the case for the validation loss and accuracy: they seem to peak at the fourth epoch. What you’re seeing is overfitting: after the fourth epoch, you’re overoptimizing on the training data, and you end up learning representations that are specific to the training data and don’t generalize to data outside of the training set.\nIn this case, to prevent overfitting, you could stop training after four epochs.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#retraining-the-model-with-four-epochs",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#retraining-the-model-with-four-epochs",
    "title": "40  IMBD Movie Reviews",
    "section": "40.8 Retraining the Model with Four Epochs",
    "text": "40.8 Retraining the Model with Four Epochs\nLet’s train a new model from scratch for four epochs and then evaluate it on the test data.\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nresults = model.evaluate(x_test, y_test)\n\nEpoch 1/4\n\n\n2023-11-07 13:54:48.663915: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1000000000 exceeds 10% of free system memory.\n\n\n49/49 [==============================] - 1s 9ms/step - loss: 0.4965 - accuracy: 0.8072\nEpoch 2/4\n49/49 [==============================] - 0s 8ms/step - loss: 0.2924 - accuracy: 0.8979\nEpoch 3/4\n49/49 [==============================] - 0s 9ms/step - loss: 0.2256 - accuracy: 0.9191\nEpoch 4/4\n49/49 [==============================] - 0s 9ms/step - loss: 0.1928 - accuracy: 0.9305\n  1/782 [..............................] - ETA: 1:02 - loss: 0.3020 - accuracy: 0.8438\n\n\n2023-11-07 13:54:53.967576: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1000000000 exceeds 10% of free system memory.\n\n\n782/782 [==============================] - 1s 1ms/step - loss: 0.2792 - accuracy: 0.8873\n\n\nThe final results are below. This fairly naive approach achieves an accuracy of 88%. With state-of-the-art approaches, you should be able to get close to 95%.\n\nresults\n\n[0.27919963002204895, 0.8872799873352051]",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#generating-predictions-on-new-data",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#generating-predictions-on-new-data",
    "title": "40  IMBD Movie Reviews",
    "section": "40.9 Generating Predictions on New Data",
    "text": "40.9 Generating Predictions on New Data\nAfter having trained a model, you’ll want to use it on new data. You can generate the likelihood of reviews being positive by using the .predict() method.\n\nmodel.predict(x_test)\n\n2023-11-07 13:55:18.938415: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1000000000 exceeds 10% of free system memory.\n\n\n782/782 [==============================] - 1s 1ms/step\n\n\narray([[0.1812639 ],\n       [0.9998338 ],\n       [0.8731738 ],\n       ...,\n       [0.08835417],\n       [0.0980044 ],\n       [0.631039  ]], dtype=float32)",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36b_imdb_classification/imdb_classification.html#further-exploration",
    "href": "chapters/36b_imdb_classification/imdb_classification.html#further-exploration",
    "title": "40  IMBD Movie Reviews",
    "section": "40.10 Further Exploration",
    "text": "40.10 Further Exploration\nThe following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement:  - You used two representation layers before the final classification layer. Try using one or three representation layers, and see how doing so affects validation and test accuracy. - Try using layers with more units or fewer units: 32 units, 64 units, and so on. - Try using the mse loss function instead of binary_crossentropy. - Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>IMBD Movie Reviews</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html",
    "href": "chapters/36c_reuters_classification/reuters_classification.html",
    "title": "41  Reuters: Classifying Newswires",
    "section": "",
    "text": "41.1 Reading-In the Data\nIn this chapter, we’ll build a model to classify Reuters newswires into 46 mutually exclusive topics. Because we have many classes, this problem is an instance of multiclass classification, and because each data point should be classified into only one category, the problem is more specifically an instance of single-label multiclass classification. If each data point could belong to multiple categories (in this case, topics), we’d be facing a multilabel multiclass classification problem.\nYou’ll work with the Reuters dataset, a set of short newswires and their topics, published by Reuters in 1986. It’s a simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\nThe Reuters dataset comes packaged as part of Keras. Let’s take a look.\nfrom tensorflow.keras.datasets import reuters\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n    num_words=10000)\n\n2023-11-07 14:06:34.508338: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-07 14:06:34.549000: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-11-07 14:06:34.549027: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2023-11-07 14:06:34.549062: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-11-07 14:06:34.556803: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-07 14:06:34.557185: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-07 14:06:35.477200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nThere are 8,982 training examples and 2,246 test examples.\nlen(train_data)\n\n8982\nlen(test_data)\n\n2246\nEach example is a list of integers (word indices).\nprint(train_data[10])\n\n[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]\nHere’s how you can decode it back to words.\nword_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in\n    train_data[0]])\ndecoded_newswire\n\n'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'\nThe label associated with an example is an integer between 0 and 45—a topic index.\ntrain_labels[10]\n\n3",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#data-wrangling",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#data-wrangling",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.2 Data Wrangling",
    "text": "41.2 Data Wrangling\nThe data requires a bit of wrangling before it can be fed into a neural network. Let’s begin by vectorizing the newswires. Notice that we are flattening our sequential data, and hence removing any sense of ordering from it.\n\nimport numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        for j in sequence:\n            results[i, j] = 1.\n    return results\n\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\nTo vectorize the labels, there are two possibilities: you can cast the label list as an integer tensor, or you can use one-hot encoding. One-hot encoding is a widely used format for categorical data, also called categorical encoding. In this case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index. We will use one-hot encoding.\n\ndef to_one_hot(labels, dimension=46):\n    results = np.zeros((len(labels), dimension))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.\n    return results\ny_train = to_one_hot(train_labels)\ny_test = to_one_hot(test_labels)\n\nNote that there is a built-in convenience function in keras for doing this.\n\nfrom tensorflow.keras.utils import to_categorical\ny_train = to_categorical(train_labels)\ny_test = to_categorical(test_labels)\n\n\ny_train\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)\n\n\n\ny_test\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#building-the-model",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#building-the-model",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.3 Building the Model",
    "text": "41.3 Building the Model\nWe will use a dense feed forward network, using keras.Sequential, with two hidden layers each consisting of 64 units (a bit larger than we used for the IMBD dataset). The hidden layers will both have a relu activation function. The output layer will have 46 units, corresponding to the different possible topics. We will use a softmax activation function so that the output is probabilities.\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(46, activation=\"softmax\")\n])\n\nNext, let’s move on to the compilation step.\nThe best loss function to use in this case is categorical_crossentropy. It measures the distance between two probability distributions: here, between the probability distribution output by the model and the true distribution of the labels. By minimizing the distance between these two distributions, you train the model to output something as close as possible to the true labels.\n\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#creating-a-validation-set",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#creating-a-validation-set",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.4 Creating a Validation Set",
    "text": "41.4 Creating a Validation Set\nLet’s set apart 1,000 samples in the training data to use as a validation set.\n\nx_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = y_train[:1000]\npartial_y_train = y_train[1000:]",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#train-the-model",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#train-the-model",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.5 Train the Model",
    "text": "41.5 Train the Model\nNow, let’s train the model for 20 epochs.\n\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n\nEpoch 1/20\n16/16 [==============================] - 1s 31ms/step - loss: 2.7817 - accuracy: 0.4762 - val_loss: 1.8855 - val_accuracy: 0.6200\nEpoch 2/20\n16/16 [==============================] - 0s 18ms/step - loss: 1.5537 - accuracy: 0.6790 - val_loss: 1.3791 - val_accuracy: 0.6950\nEpoch 3/20\n16/16 [==============================] - 0s 18ms/step - loss: 1.1681 - accuracy: 0.7497 - val_loss: 1.2099 - val_accuracy: 0.7250\nEpoch 4/20\n16/16 [==============================] - 0s 19ms/step - loss: 0.9574 - accuracy: 0.7937 - val_loss: 1.0892 - val_accuracy: 0.7680\nEpoch 5/20\n16/16 [==============================] - 0s 19ms/step - loss: 0.7947 - accuracy: 0.8311 - val_loss: 1.0249 - val_accuracy: 0.7860\nEpoch 6/20\n16/16 [==============================] - 0s 19ms/step - loss: 0.6663 - accuracy: 0.8586 - val_loss: 0.9733 - val_accuracy: 0.7850\nEpoch 7/20\n16/16 [==============================] - 0s 18ms/step - loss: 0.5599 - accuracy: 0.8812 - val_loss: 0.9353 - val_accuracy: 0.8030\nEpoch 8/20\n16/16 [==============================] - 0s 18ms/step - loss: 0.4741 - accuracy: 0.8996 - val_loss: 0.9243 - val_accuracy: 0.8190\nEpoch 9/20\n16/16 [==============================] - 0s 18ms/step - loss: 0.4029 - accuracy: 0.9151 - val_loss: 0.8864 - val_accuracy: 0.8180\nEpoch 10/20\n16/16 [==============================] - 0s 18ms/step - loss: 0.3464 - accuracy: 0.9275 - val_loss: 0.9193 - val_accuracy: 0.8010\nEpoch 11/20\n16/16 [==============================] - 0s 17ms/step - loss: 0.3019 - accuracy: 0.9357 - val_loss: 0.8919 - val_accuracy: 0.8100\nEpoch 12/20\n16/16 [==============================] - 0s 17ms/step - loss: 0.2628 - accuracy: 0.9420 - val_loss: 0.9049 - val_accuracy: 0.8170\nEpoch 13/20\n16/16 [==============================] - 0s 19ms/step - loss: 0.2364 - accuracy: 0.9481 - val_loss: 0.9300 - val_accuracy: 0.8020\nEpoch 14/20\n16/16 [==============================] - 0s 18ms/step - loss: 0.2098 - accuracy: 0.9506 - val_loss: 0.9013 - val_accuracy: 0.8090\nEpoch 15/20\n16/16 [==============================] - 0s 18ms/step - loss: 0.1943 - accuracy: 0.9514 - val_loss: 0.9285 - val_accuracy: 0.8040\nEpoch 16/20\n16/16 [==============================] - 0s 20ms/step - loss: 0.1772 - accuracy: 0.9519 - val_loss: 0.8979 - val_accuracy: 0.8180\nEpoch 17/20\n16/16 [==============================] - 0s 17ms/step - loss: 0.1688 - accuracy: 0.9545 - val_loss: 0.9187 - val_accuracy: 0.8210\nEpoch 18/20\n16/16 [==============================] - 0s 17ms/step - loss: 0.1543 - accuracy: 0.9545 - val_loss: 0.9552 - val_accuracy: 0.8140\nEpoch 19/20\n16/16 [==============================] - 0s 17ms/step - loss: 0.1432 - accuracy: 0.9557 - val_loss: 0.9277 - val_accuracy: 0.8140\nEpoch 20/20\n16/16 [==============================] - 0s 16ms/step - loss: 0.1412 - accuracy: 0.9563 - val_loss: 0.9946 - val_accuracy: 0.8040",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#plotting-loss-and-accuracy",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#plotting-loss-and-accuracy",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.6 Plotting Loss and Accuracy",
    "text": "41.6 Plotting Loss and Accuracy\nNext, let’s plot the loss and accuracy curves.\n\nimport matplotlib.pyplot as plt\n\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#retraing-the-model-for-nine-epochs",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#retraing-the-model-for-nine-epochs",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.7 Retraing the Model for Nine Epochs",
    "text": "41.7 Retraing the Model for Nine Epochs\nAs we can see in the above two plots. The model begins to overfit after nine epochs. Let’s train a new model from scratch, on the entire training data set, for nine epochs and then evaluate it on the test set.\n\nmodel = keras.Sequential([\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(46, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train,\n          y_train,\n          epochs=9,\n          batch_size=512)\nresults = model.evaluate(x_test, y_test)\n\nEpoch 1/9\n18/18 [==============================] - 1s 15ms/step - loss: 2.6433 - accuracy: 0.5213\nEpoch 2/9\n18/18 [==============================] - 0s 15ms/step - loss: 1.4630 - accuracy: 0.6888\nEpoch 3/9\n18/18 [==============================] - 0s 17ms/step - loss: 1.1098 - accuracy: 0.7630\nEpoch 4/9\n18/18 [==============================] - 0s 15ms/step - loss: 0.8990 - accuracy: 0.8071\nEpoch 5/9\n18/18 [==============================] - 0s 15ms/step - loss: 0.7384 - accuracy: 0.8383\nEpoch 6/9\n18/18 [==============================] - 0s 16ms/step - loss: 0.6143 - accuracy: 0.8671\nEpoch 7/9\n18/18 [==============================] - 0s 14ms/step - loss: 0.5098 - accuracy: 0.8938\nEpoch 8/9\n18/18 [==============================] - 0s 16ms/step - loss: 0.4276 - accuracy: 0.9090\nEpoch 9/9\n18/18 [==============================] - 0s 17ms/step - loss: 0.3652 - accuracy: 0.9210\n71/71 [==============================] - 0s 2ms/step - loss: 0.9088 - accuracy: 0.7903\n\n\n\nresults\n\n[0.9087575674057007, 0.7902938723564148]\n\n\nThis approach reaches an accuracy of ~80%. With a balanced binary classification problem, the accuracy reached by a purely random classifier would be 50%. But in this case, we have 46 classes, and they may not be equally represented. What would be the accuracy of a random baseline? We could try quickly implementing one to check this empirically.\n\nimport copy\ntest_labels_copy = copy.copy(test_labels)\nnp.random.shuffle(test_labels_copy)\nhits_array = np.array(test_labels) == np.array(test_labels_copy)\nhits_array.mean()\n\n0.18744434550311664\n\n\nAs you can see, a random classifier would score around 19% classification accuracy, so the results of our model seem pretty good in that light.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#predictions-on-new-data",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#predictions-on-new-data",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.8 Predictions on New Data",
    "text": "41.8 Predictions on New Data\nCalling the model’s predict method on new samples returns a class probability distribution over all 46 topics for each sample. Let’s generate topic predictions for all of the test data.\n\npredictions = model.predict(x_test)\n\n71/71 [==============================] - 0s 2ms/step\n\n\nEach entry in predictions is a vector of length 46:\n\npredictions[0].shape\n\n(46,)\n\n\nThe coefficients in this vector sum to 1, as they form a probability distribution:\n\nnp.sum(predictions[0])\n\n0.9999999\n\n\nThe largest entry is the predicted class — the class with the highest probability:\n\nnp.argmax(predictions[0])\n\n3",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#information-bottleneck",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#information-bottleneck",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.9 Information Bottleneck",
    "text": "41.9 Information Bottleneck\nNow let’s see what happens when we introduce an information bottleneck by having intermediate layers that are significantly less than 46-dimensions: for example, 4-dimensional.\n\nmodel = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(4, activation=\"relu\"),\n    layers.Dense(46, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(partial_x_train,\n          partial_y_train,\n          epochs=20,\n          batch_size=128,\n          validation_data=(x_val, y_val))\n\nEpoch 1/20\n63/63 [==============================] - 1s 10ms/step - loss: 3.4442 - accuracy: 0.0804 - val_loss: 3.0439 - val_accuracy: 0.0920\nEpoch 2/20\n63/63 [==============================] - 1s 9ms/step - loss: 2.4024 - accuracy: 0.4327 - val_loss: 1.9069 - val_accuracy: 0.6190\nEpoch 3/20\n63/63 [==============================] - 1s 8ms/step - loss: 1.5456 - accuracy: 0.6437 - val_loss: 1.5021 - val_accuracy: 0.6490\nEpoch 4/20\n63/63 [==============================] - 1s 8ms/step - loss: 1.2814 - accuracy: 0.6862 - val_loss: 1.3921 - val_accuracy: 0.6800\nEpoch 5/20\n63/63 [==============================] - 1s 8ms/step - loss: 1.1347 - accuracy: 0.7256 - val_loss: 1.3320 - val_accuracy: 0.6900\nEpoch 6/20\n63/63 [==============================] - 1s 8ms/step - loss: 1.0300 - accuracy: 0.7399 - val_loss: 1.3126 - val_accuracy: 0.6910\nEpoch 7/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.9475 - accuracy: 0.7546 - val_loss: 1.3068 - val_accuracy: 0.6970\nEpoch 8/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.8783 - accuracy: 0.7697 - val_loss: 1.2986 - val_accuracy: 0.6980\nEpoch 9/20\n63/63 [==============================] - 1s 9ms/step - loss: 0.8156 - accuracy: 0.7866 - val_loss: 1.2911 - val_accuracy: 0.7000\nEpoch 10/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.7587 - accuracy: 0.8037 - val_loss: 1.3032 - val_accuracy: 0.7090\nEpoch 11/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.7107 - accuracy: 0.8192 - val_loss: 1.3131 - val_accuracy: 0.7050\nEpoch 12/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.6659 - accuracy: 0.8286 - val_loss: 1.3221 - val_accuracy: 0.7110\nEpoch 13/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.6246 - accuracy: 0.8378 - val_loss: 1.3872 - val_accuracy: 0.7110\nEpoch 14/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.5920 - accuracy: 0.8484 - val_loss: 1.4122 - val_accuracy: 0.7100\nEpoch 15/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.5579 - accuracy: 0.8571 - val_loss: 1.4068 - val_accuracy: 0.7070\nEpoch 16/20\n63/63 [==============================] - 1s 9ms/step - loss: 0.5304 - accuracy: 0.8624 - val_loss: 1.4524 - val_accuracy: 0.7180\nEpoch 17/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.5034 - accuracy: 0.8676 - val_loss: 1.4665 - val_accuracy: 0.7140\nEpoch 18/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.4798 - accuracy: 0.8697 - val_loss: 1.5153 - val_accuracy: 0.7180\nEpoch 19/20\n63/63 [==============================] - 1s 8ms/step - loss: 0.4617 - accuracy: 0.8771 - val_loss: 1.5403 - val_accuracy: 0.7240\nEpoch 20/20\n63/63 [==============================] - 0s 8ms/step - loss: 0.4392 - accuracy: 0.8836 - val_loss: 1.5682 - val_accuracy: 0.7180\n\n\n&lt;keras.src.callbacks.History&gt;\n\n\n\nresults = model.evaluate(x_test, y_test)\nresults\n\n71/71 [==============================] - 0s 2ms/step - loss: 1.7752 - accuracy: 0.6941\n\n\n[1.77518892288208, 0.6941229104995728]\n\n\nThe model now peaks at ~71% validation accuracy, an 8% absolute drop. This drop is mostly due to the fact that we’re trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The model is able to cram most of the necessary information into these four-dimensional representations, but not all of it.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36c_reuters_classification/reuters_classification.html#further-exploration",
    "href": "chapters/36c_reuters_classification/reuters_classification.html#further-exploration",
    "title": "41  Reuters: Classifying Newswires",
    "section": "41.10 Further Exploration",
    "text": "41.10 Further Exploration\nTry out the following experiments to train your intuition about the kind of configuration decisions you have to make with such models:\n\nTry using larger or smaller layers: 32 units, 128 units, and so on.\nYou used two intermediate layers before the final softmax classification layer. Now try using a single intermediate layer, or three intermediate layers.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Reuters: Classifying Newswires</span>"
    ]
  },
  {
    "objectID": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html",
    "href": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html",
    "title": "42  Boston Housing Prices",
    "section": "",
    "text": "42.1 Reading-In the Data\nIn this chapter, we’ll attempt to predict the median price of homes in a given Boston suburb in the mid-1970s, given data points about the suburb at the time, such as the crime rate, the local property tax rate, and so on. The dataset we’ll use has relatively few data points: only 506, split between 404 training samples and 102 test samples. Also, each feature in the input data (for example, the crime rate) has a different scale. For instance, some values are proportions, which take values between, others take values between 1 and 12, others between 0 and 100, and so on.\nThe Boston Housing data set is built into Keras.\nfrom tensorflow.keras.datasets import boston_housing\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n\n2023-11-07 14:22:12.649785: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-07 14:22:12.688270: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-11-07 14:22:12.688298: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2023-11-07 14:22:12.688330: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-11-07 14:22:12.695200: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-11-07 14:22:12.695469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-07 14:22:13.558710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nLet’s look at the data.\ntrain_data.shape\n\n(404, 13)\ntest_data.shape\n\n(102, 13)\nThe targets are the median values of owner-occupied homes, in thousands of dollars. Note that these prices are from the mid-1970s and have not been adjusted for inflation.\ntrain_targets\n\narray([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Boston Housing Prices</span>"
    ]
  },
  {
    "objectID": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#data-wrangling",
    "href": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#data-wrangling",
    "title": "42  Boston Housing Prices",
    "section": "42.2 Data Wrangling",
    "text": "42.2 Data Wrangling\nIt would be problematic to feed into a neural network values that all take wildly different ranges. The model might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice for dealing with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in numpy.\nNote that the quantities used for normalizing the test_data are computed using the train_data. You should never use any quantity computed on the test data in your workflow, even for something as simple as data normalization.\n\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\ntest_data -= mean\ntest_data /= std",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Boston Housing Prices</span>"
    ]
  },
  {
    "objectID": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#building-the-model",
    "href": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#building-the-model",
    "title": "42  Boston Housing Prices",
    "section": "42.3 Building the Model",
    "text": "42.3 Building the Model\nWe’ll use a small model with two intermediate layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using a small model is one way to mitigate overfitting. Since we’ll need to instantiate the same model multiple times, we’ll use a function to construct it. This is a common coding pattern you will see in deep learning models.\n\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\ndef build_model():\n    model = keras.Sequential([\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(1)\n    ])\n    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n    return model\n\nThe model ends with a single unit and no activation (it will be a linear layer). This is a typical setup for scalar regression (a regression where you’re trying to predict a single continuous value). Applying an activation function would constrain the range the output can take; for instance, if you applied a sigmoid activation function to the last layer, the model could only learn to predict values between 0 and 1. Here, because the last layer is purely linear, the model is free to learn to predict values in any range.\nNote that we compile the model with the mse loss function — mean squared error, the square of the difference between the predictions and the targets. This is a widely used loss function for regression problems.\nWe’re also monitoring a new metric during training: mean absolute error (MAE). It’s the absolute value of the difference between the predictions and the targets. For instance, an MAE of 0.5 on this problem would mean your predictions are off by $500 on average",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Boston Housing Prices</span>"
    ]
  },
  {
    "objectID": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#using-k-fold-cross-validation",
    "href": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#using-k-fold-cross-validation",
    "title": "42  Boston Housing Prices",
    "section": "42.4 Using K-Fold Cross-Validation",
    "text": "42.4 Using K-Fold Cross-Validation\nWe will use k-fold cross-validation to evaluate our model as we adjusts various fitting parameters. Notice we are doing the cross-validation by hand since we don’t have a cross_validate function like we do in sklearn.\n\nk = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print(f\"Processing fold #{i}\")\n    \n    # preparing the the data from the kth partition\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    # building and fitting the model\n    model = build_model()\n    model.fit(partial_train_data, partial_train_targets,\n              epochs=num_epochs, batch_size=16, verbose=0)\n    \n    # evaluating the model and storing the mae score\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)\n\nProcessing fold #0\nProcessing fold #1\nProcessing fold #2\nProcessing fold #3\n\n\n\nall_scores\n\n[1.8698797225952148, 2.4602911472320557, 2.446345567703247, 2.5106492042541504]\n\n\n\nnp.mean(all_scores)\n\n2.321791410446167\n\n\nLet’s try training the model a bit longer: 500 epochs. To keep a record of how well the model does at each epoch, we’ll modify the training loop to save the per-epoch validation score log for each fold.\n\nnum_epochs = 500\nall_mae_histories = []\nfor i in range(k):\n    print(f\"Processing fold #{i}\")\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    history = model.fit(partial_train_data, partial_train_targets,\n                        validation_data=(val_data, val_targets),\n                        epochs=num_epochs, batch_size=16, verbose=0)\n    mae_history = history.history[\"val_mae\"]\n    all_mae_histories.append(mae_history)\n\nProcessing fold #0\nProcessing fold #1\nProcessing fold #2\nProcessing fold #3\n\n\nWe can then compute the average of the per-epoch MAE scores for all folds.\n\naverage_mae_history = [\n    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)\n]",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Boston Housing Prices</span>"
    ]
  },
  {
    "objectID": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#plotting-the-validation-scores",
    "href": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#plotting-the-validation-scores",
    "title": "42  Boston Housing Prices",
    "section": "42.5 Plotting the Validation Scores",
    "text": "42.5 Plotting the Validation Scores\nLet’s plot the the k-fold MAE scores for all the epochs.\n\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()\n\n\n\n\n\n\n\n\nIt may be a little difficult to read the plot, due to a scaling issue: the validation MAE for the first few epochs is dramatically higher than the values that follow. Let’s omit the first 10 data points, which are on a different scale than the rest of the curve.\n\ntruncated_mae_history = average_mae_history[10:]\nplt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see validation MAE stops improving significantly after 120–140 epochs (this number includes the 10 epochs we omitted). Past that point,we start overfitting.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Boston Housing Prices</span>"
    ]
  },
  {
    "objectID": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#training-the-final-model",
    "href": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#training-the-final-model",
    "title": "42  Boston Housing Prices",
    "section": "42.6 Training the Final Model",
    "text": "42.6 Training the Final Model\nOnce you’re finished tuning other parameters of the model (in addition to the number of epochs, you could also adjust the size of the intermediate layers), you can train a final production model on all of the training data, with the best parameters, and then look at its performance on the test data.\n\nmodel = build_model()\nmodel.fit(train_data, train_targets,\n          epochs=130, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n\n4/4 [==============================] - 0s 2ms/step - loss: 15.9311 - mae: 2.5011\n\n\nHere’s the final result. Performance doesn’t seem to improve.\n\ntest_mae_score\n\n2.5011422634124756",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Boston Housing Prices</span>"
    ]
  },
  {
    "objectID": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#generating-predictions-on-new-data",
    "href": "chapters/36d_boston_house_price_regression/boston_house_price_regression.html#generating-predictions-on-new-data",
    "title": "42  Boston Housing Prices",
    "section": "42.7 Generating Predictions on New Data",
    "text": "42.7 Generating Predictions on New Data\nWith this scalar regression model, predict() returns the model’s guess for the sample’s price in thousands of dollars:\n\npredictions = model.predict(test_data)\npredictions[0]\n\n4/4 [==============================] - 0s 1ms/step\n\n\narray([8.320552], dtype=float32)",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Boston Housing Prices</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html",
    "title": "43  Student Loan: Neural Network",
    "section": "",
    "text": "43.1 Importing Packages\nIn this chapter we will use the keras package to predict student loan prepayments. In particular, we will use dense feed-forward neural networks.\nLet’s begin by importing some initial packages that we will need.\nimport pandas as pd\nimport numpy as np\nimport sklearn",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#reading-in-data",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#reading-in-data",
    "title": "43  Student Loan: Neural Network",
    "section": "43.2 Reading-In Data",
    "text": "43.2 Reading-In Data\nNow we can read-in our data.\n\ndf_train = pd.read_csv('../data/student_loan.csv')",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#feature-selection",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#feature-selection",
    "title": "43  Student Loan: Neural Network",
    "section": "43.3 Feature Selection",
    "text": "43.3 Feature Selection\nNext, let’s select our features and organize our lables. Notice that we are excluding cosign and repay_status because they are categorical variables.\n\nlst_features = \\\n    ['loan_age','income_annual', 'upb',              \n    'monthly_payment','fico','origbalance',\n    'mos_to_repay','mos_to_balln',]    \ndf_X = df_train[lst_features]\n\n\ndf_y = df_train['paid_label']",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#holdout-set",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#holdout-set",
    "title": "43  Student Loan: Neural Network",
    "section": "43.4 Holdout Set",
    "text": "43.4 Holdout Set\nWe will want to create a holdout set to measure out-of-sample performance. The following code uses train_test_split() to do that.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.20, random_state=0)",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#normalization",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#normalization",
    "title": "43  Student Loan: Neural Network",
    "section": "43.5 Normalization",
    "text": "43.5 Normalization\nNext, let’s perform a Gaussian normalization of our features so that they are all the same order of magnitude and have similar variability.\n\nmu = X_train.mean()\nstd = X_train.std()\n\nNotice that we are scaling both the training set and testing set with the mean and standard deviation of the training set. It is important not to normalize the test set with it’s own standard deviation to avoid information leek into the testing data.\n\nX_train_scaled = (X_train - mu) / std\nX_test_scaled = (X_test - mu) / std",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#setting-random-seeds",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#setting-random-seeds",
    "title": "43  Student Loan: Neural Network",
    "section": "43.6 Setting Random Seeds",
    "text": "43.6 Setting Random Seeds\nFitting neural networks involves a lot of random number generation. To ensure that we get reproducible results, let’s create a user-defined function that sets a variety of random number generators that get used. In order to do that we’ll need to import a couple of other packages.\n\nimport random\nimport tensorflow as tf\n\n2023-08-31 18:58:13.416320: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-08-31 18:58:13.455995: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-08-31 18:58:13.456758: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-08-31 18:58:14.227606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n\ndef set_seeds(seed=100):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#neural-network",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#neural-network",
    "title": "43  Student Loan: Neural Network",
    "section": "43.7 Neural Network",
    "text": "43.7 Neural Network\nWe can now fit our initial neural network. Let’s begin by importing some of the functions we will need from keras and sklearn.\n\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nNext, we’ll set the random seeds with our user-defined function.\n\nset_seeds()\n\nThe next step is to construct the model. We instantiate it with the Sequential() constructor and then add two hidden layers with 16 and 8 units.\n\nmodel = Sequential()\nmodel.add(Dense(units=16, input_dim=len(df_X.columns), activation='relu'))\nmodel.add(Dense(units=8, activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\nThe learning process is defined in the compilation step.\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nOnce the construction and compilation are complete we are ready for the actual learning/fitting to happen. (I played with the class weights until I got reasonable results.)\n\n%%time\nmodel.fit(X_train_scaled, y_train, epochs=10, verbose=True, batch_size=256, class_weight={0:1, 1:1.25});\n\nEpoch 1/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.0971 - accuracy: 0.9838\nEpoch 2/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0770 - accuracy: 0.9858\nEpoch 3/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0739 - accuracy: 0.9866\nEpoch 4/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0726 - accuracy: 0.9869\nEpoch 5/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0719 - accuracy: 0.9870\nEpoch 6/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0714 - accuracy: 0.9871\nEpoch 7/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0711 - accuracy: 0.9872\nEpoch 8/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0707 - accuracy: 0.9872\nEpoch 9/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0704 - accuracy: 0.9873\nEpoch 10/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0701 - accuracy: 0.9873\nCPU times: user 1min 2s, sys: 3.79 s, total: 1min 5s\nWall time: 43.7 s\n\n\nWe can use the .evaluate() method of the model to check the out-of-sample accuracy.\n\nmodel.evaluate(X_test_scaled, y_test)\n\n6521/6521 [==============================] - 6s 857us/step - loss: 0.0586 - accuracy: 0.9874\n\n\n[0.0585552453994751, 0.9873527884483337]\n\n\nNext, let’s check the out-of-sample precision, recall, and f1 score.\n\ntest_predictions = np.where(model.predict(X_test_scaled) &gt; 0.5, 1, 0)\nprint(\"F1:       \", f1_score(y_test, test_predictions))\nprint(\"Precision:\", precision_score(y_test, test_predictions))\nprint(\"Recall:   \", recall_score(y_test, test_predictions))\n\n6521/6521 [==============================] - 4s 618us/step\nF1:        0.37920489296636084\nPrecision: 0.9361207897793263\nRecall:    0.2377581120943953\n\n\nWe can check the ratio of predicted number of prepayments to the actual number of prepayments. As you can see, the model only predicts 25% the number of actual prepayments.\n\nprint(test_predictions.sum() / y_test.sum())\n\n0.25398230088495577\n\n\nFinally, let’s check the expected balance ratio: it is about 118%.\n\nnp.sum(X_test['upb'] * np.ravel(model.predict(X_test_scaled))) / np.sum(X_test['upb'] * y_test)\n\n6521/6521 [==============================] - 4s 652us/step\n\n\n1.1768956596208677",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#dropout",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#dropout",
    "title": "43  Student Loan: Neural Network",
    "section": "43.8 Dropout",
    "text": "43.8 Dropout\nIn this section we implement drop-out regularization. We begin by first resetting the random seeds.\n\nset_seeds()\n\nNotice that in the construction of our network, we add a Dropout layer after each hidden layer.\n\nfrom keras.layers import Dropout\nmodel = Sequential()\nmodel.add(Dense(units=16, input_dim=len(df_X.columns), activation='relu'))\nmodel.add(Dropout(rate=0.3, seed=0))\nmodel.add(Dense(units=8, activation='relu'))\nmodel.add(Dropout(rate=0.3, seed=0))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nNext, we fit the network.\n\n%%time\nmodel.fit(X_train_scaled, y_train, epochs=10, verbose=True, batch_size=256, class_weight={0:1, 1:1.25});\n\nEpoch 1/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.1176 - accuracy: 0.9826\nEpoch 2/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.0916 - accuracy: 0.9839\nEpoch 3/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.0851 - accuracy: 0.9848\nEpoch 4/10\n3261/3261 [==============================] - 5s 2ms/step - loss: 0.0823 - accuracy: 0.9853\nEpoch 5/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.0810 - accuracy: 0.9855\nEpoch 6/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.0804 - accuracy: 0.9856\nEpoch 7/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.0802 - accuracy: 0.9856\nEpoch 8/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.0798 - accuracy: 0.9856\nEpoch 9/10\n3261/3261 [==============================] - 5s 2ms/step - loss: 0.0793 - accuracy: 0.9858\nEpoch 10/10\n3261/3261 [==============================] - 5s 2ms/step - loss: 0.0790 - accuracy: 0.9859\nCPU times: user 1min 11s, sys: 4.42 s, total: 1min 16s\nWall time: 49.6 s\n\n\nNow we can check the out-of-sample measures of fit. Dropout regularization doesn’t seem to improve these metrics - in fact, the F1 score decreases.\n\ntest_predictions = np.where(model.predict(X_test_scaled) &gt; 0.5, 1, 0)\nprint(\"F1:       \", f1_score(y_test, test_predictions))\nprint(\"Precision:\", precision_score(y_test, test_predictions))\nprint(\"Recall:   \", recall_score(y_test, test_predictions))\n\n6521/6521 [==============================] - 4s 680us/step\nF1:        0.31407407407407406\nPrecision: 0.9636363636363636\nRecall:    0.18761061946902655\n\n\nThere is also a further reduction in the absolute number of prepayments that are predicted\n\ntest_predictions = np.where(model.predict(X_test_scaled) &gt; 0.5, 1, 0)\ntest_predictions.sum() / y_test.sum()\n\n6521/6521 [==============================] - 5s 690us/step\n\n\n0.19469026548672566\n\n\nThe expected loan balance ratio increases to 130%.\n\nnp.sum(X_test['upb'] * np.ravel(model.predict(X_test_scaled))) / np.sum(X_test['upb'] * y_test)\n\n6521/6521 [==============================] - 5s 692us/step\n\n\n1.302716041226891\n\n\nAll in all, dropout regularization doesn’t seem to improve model performance.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#regularization",
    "href": "chapters/37_student_loan_neural_network/student_loan_neural_network.html#regularization",
    "title": "43  Student Loan: Neural Network",
    "section": "43.9 Regularization",
    "text": "43.9 Regularization\nIn this section we implement ridge (l2) regularization. (I tried lasso regularization and it was an epic fail).\n\nset_seeds()\n\nWe implement l2 regularization by populating the activity_regularizer argument of the Dense() layer constructor.\n\nfrom keras.regularizers import l2\nmodel = Sequential()\nmodel.add(Dense(units=16, input_dim=len(df_X.columns), activation='relu', activity_regularizer=l2(0.0005)))\nmodel.add(Dense(units=8, activation='relu', activity_regularizer=l2(0.0005)))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nNext, we fit the model.\n\n%%time\nmodel.fit(X_train_scaled, y_train, epochs=10, verbose=True, batch_size=256, class_weight={0:1, 1:1.25});\n\nEpoch 1/10\n3261/3261 [==============================] - 5s 1ms/step - loss: 0.1009 - accuracy: 0.9839\nEpoch 2/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0783 - accuracy: 0.9862\nEpoch 3/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0745 - accuracy: 0.9869\nEpoch 4/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0725 - accuracy: 0.9871\nEpoch 5/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0712 - accuracy: 0.9872\nEpoch 6/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0703 - accuracy: 0.9874\nEpoch 7/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0697 - accuracy: 0.9876\nEpoch 8/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0693 - accuracy: 0.9877\nEpoch 9/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0690 - accuracy: 0.9878\nEpoch 10/10\n3261/3261 [==============================] - 4s 1ms/step - loss: 0.0687 - accuracy: 0.9879\nCPU times: user 1min 6s, sys: 4.6 s, total: 1min 10s\nWall time: 44.3 s\n\n\nLet’s check the out-of-sample metrics. There is a marginal improvement in F1 score.\n\ntest_predictions = np.where(model.predict(X_test_scaled) &gt; 0.5, 1, 0)\nprint(\"F1:       \", f1_score(y_test, test_predictions))\nprint(\"Precision:\", precision_score(y_test, test_predictions))\nprint(\"Recall:   \", recall_score(y_test, test_predictions))\n\n6521/6521 [==============================] - 5s 683us/step\nF1:        0.43484102104791755\nPrecision: 0.9024163568773235\nRecall:    0.28643067846607667\n\n\nThere is also a marginal improvement in the absolute number of prepayments predicted.\n\ntest_predictions = np.where(model.predict(X_test_scaled) &gt; 0.5, 1, 0)\ntest_predictions.sum() / y_test.sum()\n\n6521/6521 [==============================] - 4s 655us/step\n\n\n0.31740412979351035\n\n\nHowever, the expected loan balance ratio is worse: it increases to 121%.\n\nnp.sum(X_test['upb'] * np.ravel(model.predict(X_test_scaled))) / np.sum(X_test['upb'] * y_test)\n\n6521/6521 [==============================] - 4s 614us/step\n\n\n1.2114536969706748\n\n\nAll in all, l2 regularization doesn’t seem to help.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Student Loan: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html",
    "href": "chapters/38_exchange_rate/exchange_rate.html",
    "title": "44  Exchange Rate Prediction",
    "section": "",
    "text": "44.1 Import Packages\nIn this tutorial we use dense neural networks to predict directional change in the EUR-USD exchange rate during 2019. This is a classification problem.\nThis code is taken from Chpater 7 of Artificial Intelligence in Finance by Yves Hilpisch.\nLet’s begin by importing the packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8')\nnp.set_printoptions(suppress=True, precision=4)\n%matplotlib inline",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#read-in-and-wrangle-data",
    "href": "chapters/38_exchange_rate/exchange_rate.html#read-in-and-wrangle-data",
    "title": "44  Exchange Rate Prediction",
    "section": "44.2 Read-In and Wrangle Data",
    "text": "44.2 Read-In and Wrangle Data\nNext, let’s read and wrangle the data.\n\nraw = pd.read_csv('../data/eur_usd.csv')\nraw.set_index('Date', inplace = True)\nraw.index = pd.to_datetime(raw.index)\nraw.head()\n\n\n\n\n\n\n\n\nHIGH\nLOW\nOPEN\nCLOSE\n\n\nDate\n\n\n\n\n\n\n\n\n2019-10-01 00:00:00\n1.0899\n1.0897\n1.0897\n1.0899\n\n\n2019-10-01 00:01:00\n1.0899\n1.0896\n1.0899\n1.0898\n\n\n2019-10-01 00:02:00\n1.0898\n1.0896\n1.0898\n1.0896\n\n\n2019-10-01 00:03:00\n1.0898\n1.0896\n1.0897\n1.0898\n\n\n2019-10-01 00:04:00\n1.0898\n1.0896\n1.0897\n1.0898\n\n\n\n\n\n\n\nThe data consist of open, high, low, close data. All we need are the close prices, so let’s grab those.\n\ndata = pd.DataFrame(raw['CLOSE'].loc[:])\ndata.columns = ['EUR_USD']\ndata.head()\n\n\n\n\n\n\n\n\nEUR_USD\n\n\nDate\n\n\n\n\n\n2019-10-01 00:00:00\n1.0899\n\n\n2019-10-01 00:01:00\n1.0898\n\n\n2019-10-01 00:02:00\n1.0896\n\n\n2019-10-01 00:03:00\n1.0898\n\n\n2019-10-01 00:04:00\n1.0898\n\n\n\n\n\n\n\nThe original data is one minute snap shots. We’ll resample this every hour.\n\ndata = data.resample('1h', label='right').last().ffill()\ndata.head()\n\n\n\n\n\n\n\n\nEUR_USD\n\n\nDate\n\n\n\n\n\n2019-10-01 01:00:00\n1.0896\n\n\n2019-10-01 02:00:00\n1.0890\n\n\n2019-10-01 03:00:00\n1.0886\n\n\n2019-10-01 04:00:00\n1.0888\n\n\n2019-10-01 05:00:00\n1.0889\n\n\n\n\n\n\n\nLet’s take a look at the EUR-USD exchange rate over time.\n\ndata.plot(figsize=(10, 6));",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#adding-features",
    "href": "chapters/38_exchange_rate/exchange_rate.html#adding-features",
    "title": "44  Exchange Rate Prediction",
    "section": "44.3 Adding Features",
    "text": "44.3 Adding Features\nWe’ll add a variety of features that are all functions of the price history. We’ll do this via a user-defined function.\n\nsymbol = 'EUR_USD'\nlags = 5\n\n\ndef add_lags(data, symbol, lags, window=20):\n    cols = []\n    df = data.copy()\n    df.dropna(inplace=True) # return\n    df['r'] = np.log(df / df.shift())\n    df['sma'] = df[symbol].rolling(window).mean()\n    df['min'] = df[symbol].rolling(window).min()\n    df['max'] = df[symbol].rolling(window).max()\n    df['mom'] = df['r'].rolling(window).mean()\n    df['vol'] = df['r'].rolling(window).std()\n    df.dropna(inplace=True)\n    df['d'] = np.where(df['r'] &gt; 0, 1, 0) # directional move\n    features = [symbol, 'r', 'd', 'sma', 'min', 'max', 'mom', 'vol']\n    for f in features: # creating lagged values\n        for lag in range(1, lags + 1):\n            col = f'{f}_lag_{lag}'\n            df[col] = df[f].shift(lag)\n            cols.append(col)\n    df.dropna(inplace=True)\n    return df, cols\n\n\ndata, cols = add_lags(data, symbol, lags)\n\nLet’s take a look at all the features that we have created.\n\ncols\n\n['EUR_USD_lag_1',\n 'EUR_USD_lag_2',\n 'EUR_USD_lag_3',\n 'EUR_USD_lag_4',\n 'EUR_USD_lag_5',\n 'r_lag_1',\n 'r_lag_2',\n 'r_lag_3',\n 'r_lag_4',\n 'r_lag_5',\n 'd_lag_1',\n 'd_lag_2',\n 'd_lag_3',\n 'd_lag_4',\n 'd_lag_5',\n 'sma_lag_1',\n 'sma_lag_2',\n 'sma_lag_3',\n 'sma_lag_4',\n 'sma_lag_5',\n 'min_lag_1',\n 'min_lag_2',\n 'min_lag_3',\n 'min_lag_4',\n 'min_lag_5',\n 'max_lag_1',\n 'max_lag_2',\n 'max_lag_3',\n 'max_lag_4',\n 'max_lag_5',\n 'mom_lag_1',\n 'mom_lag_2',\n 'mom_lag_3',\n 'mom_lag_4',\n 'mom_lag_5',\n 'vol_lag_1',\n 'vol_lag_2',\n 'vol_lag_3',\n 'vol_lag_4',\n 'vol_lag_5']\n\n\nHere is what our dataset now looks like.\n\ndata.round(4).head()\n\n\n\n\n\n\n\n\nEUR_USD\nr\nsma\nmin\nmax\nmom\nvol\nd\nEUR_USD_lag_1\nEUR_USD_lag_2\n...\nmom_lag_1\nmom_lag_2\nmom_lag_3\nmom_lag_4\nmom_lag_5\nvol_lag_1\nvol_lag_2\nvol_lag_3\nvol_lag_4\nvol_lag_5\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-10-02 02:00:00\n1.0937\n-0.0001\n1.0916\n1.0879\n1.0938\n0.0002\n0.0008\n0\n1.0938\n1.0932\n...\n0.0002\n0.0002\n0.0002\n0.0002\n0.0002\n0.0008\n0.0008\n0.0008\n0.0008\n0.0008\n\n\n2019-10-02 03:00:00\n1.0937\n0.0000\n1.0918\n1.0885\n1.0938\n0.0003\n0.0008\n0\n1.0937\n1.0938\n...\n0.0002\n0.0002\n0.0002\n0.0002\n0.0002\n0.0008\n0.0008\n0.0008\n0.0008\n0.0008\n\n\n2019-10-02 04:00:00\n1.0936\n-0.0001\n1.0921\n1.0886\n1.0938\n0.0002\n0.0008\n0\n1.0937\n1.0937\n...\n0.0003\n0.0002\n0.0002\n0.0002\n0.0002\n0.0008\n0.0008\n0.0008\n0.0008\n0.0008\n\n\n2019-10-02 05:00:00\n1.0933\n-0.0003\n1.0923\n1.0886\n1.0938\n0.0002\n0.0008\n0\n1.0936\n1.0937\n...\n0.0002\n0.0003\n0.0002\n0.0002\n0.0002\n0.0008\n0.0008\n0.0008\n0.0008\n0.0008\n\n\n2019-10-02 06:00:00\n1.0935\n0.0002\n1.0925\n1.0886\n1.0938\n0.0002\n0.0008\n1\n1.0933\n1.0936\n...\n0.0002\n0.0002\n0.0003\n0.0002\n0.0002\n0.0008\n0.0008\n0.0008\n0.0008\n0.0008\n\n\n\n\n5 rows × 48 columns\n\n\n\nLet’s look at just a few rows and columns of the dataset.\n\ndata.iloc[:5, :14].round(4)\n\n\n\n\n\n\n\n\nEUR_USD\nr\nsma\nmin\nmax\nmom\nvol\nd\nEUR_USD_lag_1\nEUR_USD_lag_2\nEUR_USD_lag_3\nEUR_USD_lag_4\nEUR_USD_lag_5\nr_lag_1\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-10-02 02:00:00\n1.0937\n-0.0001\n1.0916\n1.0879\n1.0938\n0.0002\n0.0008\n0\n1.0938\n1.0932\n1.0931\n1.0931\n1.0931\n0.0005\n\n\n2019-10-02 03:00:00\n1.0937\n0.0000\n1.0918\n1.0885\n1.0938\n0.0003\n0.0008\n0\n1.0937\n1.0938\n1.0932\n1.0931\n1.0931\n-0.0001\n\n\n2019-10-02 04:00:00\n1.0936\n-0.0001\n1.0921\n1.0886\n1.0938\n0.0002\n0.0008\n0\n1.0937\n1.0937\n1.0938\n1.0932\n1.0931\n0.0000\n\n\n2019-10-02 05:00:00\n1.0933\n-0.0003\n1.0923\n1.0886\n1.0938\n0.0002\n0.0008\n0\n1.0936\n1.0937\n1.0937\n1.0938\n1.0932\n-0.0001\n\n\n2019-10-02 06:00:00\n1.0935\n0.0002\n1.0925\n1.0886\n1.0938\n0.0002\n0.0008\n1\n1.0933\n1.0936\n1.0937\n1.0937\n1.0938\n-0.0003",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#dealing-with-class-imbalance",
    "href": "chapters/38_exchange_rate/exchange_rate.html#dealing-with-class-imbalance",
    "title": "44  Exchange Rate Prediction",
    "section": "44.4 Dealing with Class Imbalance",
    "text": "44.4 Dealing with Class Imbalance\nClassification with neural networks works best with evenly distributed classes. The .fit() method of networks built with keras has a class_weight argument that can adjust for imbalanced classes.\nIn this section we calculate the dict of class weights that will be given in the class_weight argument below.\n\nc = data['d'].value_counts()\nc\n\nd\n0    1445\n1     738\nName: count, dtype: int64\n\n\n\nnp.bincount(data['d'])\n\narray([1445,  738])\n\n\n\ndef cw(df):\n    c0, c1 = np.bincount(df['d'])\n    w0 = (1 / c0) * (len(df)) / 2\n    w1 = (1 / c1) * (len(df)) / 2\n    return {0: w0, 1: w1}\n\n\nclass_weight = cw(data)\nclass_weight\n\n{0: 0.755363321799308, 1: 1.4789972899728998}\n\n\n\nprint(class_weight[0] * c[0])\nprint(class_weight[1] * c[1])\n\n1091.5\n1091.5",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#creating-the-neural-network",
    "href": "chapters/38_exchange_rate/exchange_rate.html#creating-the-neural-network",
    "title": "44  Exchange Rate Prediction",
    "section": "44.5 Creating the Neural Network",
    "text": "44.5 Creating the Neural Network\nIn this section we proceed to build the neural network.\n\nimport random\nimport logging\nimport tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n#from keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.legacy import Adam\nfrom sklearn.metrics import accuracy_score\ntf.get_logger().setLevel(logging.ERROR)\n\n2023-09-02 13:56:54.221015: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-02 13:56:54.426295: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-02 13:56:54.427533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-09-02 13:56:55.407293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\nThis function sets all the relevant the random seeds so we always get the same values.\n\ndef set_seeds(seed=100):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nWe will use the Adam optimizer.\n\noptimizer = Adam(learning_rate=0.001)\n\nThe following user defined function creates a DNN model with hl number of hidden layers. Each layer has hu hidden units.\n\ndef create_model(hl=1, hu=128, optimizer=optimizer):\n    model = Sequential()\n    model.add(Dense(hu, input_dim=len(cols), activation='relu'))\n    for _ in range(hl):\n        model.add(Dense(hu, activation='relu'))\n    model.add(Dense(1, activation='sigmoid')) \n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\nNext, we use the function above to create the neural network.\n\nset_seeds()\nmodel = create_model(hl=1, hu=128)\n\nNow we are ready to fit the model. Notice that we are using 50 epochs, and that we are setting the class_weight with the class-weight dict that was created by the function defined above.\n\n%%time\nmodel.fit(data[cols], data['d'], epochs=50, verbose=False, class_weight=cw(data));\n\nCPU times: user 6.19 s, sys: 427 ms, total: 6.62 s\nWall time: 4.32 s\n\n\nLet’s check how the model fit the training data (in-sample). As you can see we have an accuracy of about 60%.\n\nmodel.evaluate(data[cols], data['d'])\n\n69/69 [==============================] - 0s 928us/step - loss: 0.6017 - accuracy: 0.5754\n\n\n[0.6016581058502197, 0.5753549933433533]\n\n\nWe can also check the number of gains and losses predicted. These numbers are close to the training data.\n\ndata['p'] = np.where(model.predict(data[cols]) &gt; 0.5, 1, 0)\ndata['p'].value_counts()\n\n69/69 [==============================] - 0s 856us/step\n\n\np\n1    1585\n0     598\nName: count, dtype: int64",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#train-test-split",
    "href": "chapters/38_exchange_rate/exchange_rate.html#train-test-split",
    "title": "44  Exchange Rate Prediction",
    "section": "44.6 Train Test Split",
    "text": "44.6 Train Test Split\nIn this section, we create a holdout test set with 20% of the data, leaving 80% for training. This will give us a sense for how the model performs out-of-sample.\n\nsplit = int(len(data) * 0.8)\ntrain = data.iloc[:split].copy()\ntest = data.iloc[split:].copy()\n\nWe set the random seeds and create a model with one hidden layer consisting of 128 units.\n\nset_seeds()\nmodel = create_model(hl=1, hu=128)\n\nNow we can fit our model. Notice that 20% of the training data is being used as a validation set upon which the training metrics are evaluated after each epoch. The model is not fit on the validation set.\n\n%%time \nh = model.fit(train[cols], train['d'], epochs=50, verbose=False, validation_split=0.2, shuffle=False, class_weight=cw(train));\n#h = model.fit(train[cols], train['d'], epochs=50, verbose=False, validation_split=0.2, class_weight=cw(train));\n\nCPU times: user 5.38 s, sys: 485 ms, total: 5.87 s\nWall time: 4.1 s\n\n\nLet’s see how our model performed in-sample.\n\nmodel.evaluate(train[cols], train['d'])\n\n55/55 [==============================] - 0s 1ms/step - loss: 0.6144 - accuracy: 0.5934\n\n\n[0.6143843531608582, 0.5933562517166138]\n\n\nLet’s see how our model performed out-of-sample.\n\nmodel.evaluate(test[cols], test['d'])\n\n14/14 [==============================] - 0s 1ms/step - loss: 0.5977 - accuracy: 0.6133\n\n\n[0.5977151393890381, 0.6132723093032837]\n\n\nWe can also view how many gains and losses were predicted on the test set.\n\ntest['p'] = np.where(model.predict(test[cols]) &gt; 0.5, 1, 0)\ntest['p'].value_counts()\n\n14/14 [==============================] - 0s 940us/step\n\n\np\n1    284\n0    153\nName: count, dtype: int64\n\n\nLet’s plot our model accuracy and validation accuracy as follows.\n\nres = pd.DataFrame(h.history)\nres[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#normalization",
    "href": "chapters/38_exchange_rate/exchange_rate.html#normalization",
    "title": "44  Exchange Rate Prediction",
    "section": "44.7 Normalization",
    "text": "44.7 Normalization\nNormalizing training and test data is often useful when fitting a neural network.\n\nmu, std = train.mean(), train.std()\ntrain_ = (train - mu) / std\n\nLet create a model with 2 hidden layers, each consisting of 128 hidden units.\n\nset_seeds()\nmodel = create_model(hl=2, hu=128)\n\nWe can now fit our model.\n\n%%time \nh = model.fit(train_[cols], train['d'], epochs=50, verbose=False, validation_split=0.2, shuffle=False, class_weight=cw(train));\n\nCPU times: user 6.1 s, sys: 584 ms, total: 6.68 s\nWall time: 4.34 s\n\n\nLet’s evaluate it in-sample. As we can see there is a big jump in the in-sample accuracy.\n\nmodel.evaluate(train_[cols], train['d'])\n\n55/55 [==============================] - 0s 998us/step - loss: 0.4099 - accuracy: 0.9124\n\n\n[0.40989920496940613, 0.9123711585998535]\n\n\nNext, we normalize the test set. Notice that we are using the mu and std from the training set. It is important to not use the testing set to normalize itself because this bleeds data from the training set into the testing process.\n\ntest_ = (test - mu) / std\n\nLet’s check the out-of-sample accuracy of the model.\n\nmodel.evaluate(test_[cols], test['d'])\n\n14/14 [==============================] - 0s 1ms/step - loss: 1.7293 - accuracy: 0.6430\n\n\n[1.7292619943618774, 0.6430205702781677]\n\n\nWe can also check how many gains and losses were predicted on the test set\n\ntest['p'] = np.where(model.predict(test_[cols]) &gt; 0.5, 1, 0)\ntest['p'].value_counts()\n\n14/14 [==============================] - 0s 922us/step\n\n\np\n0    268\n1    169\nName: count, dtype: int64\n\n\nFinally, let’s graph the training and validation accuracies.\n\nres = pd.DataFrame(h.history)\nres[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#dropout",
    "href": "chapters/38_exchange_rate/exchange_rate.html#dropout",
    "title": "44  Exchange Rate Prediction",
    "section": "44.8 Dropout",
    "text": "44.8 Dropout\nIn this section we implement drop-out regularization.\n\nfrom keras.layers import Dropout\n\nWe modify the user-defined function to allow for dropout regularization. Notice that a Dropout layer is added after the input layer and each hidden layer.\n\ndef create_model(hl=1, hu=128, dropout=True, rate=0.3, optimizer=optimizer):\n    model = Sequential()\n    model.add(Dense(hu, input_dim=len(cols), activation='relu'))\n    if dropout:\n        model.add(Dropout(rate, seed=100))\n    for _ in range(hl):\n        model.add(Dense(hu, activation='relu'))\n        if dropout:\n            model.add(Dropout(rate, seed=100))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\nLet’s instantiate the model with a single hidden layer, and a drop-out rate of 30%.\n\nset_seeds()\nmodel = create_model(hl=1, hu=128, rate=0.3)\n\nNow we can fit the model.\n\n%%time \nh = model.fit(train_[cols], train['d'], epochs=50, verbose=False, validation_split=0.15, shuffle=False, class_weight=cw(train));\n\nCPU times: user 5.97 s, sys: 453 ms, total: 6.43 s\nWall time: 4.48 s\n\n\nLet’s check the in-sample accuracy.\n\nmodel.evaluate(train_[cols], train['d'])\n\n55/55 [==============================] - 0s 976us/step - loss: 0.4345 - accuracy: 0.7984\n\n\n[0.43445172905921936, 0.7983963489532471]\n\n\nLet’s also check the out-of-sample accuracy.\n\nmodel.evaluate(test_[cols], test['d'])\n\n14/14 [==============================] - 0s 1ms/step - loss: 0.5777 - accuracy: 0.6705\n\n\n[0.5777181386947632, 0.6704805493354797]\n\n\nFinally, we graph the training and validation accuracy.\n\nres = pd.DataFrame(h.history)\nres[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#regularization",
    "href": "chapters/38_exchange_rate/exchange_rate.html#regularization",
    "title": "44  Exchange Rate Prediction",
    "section": "44.9 Regularization",
    "text": "44.9 Regularization\nIn this section we implement both Lasso and Ridge regularization.\n\n44.9.1 l1 Regularization\nLet’s begin with the l1 norm, i.e. Lasso regularization.\n\nfrom keras.regularizers import l1\n\nWe modify the create_model() function to accommodate regularization. Notice that this is done when adding a layer by populating the activity_regularizer argument.\n\ndef create_model(hl=1, hu=128, dropout=False, rate=0.3, regularize=False, reg=l1(0.0005), optimizer=optimizer, input_dim=len(cols)):\n    if not regularize:\n        reg = None\n    model = Sequential()\n    model.add(Dense(hu, input_dim=input_dim, activity_regularizer=reg, activation='relu'))\n    if dropout:\n        model.add(Dropout(rate, seed=100))\n    for _ in range(hl):\n        model.add(Dense(hu, activation='relu', activity_regularizer=reg))\n        if dropout:\n            model.add(Dropout(rate, seed=100))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\nLet’s instantiate the network with a single hidden layer consisting of 128 hidden units.\n\nset_seeds()\nmodel = create_model(hl=1, hu=128, regularize=True)\n\nNext we fit the model.\n\n%%time \nh = model.fit(train_[cols], train['d'], epochs=50, verbose=False, validation_split=0.2, shuffle=False, class_weight=cw(train));\n\nCPU times: user 5.75 s, sys: 533 ms, total: 6.28 s\nWall time: 4.19 s\n\n\nLet’s check in-sample accuracy.\n\nmodel.evaluate(train_[cols], train['d'])\n\n55/55 [==============================] - 0s 1ms/step - loss: 0.4188 - accuracy: 0.8608\n\n\n[0.41878587007522583, 0.8608247637748718]\n\n\nLet’s check out-of-sample accuracy.\n\nmodel.evaluate(test_[cols], test['d'])\n\n14/14 [==============================] - 0s 1ms/step - loss: 1.0168 - accuracy: 0.6384\n\n\n[1.0167690515518188, 0.6384439468383789]\n\n\nFinally, we graph the training and validation accuracies.\n\nres = pd.DataFrame(h.history)\nres[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');\n\n\n\n\n\n\n\n\n\n\n44.9.2 l2 Regularization\nNext let’s try the l2 norm, i.e Ridge regularization.\n\nfrom keras.regularizers import l2\n\nWe start by instantiating the model. This time we use two hidden layers, each consisting of 128 hidden units.\n\nset_seeds()\nh = model = create_model(hl=2, hu=128, dropout=True, rate=0.3, regularize=True, reg=l2(0.001))\n\nNext, we fit the model.\n\n%%time \nh = model.fit(train_[cols], train['d'], epochs=50, verbose=False, validation_split=0.2, shuffle=False, class_weight=cw(train));\n\nCPU times: user 7.16 s, sys: 593 ms, total: 7.75 s\nWall time: 4.94 s\n\n\nLet’s check the training accuracy.\n\nmodel.evaluate(train_[cols], train['d'])\n\n55/55 [==============================] - 0s 1ms/step - loss: 0.4517 - accuracy: 0.7852\n\n\n[0.45165252685546875, 0.7852233648300171]\n\n\nLet’s check the test accuracy.\n\nmodel.evaluate(test_[cols], test['d'])\n\n14/14 [==============================] - 0s 1ms/step - loss: 0.5871 - accuracy: 0.6270\n\n\n[0.5870832800865173, 0.6270022988319397]\n\n\nNext, we graph the training accuracy and validation accuracy.\n\nres = pd.DataFrame(h.history)\nres[['accuracy', 'val_accuracy']].plot(figsize=(10, 6), style='--');\n\n\n\n\n\n\n\n\nLet’s check the average difference in training and validation accuracy.\n\nres.mean()['accuracy'] - res.mean()['val_accuracy']\n\n0.0735767555236817",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/38_exchange_rate/exchange_rate.html#optimizers",
    "href": "chapters/38_exchange_rate/exchange_rate.html#optimizers",
    "title": "44  Exchange Rate Prediction",
    "section": "44.10 Optimizers",
    "text": "44.10 Optimizers\nIn this section we check the in-sample and out-of-sample accuracy for a variety of different optimizers.\n\nimport time\n\n\noptimizers = ['sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam']\n\n\n%%time\nfor optimizer in optimizers:\n    set_seeds()\n    model = create_model(hl=1, hu=128, dropout=True, rate=0.3, regularize=False, reg=l2(0.001), optimizer=optimizer)\n    t0 = time.time()\n    model.fit(train_[cols], train['d'], epochs=50, verbose=False, validation_split=0.2, shuffle=False, class_weight=cw(train))\n    t1 = time.time()\n    t = t1 - t0\n    acc_tr = model.evaluate(train_[cols], train['d'], verbose=False)[1]\n    acc_te = model.evaluate(test_[cols], test['d'], verbose=False)[1]\n    out = f'{optimizer:10s} | time[s]: {t:.4f} | in-sample={acc_tr:.4f}'\n    out += f' | out-of-sample={acc_te:.4f}'\n    print(out)\n\nsgd        | time[s]: 4.5323 | in-sample=0.6346 | out-of-sample=0.6728\nrmsprop    | time[s]: 4.3281 | in-sample=0.7692 | out-of-sample=0.6499\nadagrad    | time[s]: 4.2491 | in-sample=0.6254 | out-of-sample=0.6613\nadadelta   | time[s]: 4.4557 | in-sample=0.3396 | out-of-sample=0.3501\nadam       | time[s]: 4.5690 | in-sample=0.7766 | out-of-sample=0.6499\nadamax     | time[s]: 5.0954 | in-sample=0.6770 | out-of-sample=0.6293\nnadam      | time[s]: 5.4853 | in-sample=0.7852 | out-of-sample=0.6682\nCPU times: user 44.8 s, sys: 2.56 s, total: 47.3 s\nWall time: 34.3 s\n\n\nThe choice of optimizer doesn’t seem to make a huge difference, except for adadelta.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Exchange Rate Prediction</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html",
    "title": "45  Stock Returns: Neural Network",
    "section": "",
    "text": "45.1 Import Packages\nOur objective in this chapter is to predict stock returns using dense feed-forward neural networks. Specifically, we will try to predict the daily returns of MSFT from the returns of various correlated assets including stock indices, currencies, and other stocks.\nWe will begin by first reviewing our previous work in which we use linear regression and nearest neighbors to predict MSFT returns.\nLet’s begin by importing the initial packages that we will need.\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport sklearn",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#read-in-data",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#read-in-data",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.2 Read-In Data",
    "text": "45.2 Read-In Data\nNext, let’s read-in our data. We will start the stocks, whose data we will get from Yahoo.\n\nstock_tickers = ['MSFT', 'IBM', 'GOOGL'] # define tickers\ndf_stock = yf.download(\n    stock_tickers, start='2005-01-01', end='2021-07-31', auto_adjust=False,\n)\ndf_stock = df_stock['Adj Close'] # select only the adjusted close price\ndf_stock.columns = df_stock.columns.str.lower() # clean-up column names\ndf_stock.rename_axis('trade_date', inplace=True) # clean-up index name\ndf_stock.rename_axis('', axis=1, inplace=True) # clean-up index name\ndf_stock\n\n[*********************100%***********************]  3 of 3 completed\n\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\n\n\ntrade_date\n\n\n\n\n\n\n\n2005-01-03\n5.038074\n50.237442\n18.454880\n\n\n2005-01-04\n4.834026\n49.697788\n18.523895\n\n\n2005-01-05\n4.809422\n49.595024\n18.482485\n\n\n2005-01-06\n4.686148\n49.440849\n18.461790\n\n\n2005-01-07\n4.817872\n49.225010\n18.406578\n\n\n...\n...\n...\n...\n\n\n2021-07-26\n133.116882\n114.338181\n279.157227\n\n\n2021-07-27\n130.996506\n114.322174\n276.733154\n\n\n2021-07-28\n135.161743\n113.537315\n276.424042\n\n\n2021-07-29\n134.847443\n113.665459\n276.694397\n\n\n2021-07-30\n133.803650\n112.888634\n275.158905\n\n\n\n\n4173 rows × 3 columns\n\n\n\nNext, we’ll grab the currency data from FRED.\n\ncurrency_tickers = ['JPY=X', 'GBPUSD=X']\ndf_currency = yf.download(\n    currency_tickers, start='2005-01-01', end='2021-07-31',\n    auto_adjust=False, ignore_tz=True\n)\ndf_currency = df_currency['Adj Close']\ndf_currency.columns = df_currency.columns.str.lower()\ndf_currency.rename_axis('trade_date', inplace=True)\ndf_currency.rename_axis('', axis=1, inplace=True)\ndf_currency\n\n[*********************100%***********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\ngbpusd=x\njpy=x\n\n\ntrade_date\n\n\n\n\n\n\n2005-01-03\n1.904617\n102.739998\n\n\n2005-01-04\n1.883594\n104.339996\n\n\n2005-01-05\n1.885512\n103.930000\n\n\n2005-01-06\n1.876490\n104.889999\n\n\n2005-01-07\n1.871293\n104.889999\n\n\n...\n...\n...\n\n\n2021-07-26\n1.375781\n110.543999\n\n\n2021-07-27\n1.382915\n110.302002\n\n\n2021-07-28\n1.388272\n109.806000\n\n\n2021-07-29\n1.390685\n109.890999\n\n\n2021-07-30\n1.396433\n109.408997\n\n\n\n\n4313 rows × 2 columns\n\n\n\nFinally, we’ll grab the index data Yahoo.\n\nindex_tickers = ['SPY', 'DIA', '^VIX'] \ndf_index = yf.download(\n    index_tickers, start='2005-01-01', end='2021-07-31', auto_adjust=False\n)\ndf_index = df_index['Adj Close']\ndf_index.columns = df_index.columns.str.lower().str.replace('^', '')\ndf_index.rename_axis('trade_date', inplace=True)\ndf_index.rename_axis('', axis=1, inplace=True)\ndf_index\n\n[*********************100%***********************]  3 of 3 completed\n\n\n\n\n\n\n\n\n\ndia\nspy\nvix\n\n\ntrade_date\n\n\n\n\n\n\n\n2005-01-03\n67.762741\n82.074020\n14.080000\n\n\n2005-01-04\n67.118706\n81.071114\n13.980000\n\n\n2005-01-05\n66.746117\n80.511688\n14.090000\n\n\n2005-01-06\n66.954491\n80.921036\n13.580000\n\n\n2005-01-07\n66.828247\n80.805092\n13.490000\n\n\n...\n...\n...\n...\n\n\n2021-07-26\n326.679871\n416.758026\n17.580000\n\n\n2021-07-27\n325.945435\n414.858612\n19.360001\n\n\n2021-07-28\n324.774048\n414.688477\n18.309999\n\n\n2021-07-29\n326.131317\n416.408386\n17.700001\n\n\n2021-07-30\n324.885559\n414.386139\n18.240000\n\n\n\n\n4173 rows × 3 columns",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#join-and-clean-data",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#join-and-clean-data",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.3 Join and Clean Data",
    "text": "45.3 Join and Clean Data\nNow we can join together our price data and convert it into returns and differences (for VIX, as these are more stationary). Notice that we are implicitly adding a time series component to our regression by adding lagged msft returns as a feature.\n\ndf_data = \\\n    (\n    df_stock\n        .merge(df_index, how='left', left_index=True, right_index=True) # join currency data\n        .merge(df_currency, how='left', left_index=True, right_index=True) # join index data\n        .dropna()\n        .assign(msft = lambda df: df['msft'].pct_change())   # percent change\n        .assign(msft_lag_0 = lambda df: df['msft'].shift(0)) #\n        .assign(msft_lag_1 = lambda df: df['msft'].shift(1)) #\n        .assign(ibm = lambda df: df['ibm'].pct_change())     #\n        .assign(googl = lambda df: df['googl'].pct_change()) #\n        .assign(spy = lambda df: df['spy'].pct_change())     #\n        .assign(dia = lambda df: df['dia'].pct_change())     #\n        .assign(vix = lambda df: df['vix'].diff())           # absolute change\n        .assign(dexjpus = lambda df: df['jpy=x'].pct_change()) # percent change\n        .assign(dexusuk = lambda df: df['gbpusd=x'].pct_change()) #\n        .dropna()\n    )\ndf_data\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.002236\n-0.005551\n-0.006900\n0.110001\n1.885512\n103.930000\n-0.002236\n0.003740\n-0.003929\n0.001018\n\n\n2005-01-06\n-0.025632\n-0.003109\n-0.001120\n0.003122\n0.005084\n-0.510000\n1.876490\n104.889999\n-0.001120\n-0.002236\n0.009237\n-0.004785\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.002991\n-0.001886\n-0.001433\n-0.090000\n1.871293\n104.889999\n-0.002991\n-0.001120\n0.000000\n-0.002769\n\n\n2005-01-10\n0.006242\n-0.001044\n0.004874\n0.003401\n0.004728\n-0.260000\n1.876912\n104.169998\n0.004874\n-0.002991\n-0.006864\n0.003003\n\n\n2005-01-11\n-0.007793\n-0.007108\n-0.002612\n-0.006402\n-0.006891\n-0.040000\n1.878605\n103.419998\n-0.002612\n0.004874\n-0.007200\n0.000902\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-26\n0.007668\n0.010117\n-0.002140\n0.002396\n0.002455\n0.379999\n1.375781\n110.543999\n-0.002140\n0.012336\n0.003668\n-0.001168\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.008684\n-0.002248\n-0.004558\n1.780001\n1.382915\n110.302002\n-0.008684\n-0.002140\n-0.002189\n0.005186\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.001117\n-0.003594\n-0.000410\n-1.050001\n1.388272\n109.806000\n-0.001117\n-0.008684\n-0.004497\n0.003873\n\n\n2021-07-29\n-0.002325\n0.001129\n0.000978\n0.004179\n0.004147\n-0.609999\n1.390685\n109.890999\n0.000978\n-0.001117\n0.000774\n0.001738\n\n\n2021-07-30\n-0.007741\n-0.006834\n-0.005549\n-0.003820\n-0.004856\n0.539999\n1.396433\n109.408997\n-0.005549\n0.000978\n-0.004386\n0.004133\n\n\n\n\n4140 rows × 12 columns",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#training-set-and-testing-set",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#training-set-and-testing-set",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.4 Training Set and Testing Set",
    "text": "45.4 Training Set and Testing Set\nWe’ll train our models on data prior to 2016, and then we’ll use data from 2016 onward for testing. So let’s separate out these two subsets of data.\n\ndf_train = df_data.query('trade_date &lt; \"2016-01-01\"')\ndf_train\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.002236\n-0.005551\n-0.006900\n0.110001\n1.885512\n103.930000\n-0.002236\n0.003740\n-0.003929\n0.001018\n\n\n2005-01-06\n-0.025632\n-0.003109\n-0.001120\n0.003122\n0.005084\n-0.510000\n1.876490\n104.889999\n-0.001120\n-0.002236\n0.009237\n-0.004785\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.002991\n-0.001886\n-0.001433\n-0.090000\n1.871293\n104.889999\n-0.002991\n-0.001120\n0.000000\n-0.002769\n\n\n2005-01-10\n0.006242\n-0.001044\n0.004874\n0.003401\n0.004728\n-0.260000\n1.876912\n104.169998\n0.004874\n-0.002991\n-0.006864\n0.003003\n\n\n2005-01-11\n-0.007793\n-0.007108\n-0.002612\n-0.006402\n-0.006891\n-0.040000\n1.878605\n103.419998\n-0.002612\n0.004874\n-0.007200\n0.000902\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2015-12-24\n-0.003474\n-0.002093\n-0.002687\n-0.003356\n-0.001650\n0.170000\n1.487697\n120.934998\n-0.002687\n0.008491\n-0.000785\n0.003615\n\n\n2015-12-28\n0.021414\n-0.004629\n0.005029\n-0.001370\n-0.002285\n1.170000\n1.493206\n120.231003\n0.005029\n-0.002687\n-0.005821\n0.003703\n\n\n2015-12-29\n0.014983\n0.015769\n0.010724\n0.011430\n0.010672\n-0.830000\n1.489403\n120.349998\n0.010724\n0.005029\n0.000990\n-0.002547\n\n\n2015-12-30\n-0.004610\n-0.003148\n-0.004244\n-0.006668\n-0.007088\n1.210001\n1.482228\n120.528999\n-0.004244\n0.010724\n0.001487\n-0.004817\n\n\n2015-12-31\n-0.015551\n-0.012344\n-0.014740\n-0.010295\n-0.010003\n0.919998\n1.481921\n120.449997\n-0.014740\n-0.004244\n-0.000655\n-0.000207\n\n\n\n\n2739 rows × 12 columns\n\n\n\n\ndf_test = df_data.query('trade_date &gt; \"2016-01-01\"')\ndf_test\n\n\n\n\n\n\n\n\ngoogl\nibm\nmsft\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n-0.023869\n-0.012135\n-0.012257\n-0.015518\n-0.013979\n2.490002\n1.473709\n120.310997\n-0.012257\n-0.014740\n-0.001154\n-0.005541\n\n\n2016-01-05\n0.002752\n-0.000735\n0.004562\n0.000583\n0.001691\n-1.360001\n1.471410\n119.467003\n0.004562\n-0.012257\n-0.007015\n-0.001560\n\n\n2016-01-06\n-0.002889\n-0.005006\n-0.018165\n-0.014294\n-0.012614\n1.250000\n1.467394\n119.101997\n-0.018165\n0.004562\n-0.003055\n-0.002729\n\n\n2016-01-07\n-0.024140\n-0.017090\n-0.034783\n-0.023559\n-0.023991\n4.400000\n1.462994\n118.610001\n-0.034783\n-0.018165\n-0.004131\n-0.002999\n\n\n2016-01-08\n-0.013617\n-0.009258\n0.003067\n-0.010427\n-0.010977\n2.020000\n1.462694\n117.540001\n0.003067\n-0.034783\n-0.009021\n-0.000205\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-26\n0.007668\n0.010117\n-0.002140\n0.002396\n0.002455\n0.379999\n1.375781\n110.543999\n-0.002140\n0.012336\n0.003668\n-0.001168\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.008684\n-0.002248\n-0.004558\n1.780001\n1.382915\n110.302002\n-0.008684\n-0.002140\n-0.002189\n0.005186\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.001117\n-0.003594\n-0.000410\n-1.050001\n1.388272\n109.806000\n-0.001117\n-0.008684\n-0.004497\n0.003873\n\n\n2021-07-29\n-0.002325\n0.001129\n0.000978\n0.004179\n0.004147\n-0.609999\n1.390685\n109.890999\n0.000978\n-0.001117\n0.000774\n0.001738\n\n\n2021-07-30\n-0.007741\n-0.006834\n-0.005549\n-0.003820\n-0.004856\n0.539999\n1.396433\n109.408997\n-0.005549\n0.000978\n-0.004386\n0.004133\n\n\n\n\n1401 rows × 12 columns",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#training-linear-regression-and-k-nearest-neighbors",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#training-linear-regression-and-k-nearest-neighbors",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.5 Training Linear Regression and K-Nearest Neighbors",
    "text": "45.5 Training Linear Regression and K-Nearest Neighbors\nIn order to train our model, we first put our training features into X_train and our training labels into y_train\n\nX_train = df_train.drop(columns=['msft'])[0:len(df_train)-1]\nX_train\n\n\n\n\n\n\n\n\ngoogl\nibm\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-01-05\n-0.005090\n-0.002068\n-0.005551\n-0.006900\n0.110001\n1.885512\n103.930000\n-0.002236\n0.003740\n-0.003929\n0.001018\n\n\n2005-01-06\n-0.025632\n-0.003109\n0.003122\n0.005084\n-0.510000\n1.876490\n104.889999\n-0.001120\n-0.002236\n0.009237\n-0.004785\n\n\n2005-01-07\n0.028109\n-0.004366\n-0.001886\n-0.001433\n-0.090000\n1.871293\n104.889999\n-0.002991\n-0.001120\n0.000000\n-0.002769\n\n\n2005-01-10\n0.006242\n-0.001044\n0.003401\n0.004728\n-0.260000\n1.876912\n104.169998\n0.004874\n-0.002991\n-0.006864\n0.003003\n\n\n2005-01-11\n-0.007793\n-0.007108\n-0.006402\n-0.006891\n-0.040000\n1.878605\n103.419998\n-0.002612\n0.004874\n-0.007200\n0.000902\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2015-12-23\n0.001799\n0.004422\n0.010344\n0.012384\n-1.030001\n1.482338\n121.029999\n0.008491\n0.009484\n-0.001452\n-0.004936\n\n\n2015-12-24\n-0.003474\n-0.002093\n-0.003356\n-0.001650\n0.170000\n1.487697\n120.934998\n-0.002687\n0.008491\n-0.000785\n0.003615\n\n\n2015-12-28\n0.021414\n-0.004629\n-0.001370\n-0.002285\n1.170000\n1.493206\n120.231003\n0.005029\n-0.002687\n-0.005821\n0.003703\n\n\n2015-12-29\n0.014983\n0.015769\n0.011430\n0.010672\n-0.830000\n1.489403\n120.349998\n0.010724\n0.005029\n0.000990\n-0.002547\n\n\n2015-12-30\n-0.004610\n-0.003148\n-0.006668\n-0.007088\n1.210001\n1.482228\n120.528999\n-0.004244\n0.010724\n0.001487\n-0.004817\n\n\n\n\n2738 rows × 11 columns\n\n\n\nNotice that the label we are predicting is the next day msft return; the features we are using to predict are the current day returns of the various correlated assets.\n\ny_train = df_train[['msft']][1:len(df_train)]\ny_train\n\n\n\n\n\n\n\n\nmsft\n\n\ntrade_date\n\n\n\n\n\n2005-01-06\n-0.001120\n\n\n2005-01-07\n-0.002991\n\n\n2005-01-10\n0.004874\n\n\n2005-01-11\n-0.002612\n\n\n2005-01-12\n0.001870\n\n\n...\n...\n\n\n2015-12-24\n-0.002687\n\n\n2015-12-28\n0.005029\n\n\n2015-12-29\n0.010724\n\n\n2015-12-30\n-0.004244\n\n\n2015-12-31\n-0.014740\n\n\n\n\n2738 rows × 1 columns\n\n\n\n\n45.5.1 Linear Regression\nLet’s first fit a simple linear regression to our training data.\n\nfrom sklearn.linear_model import LinearRegression\nlinear_regression = LinearRegression()\nlinear_regression.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nRecall that the .score() of a LinearRegression gives the \\(R^2\\).\n\nlinear_regression.score(X_train, y_train)\n\n0.017959946658375414\n\n\nWe can also examine the coefficients of our model.\n\nnp.round(linear_regression.coef_, 3)\n\narray([[ 0.002, -0.016,  0.214, -0.328,  0.   , -0.003,  0.   ,  0.012,\n        -0.048, -0.002, -0.002]])\n\n\n\n\n45.5.2 KNN\nNext, let’s fit a KNN to our model. As you can see, the in-sample \\(R^2\\) is higher for KNN over Linear Regression.\n\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn.score(X_train, y_train)\n\n0.1212395470121359\n\n\n\n45.5.2.1 Mean-Squared Error\nAnother goodness-of-fit metric is the mean squared error. As you can see the models are close on this metric.\n\nsklearn.metrics.mean_squared_error(y_train, linear_regression.predict(X_train))\n\n0.000294029286668637\n\n\n\nsklearn.metrics.mean_squared_error(y_train, knn.predict(X_train))\n\n0.00026310669128558063",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#testing-linear-regression-and-k-nearest-neighbors",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#testing-linear-regression-and-k-nearest-neighbors",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.6 Testing Linear Regression and K-Nearest Neighbors",
    "text": "45.6 Testing Linear Regression and K-Nearest Neighbors\nLet’s now test the model with the data after 2016.\n\nX_test = df_test.drop(columns=['msft'])[0:len(df_test)-1]\nX_test\n\n\n\n\n\n\n\n\ngoogl\nibm\ndia\nspy\nvix\ngbpusd=x\njpy=x\nmsft_lag_0\nmsft_lag_1\ndexjpus\ndexusuk\n\n\ntrade_date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-04\n-0.023869\n-0.012135\n-0.015518\n-0.013979\n2.490002\n1.473709\n120.310997\n-0.012257\n-0.014740\n-0.001154\n-0.005541\n\n\n2016-01-05\n0.002752\n-0.000735\n0.000583\n0.001691\n-1.360001\n1.471410\n119.467003\n0.004562\n-0.012257\n-0.007015\n-0.001560\n\n\n2016-01-06\n-0.002889\n-0.005006\n-0.014294\n-0.012614\n1.250000\n1.467394\n119.101997\n-0.018165\n0.004562\n-0.003055\n-0.002729\n\n\n2016-01-07\n-0.024140\n-0.017090\n-0.023559\n-0.023991\n4.400000\n1.462994\n118.610001\n-0.034783\n-0.018165\n-0.004131\n-0.002999\n\n\n2016-01-08\n-0.013617\n-0.009258\n-0.010427\n-0.010977\n2.020000\n1.462694\n117.540001\n0.003067\n-0.034783\n-0.009021\n-0.000205\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-07-23\n0.035769\n0.004477\n0.006633\n0.010288\n-0.490000\n1.377390\n110.139999\n0.012336\n0.016845\n-0.001043\n0.004365\n\n\n2021-07-26\n0.007668\n0.010117\n0.002396\n0.002455\n0.379999\n1.375781\n110.543999\n-0.002140\n0.012336\n0.003668\n-0.001168\n\n\n2021-07-27\n-0.015929\n-0.000140\n-0.002248\n-0.004558\n1.780001\n1.382915\n110.302002\n-0.008684\n-0.002140\n-0.002189\n0.005186\n\n\n2021-07-28\n0.031797\n-0.006865\n-0.003594\n-0.000410\n-1.050001\n1.388272\n109.806000\n-0.001117\n-0.008684\n-0.004497\n0.003873\n\n\n2021-07-29\n-0.002325\n0.001129\n0.004179\n0.004147\n-0.609999\n1.390685\n109.890999\n0.000978\n-0.001117\n0.000774\n0.001738\n\n\n\n\n1400 rows × 11 columns\n\n\n\n\ny_test = df_test[['msft']][1:len(df_test)]\ny_test\n\n\n\n\n\n\n\n\nmsft\n\n\ntrade_date\n\n\n\n\n\n2016-01-05\n0.004562\n\n\n2016-01-06\n-0.018165\n\n\n2016-01-07\n-0.034783\n\n\n2016-01-08\n0.003067\n\n\n2016-01-11\n-0.000573\n\n\n...\n...\n\n\n2021-07-26\n-0.002140\n\n\n2021-07-27\n-0.008684\n\n\n2021-07-28\n-0.001117\n\n\n2021-07-29\n0.000978\n\n\n2021-07-30\n-0.005549\n\n\n\n\n1400 rows × 1 columns\n\n\n\nIn terms of \\(R^2\\), the LinearRegression performs better than KNN on the testing data.\n\nlinear_regression.score(X_test, y_test)\n\n0.03710052545948106\n\n\n\nknn.score(X_test, y_test)\n\n-0.022694775311824067\n\n\nOn the testing data, the models are again quite similar from an mean squared error perspective.\n\nsklearn.metrics.mean_squared_error(y_test, linear_regression.predict(X_test))\n\n0.00028192086585344305\n\n\n\nsklearn.metrics.mean_squared_error(y_test, knn.predict(X_test))\n\n0.0002994279301037974",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#neural-network",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#neural-network",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.7 Neural Network",
    "text": "45.7 Neural Network\nLet’s now fit our first neural network. We begin by importing some addition packages and functions.\n\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\n2025-09-15 15:56:21.845234: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-09-15 15:56:21.881683: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-09-15 15:56:21.881713: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-09-15 15:56:21.881741: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-09-15 15:56:21.888254: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-09-15 15:56:21.888862: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-09-15 15:56:22.671910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\nIn order to make our results reproducible, we’ll use the following user defined function to set the various seeds for random number generation (this doesn’t seem to fully work for some reason, although in previous chapters it seems to work, I’m not sure what the difference is).\n\ndef set_seeds(seed=100):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n\nset_seeds()\n\nNow we can build and compile our model.\n\nmodel = Sequential()\nmodel.add(Dense(units=128, input_dim=len(X_train.columns), activation='relu'))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n\nLet’s fit our model.\n\n%%time\nmodel.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0);\n\nCPU times: user 5.76 s, sys: 286 ms, total: 6.04 s\nWall time: 4.23 s\n\n\nAs we can see from our two metrics, this baseline neural network performs better than LinearRegression and k-nearest neighbors.\n\nsklearn.metrics.r2_score(y_test, model.predict(X_test))\n\n44/44 [==============================] - 0s 769us/step\n\n\n0.061010003089904785\n\n\n\nsklearn.metrics.mean_squared_error(y_test, model.predict(X_test))\n\n44/44 [==============================] - 0s 919us/step\n\n\n0.0002749205786398829",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#normalization",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#normalization",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.8 Normalization",
    "text": "45.8 Normalization\nNext, let’s normalize our data and refit.\n\nmu = X_train.mean()\nstd = X_train.std()\n\n\nX_train_scaled = (X_train - mu) / std\nX_test_scaled = (X_test - mu) / std\n\n\nset_seeds()\n\n\nmodel = Sequential()\nmodel.add(Dense(units=128, input_dim=len(X_train_scaled.columns), activation='relu'))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n\n\n%%time\nmodel.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0);\n\nCPU times: user 5.63 s, sys: 269 ms, total: 5.9 s\nWall time: 4.17 s\n\n\nThis seems to have a drastically negative impact on model performance. So we won’t use normalization as we proceed.\n\nsklearn.metrics.r2_score(y_test, model.predict(X_test_scaled))\n\n44/44 [==============================] - 0s 799us/step\n\n\n-1.2055394649505615\n\n\n\nsklearn.metrics.mean_squared_error(y_test, model.predict(X_test_scaled))\n\n44/44 [==============================] - 0s 927us/step\n\n\n0.0006457450377929333",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#dropout",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#dropout",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.9 Dropout",
    "text": "45.9 Dropout\nIn this section, we perform dropout regularization.\n\nfrom keras.layers import Dropout\n\n\nset_seeds()\n\n\nmodel = Sequential()\nmodel.add(Dense(units=128, input_dim=len(X_train.columns), activation='relu'))\nmodel.add(Dropout(rate=0.3, seed=100))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dropout(rate=0.3, seed=100))\nmodel.add(Dense(1))\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n\n\n%%time\nmodel.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0);\n\nCPU times: user 6.34 s, sys: 242 ms, total: 6.58 s\nWall time: 4.66 s\n\n\nWith dropout regularization, the model performs poorly relative to linear regression, nearest neighbors, and the baseline neural network.\n\nsklearn.metrics.r2_score(y_test, model.predict(X_test))\n\n44/44 [==============================] - 0s 770us/step\n\n\n-0.0013687610626220703\n\n\n\nsklearn.metrics.mean_squared_error(y_test, model.predict(X_test))\n\n44/44 [==============================] - 0s 1ms/step\n\n\n0.00029318401815150996",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#regularization",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#regularization",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.10 Regularization",
    "text": "45.10 Regularization\nNow let’s try l2 (ridge) regularization.\n\nfrom keras.regularizers import l2\n\n\nset_seeds()\n\n\nmodel = Sequential()\nmodel.add(Dense(units=128, input_dim=len(X_train.columns), activation='relu', activity_regularizer=l2(0.0005)))\nmodel.add(Dense(units=128, activation='relu', activity_regularizer=l2(0.0005)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n\n\n%%time\nmodel.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0);\n\nCPU times: user 6.05 s, sys: 318 ms, total: 6.37 s\nWall time: 4.31 s\n\n\nUsing l2 doesn’t improve the model compared to linear regression, nearest neighbors, and the baseline linear regression.\n\nsklearn.metrics.r2_score(y_test, model.predict(X_test))\n\n44/44 [==============================] - 0s 774us/step\n\n\n0.04668128490447998\n\n\n\nsklearn.metrics.mean_squared_error(y_test, model.predict(X_test))\n\n44/44 [==============================] - 0s 921us/step\n\n\n0.0002791157822826718",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#classification",
    "href": "chapters/39_stock_prediction_neural_network/stock_prediction_neural_network.html#classification",
    "title": "45  Stock Returns: Neural Network",
    "section": "45.11 Classification",
    "text": "45.11 Classification\nFinally, let’s recast this as a classification problem where we are simply trying to predict gains and losses. First we have to change our labels to binary outcomes.\n\ny_train_classification = np.where(y_train['msft'] &gt; 0, 1, 0)\ny_test_classification = np.where(y_test['msft'] &gt; 0, 1, 0)\n\n\nset_seeds()\n\n\nmodel = Sequential()\nmodel.add(Dense(units=128, input_dim=len(X_train.columns), activation='relu'))\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n%%time\nmodel.fit(X_train, y_train_classification, epochs=50, batch_size=32, verbose=0);\n\nCPU times: user 6.3 s, sys: 335 ms, total: 6.63 s\nWall time: 4.7 s\n\n\nThe binary classification is right about 45% of the time..\n\nmodel.evaluate(X_test, y_test_classification)\n\n44/44 [==============================] - 0s 1ms/step - loss: 0.7167 - accuracy: 0.4657\n\n\n[0.7166569232940674, 0.4657142758369446]\n\n\nGuessing that MSFT will rise everyday is right 55% of the time.\n\ny_test_classification.mean()\n\n0.5514285714285714",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Stock Returns: Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html",
    "href": "chapters/40_numerai/numerai.html",
    "title": "46  Numerai",
    "section": "",
    "text": "46.1 Reading-In Data\nNumerai is a hedge fund that crowdsources their market predictions. They disseminate data that is anonymized so that the data scientists who are working on the forecasting are not even aware of the features and labels they are working with. The prediction problem is reduced to a classification of predicting a gain or loss.\nIn this chapter we will fit a variety models to Numerai data.\nLet’s begin by reading-in the data and organizing our features and labels.\nimport pandas as pd\nimport numpy as np\ndf_data = pd.read_csv('../data/numerai_training_data.csv')\ndf_data.head()\n\n\n\n\n\n\n\n\nfeature1\nfeature2\nfeature3\nfeature4\nfeature5\nfeature6\nfeature7\nfeature8\nfeature9\nfeature10\n...\nfeature13\nfeature14\nfeature15\nfeature16\nfeature17\nfeature18\nfeature19\nfeature20\nfeature21\ntarget\n\n\n\n\n0\n0.499664\n0.951271\n0.127110\n0.469706\n0.188336\n0.113830\n0.917618\n0.398412\n0.418910\n0.452983\n...\n0.137192\n0.201437\n0.507708\n0.919475\n0.978169\n0.177080\n0.101372\n0.722138\n0.832319\n0\n\n\n1\n0.099515\n0.682824\n0.867939\n0.943828\n0.505526\n0.886766\n0.530862\n0.531002\n0.980002\n0.941859\n...\n0.642640\n0.533367\n0.616879\n0.697038\n0.741461\n0.086690\n0.109533\n0.324666\n0.552276\n1\n\n\n2\n0.671993\n0.383901\n0.533011\n0.690863\n0.176539\n0.600196\n0.381543\n0.648849\n0.831643\n0.861746\n...\n0.520068\n0.660924\n0.538882\n0.160117\n0.765317\n0.301772\n0.352097\n0.638205\n0.383552\n0\n\n\n3\n0.578177\n0.872357\n0.679625\n0.108961\n0.945910\n0.571062\n0.891958\n0.916592\n0.141508\n0.258504\n...\n0.037959\n0.604539\n0.974103\n0.187519\n0.938254\n0.560129\n0.136483\n0.284507\n0.199446\n1\n\n\n4\n0.474311\n0.639613\n0.563562\n0.169508\n0.456858\n0.580710\n0.969811\n0.357417\n0.157594\n0.251147\n...\n0.038095\n0.770200\n0.697395\n0.792327\n0.711650\n0.177080\n0.247403\n0.666598\n0.755557\n0\n\n\n\n\n5 rows × 22 columns\ndf_X = df_data.drop(columns='target').copy()\ndf_y = df_data['target'].copy()",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html#the-data-is-clean",
    "href": "chapters/40_numerai/numerai.html#the-data-is-clean",
    "title": "46  Numerai",
    "section": "46.2 The Data is Clean",
    "text": "46.2 The Data is Clean\nOne nice thing about working with the Numerai data is that it is clean and normalized.\n\nAll the features are in the range of \\([0, 1]\\).\nAll the features have a mean of 0.50 and a standard deviation of 0.28.\nThe occurrence of labels is even at 50% gains and 50% losses.\n\n\ndf_X.mean()\n\nfeature1     0.511372\nfeature2     0.492770\nfeature3     0.492105\nfeature4     0.499420\nfeature5     0.502291\nfeature6     0.493039\nfeature7     0.480280\nfeature8     0.494526\nfeature9     0.492926\nfeature10    0.489265\nfeature11    0.495725\nfeature12    0.510969\nfeature13    0.489852\nfeature14    0.509350\nfeature15    0.487469\nfeature16    0.509012\nfeature17    0.488944\nfeature18    0.484929\nfeature19    0.491757\nfeature20    0.509223\nfeature21    0.498371\ndtype: float64\n\n\n\ndf_X.std()\n\nfeature1     0.282260\nfeature2     0.287446\nfeature3     0.282481\nfeature4     0.284493\nfeature5     0.289867\nfeature6     0.287061\nfeature7     0.287526\nfeature8     0.288087\nfeature9     0.293945\nfeature10    0.287046\nfeature11    0.290922\nfeature12    0.285451\nfeature13    0.291276\nfeature14    0.290140\nfeature15    0.286997\nfeature16    0.289279\nfeature17    0.284790\nfeature18    0.290445\nfeature19    0.283742\nfeature20    0.291001\nfeature21    0.289637\ndtype: float64\n\n\nNotice that a guess of increase for all assets would yield an accuracy of 50.5%.\n\ndf_y.mean()\n\n0.5051702657807309",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html#logistic-regression",
    "href": "chapters/40_numerai/numerai.html#logistic-regression",
    "title": "46  Numerai",
    "section": "46.3 Logistic Regression",
    "text": "46.3 Logistic Regression\nThe first model that we will fit is a simple LogisticRegression. We will use a 10-fold cross-validation accuracy as our goodness of fit metric.\n\nfrom sklearn.linear_model import LogisticRegression\nmdl_logistic_regression = LogisticRegression(C=1.0, random_state=0)\n\n\n%%time\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(mdl_logistic_regression, df_X, df_y, cv=10, verbose=0)\n\nCPU times: user 21 s, sys: 17.9 s, total: 39 s\nWall time: 5.1 s\n\n\nWe get a mean accuracy of about 52% which is just slightly higher than guessing a gain for all assets.\n\nnp.mean(scores)\n\n0.5219476744186047",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html#random-forest",
    "href": "chapters/40_numerai/numerai.html#random-forest",
    "title": "46  Numerai",
    "section": "46.4 Random Forest",
    "text": "46.4 Random Forest\nNext let’s fit a RandomForestClassifier. We use a 5-fold cross-validation accuracy rather than 10-fold because these models take time to run.\n\nfrom sklearn.ensemble import RandomForestClassifier\nmdl_random_forest = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=0)\n\n\n%%time\nscores = cross_val_score(mdl_random_forest, df_X, df_y, cv=5, scoring='accuracy', verbose=0)\nscores\n\nCPU times: user 44.4 s, sys: 0 ns, total: 44.4 s\nWall time: 44.4 s\n\n\narray([0.51858389, 0.51723422, 0.52450166, 0.51650748, 0.52460548])\n\n\nSimilar to logistic regression we get a mean score of around 52%\n\nnp.mean(scores)\n\n0.5202865448504983",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html#xgboost",
    "href": "chapters/40_numerai/numerai.html#xgboost",
    "title": "46  Numerai",
    "section": "46.5 XGBoost",
    "text": "46.5 XGBoost\nThe next model that we will fit is a gradient boosted tree with the xgboost package. We use 5-fold cross-validation to assess model performance.\n\nfrom xgboost import XGBClassifier\nmdl_xgboost = XGBClassifier(n_estimators=100, max_depth=10, learning_rate=0.01, eval_metric='logloss')\n\n\nscores = cross_val_score(mdl_xgboost, df_X, df_y, cv=5, scoring='accuracy', verbose=0)\nscores\n\narray([0.51292566, 0.51775332, 0.51977782, 0.51629983, 0.515625  ])\n\n\nOur mean accuracy score is slightly lower at 51.5%.\n\nnp.mean(scores)\n\n0.5164763289036545",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html#validation-set",
    "href": "chapters/40_numerai/numerai.html#validation-set",
    "title": "46  Numerai",
    "section": "46.6 Validation Set",
    "text": "46.6 Validation Set\nIn the subsequent sections we will try a variety of neural networks. In order to check out-of-sample accuracy, let’s first create a holdout set with train_test_split().\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.20, random_state=0)",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html#initial-neural-network",
    "href": "chapters/40_numerai/numerai.html#initial-neural-network",
    "title": "46  Numerai",
    "section": "46.7 Initial Neural Network",
    "text": "46.7 Initial Neural Network\nWe are now ready to fit our first neural network.\n\nimport random\nimport tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score\n\n2023-09-02 13:39:44.717146: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-02 13:39:44.941096: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-02 13:39:44.943020: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-09-02 13:39:45.900127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\nLet’s set our random seeds to get reproducible results.\n\ndef set_seeds(seed=100):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n\nset_seeds()\n\nNext, let’s build up our dense feed-forward neural network. We use 3 hidden layer with 16, 8, and 4 units, respectively.\n\nmodel = Sequential()\nmodel.add(Dense(units=16, input_dim=len(df_X.columns), activation='relu'))\nmodel.add(Dense(units=8, input_dim=len(df_X.columns), activation='relu'))\nmodel.add(Dense(units=4, input_dim=len(df_X.columns), activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\nNow we can fit our model.\n\n%%time\nh = model.fit(X_train, y_train, epochs=10, verbose=False);\n\nCPU times: user 30.1 s, sys: 2.14 s, total: 32.2 s\nWall time: 21.2 s\n\n\nAs we can see, our neural network model performs about the same as the previous models.\n\nmodel.evaluate(X_test, y_test)\n\n602/602 [==============================] - 1s 813us/step - loss: 0.6916 - accuracy: 0.5192\n\n\n[0.6915713548660278, 0.5192068219184875]",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html#dropout",
    "href": "chapters/40_numerai/numerai.html#dropout",
    "title": "46  Numerai",
    "section": "46.8 Dropout",
    "text": "46.8 Dropout\nIn this section we implement dropout regularization with our neural network. We use a slightly different architecture with 2 hidden units of 64 and 32 units, respectively.\n\nfrom keras.layers import Dropout\n\n\nset_seeds()\n\n\nmodel = Sequential()\nmodel.add(Dense(units=64, input_dim=len(df_X.columns), activation='relu'))\nmodel.add(Dropout(rate=0.1, seed=100))\nmodel.add(Dense(units=32, input_dim=len(df_X.columns), activation='relu'))\nmodel.add(Dropout(rate=0.1, seed=100))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\n\n%%time\nmodel.fit(X_train, y_train, epochs=10, verbose=False);\n\nCPU times: user 32.3 s, sys: 2.01 s, total: 34.3 s\nWall time: 21.2 s\n\n\nDropout reglarization doesn’t seem to have much of an effect on our model.\n\nmodel.evaluate(X_test, y_test)\n\n602/602 [==============================] - 1s 898us/step - loss: 0.6916 - accuracy: 0.5192\n\n\n[0.6916125416755676, 0.5192068219184875]",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/40_numerai/numerai.html#regularization",
    "href": "chapters/40_numerai/numerai.html#regularization",
    "title": "46  Numerai",
    "section": "46.9 Regularization",
    "text": "46.9 Regularization\nFinally we perform l1 regularization on our neural network.\n\nfrom keras.regularizers import l1, l2\n\n\nset_seeds()\n\n\nmodel = Sequential()\nmodel.add(Dense(units=64, input_dim=len(df_X.columns), activation='relu', activity_regularizer=l1(0.0005)))\nmodel.add(Dense(units=32, input_dim=len(df_X.columns), activation='relu', activity_regularizer=l1(0.0005)))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n\n\n%%time\nmodel.fit(X_train, y_train, epochs=10, verbose=False);\n\nCPU times: user 29.2 s, sys: 1.75 s, total: 30.9 s\nWall time: 20.1 s\n\n\nOnce again, l1 regularization doesn’t seem improve performance that much.\n\nmodel.evaluate(X_test, y_test)\n\n602/602 [==============================] - 1s 873us/step - loss: 0.6925 - accuracy: 0.5205\n\n\n[0.6924766302108765, 0.5205045938491821]\n\n\n\nDiscussion Question: Based on our results so far, which model would you select?\n\n\n\n\n\n\n\nCode Challenge: Try some different neural architectures and see if you can improve model performance.",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Numerai</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "",
    "text": "47.1 Importing Packages\nIn this chapter we will use recurrent neural networks to predict financial time series related to the EUR/USD exchange rate.\nThis code is taken from Chapter 8 of Artificial Intelligence in Finance by Yves Hilpisch.\nLet’s begin by importing the packages we will need.\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8')\nimport tensorflow as tf\nnp.set_printoptions(suppress=True, precision=4)\nos.environ['PYTHONHASHSEED'] = '0'\n\n2023-09-02 14:13:55.462630: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-02 14:13:55.626494: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2023-09-02 14:13:55.627749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-09-02 14:13:56.625861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nLet’s set the random seeds for the various random generators that get used by tensorflow and keras.\ndef set_seeds(seed=100):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nset_seeds()\nThe TimeSeriesGenerator is used to transform raw data into an object that is suited for recurrent neural networks.\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nFinally, let’s import the layer objects that we will need. Notice two new layer types: SimpleRNN and LSTM.\nfrom keras.models import Sequential\nfrom keras.layers import SimpleRNN, LSTM, Dense",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#wrangling-data",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#wrangling-data",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "47.2 Wrangling Data",
    "text": "47.2 Wrangling Data\nNext, let’s wrangle our data.\n\nraw = pd.read_csv('../data/eur_usd.csv', parse_dates=['Date'])\nraw.set_index('Date', inplace=True)\nraw.head()\n\n\n\n\n\n\n\n\nHIGH\nLOW\nOPEN\nCLOSE\n\n\nDate\n\n\n\n\n\n\n\n\n2019-10-01 00:00:00\n1.0899\n1.0897\n1.0897\n1.0899\n\n\n2019-10-01 00:01:00\n1.0899\n1.0896\n1.0899\n1.0898\n\n\n2019-10-01 00:02:00\n1.0898\n1.0896\n1.0898\n1.0896\n\n\n2019-10-01 00:03:00\n1.0898\n1.0896\n1.0897\n1.0898\n\n\n2019-10-01 00:04:00\n1.0898\n1.0896\n1.0897\n1.0898\n\n\n\n\n\n\n\n\ndef generate_data():\n    data = pd.DataFrame(raw['CLOSE'])\n    data.columns = ['EUR_USD']\n    data = data.resample('30min', label='right').last().ffill()\n    return data\n\nNotice that we are taking 30 minute snapshots.\n\ndata = generate_data()\ndata.head()\n\n\n\n\n\n\n\n\nEUR_USD\n\n\nDate\n\n\n\n\n\n2019-10-01 00:30:00\n1.0899\n\n\n2019-10-01 01:00:00\n1.0896\n\n\n2019-10-01 01:30:00\n1.0892\n\n\n2019-10-01 02:00:00\n1.0890\n\n\n2019-10-01 02:30:00\n1.0886\n\n\n\n\n\n\n\nNow let’s apply Gaussian normalization.\n\ndata = (data - data.mean()) / data.std()\n\n\np = data['EUR_USD'].values\np = p.reshape((len(p), -1))\np\n\narray([[-2.7112],\n       [-2.7583],\n       [-2.8211],\n       ...,\n       [ 2.1877],\n       [ 2.1877],\n       [ 2.1877]])",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#fitting-to-eur_usd-directly",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#fitting-to-eur_usd-directly",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "47.3 Fitting to EUR_USD Directly",
    "text": "47.3 Fitting to EUR_USD Directly\nAs a first step, let’s use TimeseriesGenerator to put our data in the correct form.\n\nlags = 5\n\n\ng = TimeseriesGenerator(p, p, length=lags, batch_size=5)\nlist(g)[0]\n\n(array([[[-2.7112],\n         [-2.7583],\n         [-2.8211],\n         [-2.8525],\n         [-2.9153]],\n \n        [[-2.7583],\n         [-2.8211],\n         [-2.8525],\n         [-2.9153],\n         [-2.9153]],\n \n        [[-2.8211],\n         [-2.8525],\n         [-2.9153],\n         [-2.9153],\n         [-2.9153]],\n \n        [[-2.8525],\n         [-2.9153],\n         [-2.9153],\n         [-2.9153],\n         [-2.8839]],\n \n        [[-2.9153],\n         [-2.9153],\n         [-2.9153],\n         [-2.8839],\n         [-2.8525]]]),\n array([[-2.9153],\n        [-2.9153],\n        [-2.8839],\n        [-2.8525],\n        [-2.8682]]))\n\n\nThe following user-defined function allows the creation of an RNN with a SimpleRNN or LSTM layer.\n\ndef create_rnn_model(hu=100, lags=lags, layer='SimpleRNN', \n                     features=1, algorithm='estimation'):\n    model = Sequential()\n    if layer == 'SimpleRNN':\n        model.add(SimpleRNN(hu, activation='relu', input_shape=(lags, features)))\n    else:\n        model.add(LSTM(hu, activation='relu',input_shape=(lags, features)))\n    if algorithm == 'estimation':\n        model.add(Dense(1, activation='linear'))\n        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    else:\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n    return model\n\nNext we use the function above to create the RNN model.\n\nmodel = create_rnn_model()\n\nWe are now ready to fit the model.\n\n%%time\nmodel.fit(g, epochs=500, steps_per_epoch=10, verbose=False);\n\nCPU times: user 23.8 s, sys: 1.06 s, total: 24.9 s\nWall time: 15.4 s\n\n\n\ny = model.predict(g, verbose=False)\ny\n\narray([[-2.7377],\n       [-2.7582],\n       [-2.7608],\n       ...,\n       [ 2.1769],\n       [ 2.2127],\n       [ 2.227 ]], dtype=float32)\n\n\nLet’s take a look at our predictions.\n\ndata['pred'] = np.nan\ndata['pred'].iloc[lags:] = y.flatten()\ndata.head(10)\n\n\n\n\n\n\n\n\nEUR_USD\npred\n\n\nDate\n\n\n\n\n\n\n2019-10-01 00:30:00\n-2.711169\nNaN\n\n\n2019-10-01 01:00:00\n-2.758273\nNaN\n\n\n2019-10-01 01:30:00\n-2.821080\nNaN\n\n\n2019-10-01 02:00:00\n-2.852483\nNaN\n\n\n2019-10-01 02:30:00\n-2.915289\nNaN\n\n\n2019-10-01 03:00:00\n-2.915289\n-2.737728\n\n\n2019-10-01 03:30:00\n-2.915289\n-2.758190\n\n\n2019-10-01 04:00:00\n-2.883886\n-2.760781\n\n\n2019-10-01 04:30:00\n-2.852483\n-2.741888\n\n\n2019-10-01 05:00:00\n-2.868184\n-2.716055\n\n\n\n\n\n\n\nAt a first glance our predictions seem quite good.\n\n%matplotlib inline\ndata[['EUR_USD', 'pred']].plot(figsize=(10, 6), style=['b', 'r-.'],alpha=0.75);\n\n\n\n\n\n\n\n\nHowever, by zooming in we can see that our prediction of tommorrow’s price is essentially today’s price.\n\ndata[['EUR_USD', 'pred']].iloc[50:100].plot(figsize=(10, 6), style=['b', 'r-.'], alpha=0.75);",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#fitting-to-eur_usd-returns",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#fitting-to-eur_usd-returns",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "47.4 Fitting to EUR_USD Returns",
    "text": "47.4 Fitting to EUR_USD Returns\nIn this section we fit an RNN to the returns of EUR_USD. Let’s begin by wrangling our data.\n\ndata = generate_data()\ndata['r'] = np.log(data / data.shift(1)) # calculating returns\ndata.dropna(inplace=True)\ndata = (data - data.mean()) / data.std() # normalizing\n\nNext, using TimeseriesGenerator, let’s put the returns in the correct format to be fed into an RNN model.\n\nr = data['r'].values\nr = r.reshape((len(r), -1))\ng = TimeseriesGenerator(r, r, length=lags, batch_size=5)\n\nWe are now ready to create and fit our model.\n\nmodel = create_rnn_model()\n\n\n%%time\nmodel.fit(g, epochs=500, steps_per_epoch=10, verbose=False);\n\nCPU times: user 23.6 s, sys: 1.26 s, total: 24.9 s\nWall time: 15.3 s\n\n\n\ny = model.predict(g, verbose=False)\n\nLet’s examine our predictions.\n\ndata['pred'] = np.nan\ndata['pred'].iloc[lags:] = y.flatten()\ndata.dropna(inplace=True)\ndata.head()\n\n\n\n\n\n\n\n\nEUR_USD\nr\npred\n\n\nDate\n\n\n\n\n\n\n\n2019-10-01 03:30:00\n-2.918004\n-0.017915\n-0.053734\n\n\n2019-10-01 04:00:00\n-2.886578\n0.496771\n-0.019448\n\n\n2019-10-01 04:30:00\n-2.855152\n0.496676\n-0.114525\n\n\n2019-10-01 05:00:00\n-2.870865\n-0.275199\n-0.121304\n\n\n2019-10-01 05:30:00\n-2.855152\n0.239369\n-0.006001\n\n\n\n\n\n\n\nIf we examine the graph of our predictions versus ground-truth, we find that the order of magnitude of our predictions is off, but directionally the model doesn’t do too badly.\n\ndata[['r', 'pred']].iloc[50:100].plot(figsize=(10, 6), style=['b', 'r-.'], alpha=0.75);\nplt.axhline(0, c='grey', ls='--');\n\n\n\n\n\n\n\n\nLet’s check how often we are directionally correct.\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(np.sign(data['r']), np.sign(data['pred']))\n\n0.6704468133363575",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#holdout-set",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#holdout-set",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "47.5 Holdout Set",
    "text": "47.5 Holdout Set\nLet’s now check our out-of-sample directional correctness by creating a holdout set.\n\nsplit = int(len(r) * 0.8)\ntrain = r[:split]\ntest = r[split:]\n\n\ng = TimeseriesGenerator(train, train, length=lags, batch_size=5)\n\n\nset_seeds()\nmodel = create_rnn_model(hu=100)\n\n\n%%time\nmodel.fit(g, epochs=100, steps_per_epoch=10, verbose=False);\n\nCPU times: user 5.24 s, sys: 261 ms, total: 5.5 s\nWall time: 3.54 s\n\n\n\ng_ = TimeseriesGenerator(test, test, length=lags, batch_size=5)\n\n\ny = model.predict(g_)\n\n176/176 [==============================] - 0s 1ms/step\n\n\nAs we can see, our out-of-sample accuracy is still in the mid 60% range, which is quite good.\n\naccuracy_score(np.sign(test[lags:]), np.sign(y))\n\n0.6753986332574032",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#additional-features",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#additional-features",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "47.6 Additional Features",
    "text": "47.6 Additional Features\nNow, we will add a couple of additional features to our data set: momentum and volatility.\n\ndata = generate_data()\ndata['r'] = np.log(data / data.shift(1)) # calculating returns\nwindow = 20\ndata['mom'] = data['r'].rolling(window).mean() # adding momentum\ndata['vol'] = data['r'].rolling(window).std() # adding volatility\ndata.dropna(inplace=True)\ndata.head()\n\n\n\n\n\n\n\n\nEUR_USD\nr\nmom\nvol\n\n\nDate\n\n\n\n\n\n\n\n\n2019-10-01 10:30:00\n1.0900\n0.000734\n0.000005\n0.000388\n\n\n2019-10-01 11:00:00\n1.0899\n-0.000092\n0.000014\n0.000383\n\n\n2019-10-01 11:30:00\n1.0905\n0.000550\n0.000060\n0.000390\n\n\n2019-10-01 12:00:00\n1.0899\n-0.000550\n0.000041\n0.000410\n\n\n2019-10-01 12:30:00\n1.0893\n-0.000551\n0.000032\n0.000422",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#regression-with-new-features",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#regression-with-new-features",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "47.7 Regression with New Features",
    "text": "47.7 Regression with New Features\nIn this section, we use RNN to predict returns via regression with the addition of our new features. Let’s begin by creating a holdout set.\n\nsplit = int(len(data) * 0.8)\ntrain = data.iloc[:split].copy()\ntest = data.iloc[split:].copy()\n\nNext, we normalize our data.\n\nmu, std = train.mean(), train.std()\ntrain = (train - mu) / std\ntest = (test - mu) / std\n\nWe’ll use the TimeseriesGenerator to get our input data into the right format.\n\ng = TimeseriesGenerator(train.values, train['r'].values, length=lags, batch_size=5)\ng[0]\n\n(array([[[-2.6698,  1.9643, -0.035 ,  0.4127],\n         [-2.6866, -0.2643,  0.0911,  0.3915],\n         [-2.5858,  1.4682,  0.7215,  0.4216],\n         [-2.6866, -1.5018,  0.4695,  0.5101],\n         [-2.7874, -1.5026,  0.3436,  0.5606]],\n \n        [[-2.6866, -0.2643,  0.0911,  0.3915],\n         [-2.5858,  1.4682,  0.7215,  0.4216],\n         [-2.6866, -1.5018,  0.4695,  0.5101],\n         [-2.7874, -1.5026,  0.3436,  0.5606],\n         [-2.7538,  0.4786,  0.4697,  0.5661]],\n \n        [[-2.5858,  1.4682,  0.7215,  0.4216],\n         [-2.6866, -1.5018,  0.4695,  0.5101],\n         [-2.7874, -1.5026,  0.3436,  0.5606],\n         [-2.7538,  0.4786,  0.4697,  0.5661],\n         [-2.7706, -0.2645,  0.4067,  0.5703]],\n \n        [[-2.6866, -1.5018,  0.4695,  0.5101],\n         [-2.7874, -1.5026,  0.3436,  0.5606],\n         [-2.7538,  0.4786,  0.4697,  0.5661],\n         [-2.7706, -0.2645,  0.4067,  0.5703],\n         [-2.905 , -1.9989, -0.2243,  0.7099]],\n \n        [[-2.7874, -1.5026,  0.3436,  0.5606],\n         [-2.7538,  0.4786,  0.4697,  0.5661],\n         [-2.7706, -0.2645,  0.4067,  0.5703],\n         [-2.905 , -1.9989, -0.2243,  0.7099],\n         [-2.5522,  5.1832,  0.9736,  1.4687]]]),\n array([ 0.4786, -0.2645, -1.9989,  5.1832,  1.9615]))\n\n\nWe are now ready to create the model.\n\nset_seeds()\nmodel = create_rnn_model(hu=100, features=len(data.columns), layer='SimpleRNN')\n\nLet’s fit the model to our training data.\n\n%%time\nmodel.fit(g, epochs=100, steps_per_epoch=10, verbose=False);\n\nCPU times: user 5.48 s, sys: 307 ms, total: 5.79 s\nWall time: 3.74 s\n\n\nAnd now we can check our out-of-sample accuracy.\n\ng_ = TimeseriesGenerator(test.values, test['r'].values, length=lags, batch_size=5)\ny = model.predict(g_).flatten()\naccuracy_score(np.sign(test['r'].iloc[lags:]), np.sign(y))\n\n175/175 [==============================] - 0s 1ms/step\n\n\n0.6739130434782609",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#classification",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#classification",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "47.8 Classification",
    "text": "47.8 Classification\nIn this section we recast our prediction problem as a classification problem directly. We begin by instantiating our model.\n\nset_seeds()\nmodel = create_rnn_model(hu=50, features=len(data.columns), layer='LSTM', algorithm='classification')\n\nNext, we convert our numerical returns to binary labels.\n\ntrain_y = np.where(train['r'] &gt; 0, 1, 0)\n\nThe following code block creates a dict to adjust for class imbalance.\n\nnp.bincount(train_y)\n\narray([2374, 1142])\n\n\n\ndef cw(a):\n    c0, c1 = np.bincount(a)\n    w0 = (1 / c0) * (len(a)) / 2\n    w1 = (1 / c1) * (len(a)) / 2\n    return {0: w0, 1: w1}\n\nWe are now ready to fit our model.\n\ng = TimeseriesGenerator(train.values, train_y, length=lags, batch_size=5)\n\n\n%%time\nmodel.fit(g, epochs=5, steps_per_epoch=10, verbose=False, class_weight=cw(train_y));\n\nCPU times: user 1.39 s, sys: 53.3 ms, total: 1.44 s\nWall time: 1.21 s\n\n\nWe can now test our model out-of-sample.\n\ntest_y = np.where(test['r'] &gt; 0, 1, 0)\n\n\ng_ = TimeseriesGenerator(test.values, test_y,\n                         length=lags, batch_size=5)\n\n\ny = np.where(model.predict(g_, batch_size=None) &gt; 0.5, 1, 0).flatten()\n\n175/175 [==============================] - 0s 1ms/step\n\n\n\nnp.bincount(y)\n\narray([534, 340])\n\n\n\naccuracy_score(test_y[lags:], y)\n\n0.6086956521739131",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#deep-rnns",
    "href": "chapters/41_exchange_rate_rnn/exchange_rate_rnn.html#deep-rnns",
    "title": "47  Exchange Rate: Recurrent Neural Network",
    "section": "47.9 Deep RNNs",
    "text": "47.9 Deep RNNs\nIn this section we create a deep (multilayer) RNN with dropout regularization.\n\nfrom keras.layers import Dropout\n\nThis user defined function creates the deep RNN.\n\ndef create_deep_rnn_model(hl=2, hu=100, layer='SimpleRNN', \n                          optimizer='rmsprop', features=1,\n                          dropout=False, rate=0.3, seed=100):\n    if hl &lt;= 2: hl = 2\n    if layer == 'SimpleRNN':\n        layer = SimpleRNN\n    else:\n        layer = LSTM\n    model = Sequential()\n    model.add(layer(hu, input_shape=(lags, features), return_sequences=True,))\n    if dropout:\n        model.add(Dropout(rate, seed=seed))\n    for _ in range(2, hl):\n        model.add(layer(hu, return_sequences=True))\n        if dropout:\n            model.add(Dropout(rate, seed=seed))\n    model.add(layer(hu))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\nNext, we set the seeds and create the model.\n\nset_seeds()\nmodel = create_deep_rnn_model(hl=2, hu=50, layer='SimpleRNN', features=len(data.columns), dropout=True, rate=0.3)\n\nThis model summary give some information about the neural network that we have created.\n\nmodel.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_4 (SimpleRNN)    (None, 5, 50)             2750      \n                                                                 \n dropout (Dropout)           (None, 5, 50)             0         \n                                                                 \n simple_rnn_5 (SimpleRNN)    (None, 50)                5050      \n                                                                 \n dense_5 (Dense)             (None, 1)                 51        \n                                                                 \n=================================================================\nTotal params: 7851 (30.67 KB)\nTrainable params: 7851 (30.67 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nLet’s fit the model.\n\n%%time\nmodel.fit(g, epochs=200, steps_per_epoch=10, verbose=False, class_weight=cw(train_y));\n\nCPU times: user 16.7 s, sys: 1e+03 ms, total: 17.7 s\nWall time: 9.5 s\n\n\nNext we make our predictions.\n\ny = np.where(model.predict(g_, batch_size=None) &gt; 0.5, 1, 0).flatten()\n\n175/175 [==============================] - 0s 1ms/step\n\n\n\nnp.bincount(y)\n\narray([419, 455])\n\n\nUsing deep RNNs doesn’t seem to help performance.\n\naccuracy_score(test_y[lags:], y)\n\n0.5869565217391305",
    "crumbs": [
      "Neural Networks",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Exchange Rate: Recurrent Neural Network</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html",
    "title": "48  Introduction to spaCy",
    "section": "",
    "text": "48.1 Importing the Package and Language Model\nIf you want to do natural language processing (NLP) in Python, then look no further than spaCy, a free and open-source library with a lot of built-in capabilities. It’s becoming increasingly popular for processing and analyzing data in the field of NLP.\nThis chapter is an introduction to various aspects of the spaCy package. It is based on the following tutorial: https://realpython.com/natural-language-processing-spacy-python/\nThere are various spaCy models for different languages. The default model for the English language is designated as en_core_web_sm. Since the models are quite large, it’s best to install them separately — including all languages in one package would make the download too massive.\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nnlp\n\n&lt;spacy.lang.en.English&gt;",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#the-doc-object-for-processed-text",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#the-doc-object-for-processed-text",
    "title": "48  Introduction to spaCy",
    "section": "48.2 The Doc Object for Processed Text",
    "text": "48.2 The Doc Object for Processed Text\nIn this section, you’ll use spaCy to deconstruct a given input string.\nTo start processing your input, you construct a Doc object. A Doc object is a sequence of Token objects representing a lexical token. Each Token object has information about a particular piece — typically one word — of text. You can instantiate a Doc object by calling the Language object with the input string as an argument:\n\nintroduction_doc = nlp(\n    \"This tutorial is about Natural Language Processing in spaCy.\"\n)\n\nWe can check the type of the Doc object.\n\ntype(introduction_doc)\n\nspacy.tokens.doc.Doc\n\n\nWe can use a list comprehension to see all the tokens in the Doc object.\n\n[token.text for token in introduction_doc]\n\n['This',\n 'tutorial',\n 'is',\n 'about',\n 'Natural',\n 'Language',\n 'Processing',\n 'in',\n 'spaCy',\n '.']",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#sentence-detection",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#sentence-detection",
    "title": "48  Introduction to spaCy",
    "section": "48.3 Sentence Detection",
    "text": "48.3 Sentence Detection\nSentence detection is the process of locating where sentences start and end in a given text. This allows you to you divide a text into linguistically meaningful units. You’ll use these units when you’re processing your text to perform tasks such as part-of-speech (POS) tagging and named-entity recognition, which you’ll come to later in the tutorial.\nIn spaCy, the .sents property is used to extract sentences from the Doc object. Here’s how you would extract the total number of sentences and the sentences themselves for a given input.\nLet’s start with a simple two sentence piece of text.\n\nabout_text = (\n    \"Gus Proto is a Python developer currently\"\n    \" working for a London-based Fintech\"\n    \" company. He is interested in learning\"\n    \" Natural Language Processing.\"\n)\n\nWe can create a Doc object from about_text and then extract the sentences from it.\n\nabout_doc = nlp(about_text)\nsentences = list(about_doc.sents)\nlen(sentences)\n\n2\n\n\nLet’s print out the beginning of each sentence.\n\nfor sentence in sentences:\n    print(f'{sentence[:5]}...')\n\nGus Proto is a Python...\nHe is interested in learning...\n\n\nEach element of a sentence is a Span object.\n\ntype(sentences[0])\n\nspacy.tokens.span.Span",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#tokens-in-spacy",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#tokens-in-spacy",
    "title": "48  Introduction to spaCy",
    "section": "48.4 Tokens in spaCy",
    "text": "48.4 Tokens in spaCy\nBuilding the Doc container involves tokenizing the text. The process of tokenization breaks a text down into its basic units — or tokens — which are represented in spaCy as Token objects.\nAs you’ve already seen, with spaCy, you can print the tokens by iterating over the Doc object. But Token objects also have other attributes available for exploration. For instance, the token’s original index position in the string is available as an attribute on Token.\nLet’s begin with our same two sentence piece of text, create a Doc from it, and then print each token along with its index position.\n\nabout_text\n\n'Gus Proto is a Python developer currently working for a London-based Fintech company. He is interested in learning Natural Language Processing.'\n\n\n\nabout_doc = nlp(about_text)\nfor token in about_doc:\n    print(token, token.idx)\n\nGus 0\nProto 4\nis 10\na 13\nPython 15\ndeveloper 22\ncurrently 32\nworking 42\nfor 50\na 54\nLondon 56\n- 62\nbased 63\nFintech 69\ncompany 77\n. 84\nHe 86\nis 89\ninterested 92\nin 103\nlearning 106\nNatural 115\nLanguage 123\nProcessing 132\n. 142\n\n\nThere are many other pieces of information that can be gleaned from tokens. In the code below, we use list comprehensions and a pandas DataFrame to display some of this other information.\n\n.text_with_ws prints the token text along with any trailing space, if present.\n.is_alpha indicates whether the token consists of alphabetic characters or not.\n.is_punct indicates whether the token is a punctuation symbol or not.\n.is_stop indicates whether the token is a stop word or not. We’ll be covering stop words a bit later in this tutorial.\n\n\nimport pandas as pd\npd.DataFrame({\n    'text_whitespace': [str(token.text_with_ws) for token in about_doc],\n    'alphanumeric': [str(token.is_alpha) for token in about_doc],\n    'punctuation': [str(token.is_punct) for token in about_doc],\n    'stop_word': [str(token.is_stop) for token in about_doc],\n})\n\n\n\n\n\n\n\n\ntext_whitespace\nalphanumeric\npunctuation\nstop_word\n\n\n\n\n0\nGus\nTrue\nFalse\nFalse\n\n\n1\nProto\nTrue\nFalse\nFalse\n\n\n2\nis\nTrue\nFalse\nTrue\n\n\n3\na\nTrue\nFalse\nTrue\n\n\n4\nPython\nTrue\nFalse\nFalse\n\n\n5\ndeveloper\nTrue\nFalse\nFalse\n\n\n6\ncurrently\nTrue\nFalse\nFalse\n\n\n7\nworking\nTrue\nFalse\nFalse\n\n\n8\nfor\nTrue\nFalse\nTrue\n\n\n9\na\nTrue\nFalse\nTrue\n\n\n10\nLondon\nTrue\nFalse\nFalse\n\n\n11\n-\nFalse\nTrue\nFalse\n\n\n12\nbased\nTrue\nFalse\nFalse\n\n\n13\nFintech\nTrue\nFalse\nFalse\n\n\n14\ncompany\nTrue\nFalse\nFalse\n\n\n15\n.\nFalse\nTrue\nFalse\n\n\n16\nHe\nTrue\nFalse\nTrue\n\n\n17\nis\nTrue\nFalse\nTrue\n\n\n18\ninterested\nTrue\nFalse\nFalse\n\n\n19\nin\nTrue\nFalse\nTrue\n\n\n20\nlearning\nTrue\nFalse\nFalse\n\n\n21\nNatural\nTrue\nFalse\nFalse\n\n\n22\nLanguage\nTrue\nFalse\nFalse\n\n\n23\nProcessing\nTrue\nFalse\nFalse\n\n\n24\n.\nFalse\nTrue\nFalse",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#stop-words",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#stop-words",
    "title": "48  Introduction to spaCy",
    "section": "48.5 Stop Words",
    "text": "48.5 Stop Words\nStop words are typically defined as the most common words in a language. In the English language, some examples of stop words are the, are, but, and they. Most sentences need to contain stop words in order to be full sentences that make grammatical sense.\nWith NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. spaCy stores a set of stop words for the English language:\n\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\n\ntype(spacy_stopwords)\n\nset\n\n\n\nlen(spacy_stopwords)\n\n326\n\n\nLet’s observe a few of them.\n\nfor stop_word in list(spacy_stopwords)[:10]:\n    print(stop_word)\n\n‘re\namongst\nsometime\nthus\n‘ll\nhere\nrather\nnever\nduring\n‘m\n\n\nAs we can see below, we can remove stop words from text by making use of the .is_stop attribute of each token.\n\ncustom_about_text = (\n    \"Gus Proto is a Python developer currently\"\n    \" working for a London-based Fintech\"\n    \" company. He is interested in learning\"\n    \" Natural Language Processing.\"\n)\nabout_doc = nlp(custom_about_text)\n\nLet’s use a list comprehension with a conditional expression to produce a list of all the words that are not stop words in custom_about_text.\n\nprint([token for token in about_doc if not token.is_stop])\n\n[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#lemmatization",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#lemmatization",
    "title": "48  Introduction to spaCy",
    "section": "48.6 Lemmatization",
    "text": "48.6 Lemmatization\nLemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form, or root word, is called a lemma.\nFor example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma. The inflection of a word allows you to express different grammatical categories, like tense (organized vs organize), number (trains vs train), and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text.\nspaCy puts a lemma_ attribute on the Token class. This attribute has the lemmatized form of the token:\n\nconference_help_text = (\n    \"Gus is helping organize a developer\"\n    \" conference on Applications of Natural Language\"\n    \" Processing. He keeps organizing local Python meetups\"\n    \" and several internal talks at his workplace.\"\n)\nconference_help_doc = nlp(conference_help_text)\n\nLet’s print out all the words that are different from their lemma.\n\nfor token in conference_help_doc:\n    if str(token) != str(token.lemma_):\n        print(f'{str(token)} : {str(token.lemma_)}')\n\nis : be\nHe : he\nkeeps : keep\norganizing : organize\nmeetups : meetup\ntalks : talk\n\n\nNotice that this is not perfect, helping is not lemmaztized to help.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#word-frequency",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#word-frequency",
    "title": "48  Introduction to spaCy",
    "section": "48.7 Word Frequency",
    "text": "48.7 Word Frequency\nWe can now convert a given text into tokens and perform statistical analysis on it. This analysis can give you various insights, such as common words or unique words in the text. In this section we’ll do a simple word-frequency analysis.\n\ncomplete_text = (\n    \"Gus Proto is a Python developer currently\"\n    \" working for a London-based Fintech company. He is\"\n    \" interested in learning Natural Language Processing.\"\n    \" There is a developer conference happening on 21 July\"\n    ' 2019 in London. It is titled \"Applications of Natural'\n    ' Language Processing\". There is a helpline number'\n    \" available at +44-1234567891. Gus is helping organize it.\"\n    \" He keeps organizing local Python meetups and several\"\n    \" internal talks at his workplace. Gus is also presenting\"\n    ' a talk. The talk will introduce the reader about \"Use'\n    ' cases of Natural Language Processing in Fintech\".'\n    \" Apart from his work, he is very passionate about music.\"\n    \" Gus is learning to play the Piano. He has enrolled\"\n    \" himself in the weekend batch of Great Piano Academy.\"\n    \" Great Piano Academy is situated in Mayfair or the City\"\n    \" of London and has world-class piano instructors.\"\n)\ncomplete_doc = nlp(complete_text)\n\nLet’s first try counting word frequencies without removing stop words. Notice the prominence of uninformative works such as is and a.\n\nfrom collections import Counter\nCounter(\n    [token.text for token in complete_doc if not token.is_punct]\n).most_common(5)\n\n[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]\n\n\nSo next, let’s remove the stop words with a list comprehension.\n\nwords = [\n    token.text\n    for token in complete_doc\n    if not token.is_stop and not token.is_punct\n]\nprint(words)\n\n['Gus', 'Proto', 'Python', 'developer', 'currently', 'working', 'London', 'based', 'Fintech', 'company', 'interested', 'learning', 'Natural', 'Language', 'Processing', 'developer', 'conference', 'happening', '21', 'July', '2019', 'London', 'titled', 'Applications', 'Natural', 'Language', 'Processing', 'helpline', 'number', 'available', '+44', '1234567891', 'Gus', 'helping', 'organize', 'keeps', 'organizing', 'local', 'Python', 'meetups', 'internal', 'talks', 'workplace', 'Gus', 'presenting', 'talk', 'talk', 'introduce', 'reader', 'Use', 'cases', 'Natural', 'Language', 'Processing', 'Fintech', 'Apart', 'work', 'passionate', 'music', 'Gus', 'learning', 'play', 'Piano', 'enrolled', 'weekend', 'batch', 'Great', 'Piano', 'Academy', 'Great', 'Piano', 'Academy', 'situated', 'Mayfair', 'City', 'London', 'world', 'class', 'piano', 'instructors']\n\n\nNow, counting words is much more meaningful. We can guess that this text has a lot to do with Gus and natural language processing. Of course, this is a crude analysis and a lot of context is being excluded.\n\nCounter(words).most_common(5)\n\n[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#part-of-speech-tagging",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#part-of-speech-tagging",
    "title": "48  Introduction to spaCy",
    "section": "48.8 Part-of-Speech Tagging",
    "text": "48.8 Part-of-Speech Tagging\nPart of speech or POS is a grammatical role that explains how a particular word is used in a sentence. There are typically eight parts of speech:\n\nNoun\nPronoun\nAdjective\nVerb\nAdverb\nPreposition\nConjunction\nInterjection\n\nPart-of-speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.\nIn spaCy, POS tags are available as an attribute on the Token object:\n\nabout_text = (\n    \"Gus Proto is a Python developer currently\"\n    \" working for a London-based Fintech\"\n    \" company. He is interested in learning\"\n    \" Natural Language Processing.\"\n)\nabout_doc = nlp(about_text)\n\nIn the code below, two attributes of the Token class are accessed and printed using DataFrames and list comprehensions:\n\n.tag_ displays a fine-grained tag.\n.pos_ displays a coarse-grained tag, which is a reduced version of the fine-grained tags.\n\nWe also use spacy.explain() to give descriptive details about a particular POS tag, which can be a valuable reference tool.\n\npd.DataFrame({\n    'token': [token for token in about_doc],\n    'tag': [token.tag_ for token in about_doc],\n    'part_of_speech': [token.pos_ for token in about_doc],\n    'explanation': [spacy.explain(token.tag_) for token in about_doc],\n})\n\n\n\n\n\n\n\n\ntoken\ntag\npart_of_speech\nexplanation\n\n\n\n\n0\nGus\nNNP\nPROPN\nnoun, proper singular\n\n\n1\nProto\nNNP\nPROPN\nnoun, proper singular\n\n\n2\nis\nVBZ\nAUX\nverb, 3rd person singular present\n\n\n3\na\nDT\nDET\ndeterminer\n\n\n4\nPython\nNNP\nPROPN\nnoun, proper singular\n\n\n5\ndeveloper\nNN\nNOUN\nnoun, singular or mass\n\n\n6\ncurrently\nRB\nADV\nadverb\n\n\n7\nworking\nVBG\nVERB\nverb, gerund or present participle\n\n\n8\nfor\nIN\nADP\nconjunction, subordinating or preposition\n\n\n9\na\nDT\nDET\ndeterminer\n\n\n10\nLondon\nNNP\nPROPN\nnoun, proper singular\n\n\n11\n-\nHYPH\nPUNCT\npunctuation mark, hyphen\n\n\n12\nbased\nVBN\nVERB\nverb, past participle\n\n\n13\nFintech\nNNP\nPROPN\nnoun, proper singular\n\n\n14\ncompany\nNN\nNOUN\nnoun, singular or mass\n\n\n15\n.\n.\nPUNCT\npunctuation mark, sentence closer\n\n\n16\nHe\nPRP\nPRON\npronoun, personal\n\n\n17\nis\nVBZ\nAUX\nverb, 3rd person singular present\n\n\n18\ninterested\nJJ\nADJ\nadjective (English), other noun-modifier (Chin...\n\n\n19\nin\nIN\nADP\nconjunction, subordinating or preposition\n\n\n20\nlearning\nVBG\nVERB\nverb, gerund or present participle\n\n\n21\nNatural\nNNP\nPROPN\nnoun, proper singular\n\n\n22\nLanguage\nNNP\nPROPN\nnoun, proper singular\n\n\n23\nProcessing\nNNP\nPROPN\nnoun, proper singular\n\n\n24\n.\n.\nPUNCT\npunctuation mark, sentence closer\n\n\n\n\n\n\n\nBy using POS tags, you can extract a particular category of words. You can use this type of word classification to derive insights. For instance, you could gauge sentiment by analyzing which adjectives are most commonly used alongside nouns.\n\nnouns = []\nadjectives = []\nfor token in about_doc:\n    if token.pos_ == \"NOUN\":\n        nouns.append(token)\n    if token.pos_ == \"ADJ\":\n        adjectives.append(token)\n\n\nnouns\n\n[developer, company]\n\n\n\nadjectives\n\n[interested]",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#preprocessing-functions",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#preprocessing-functions",
    "title": "48  Introduction to spaCy",
    "section": "48.9 Preprocessing Functions",
    "text": "48.9 Preprocessing Functions\nTo bring your text into a format ideal for analysis, you can write preprocessing functions to encapsulate your cleaning process. For example, in this section, you’ll create a preprocessor that applies the following operations:\n\nLowercases the text\nLemmatizes each token\nRemoves punctuation symbols\nRemoves stop words\n\nA preprocessing function converts text to an analyzable format. It’s typical for most NLP tasks. Here’s an example:\n\ncomplete_text = (\n    \"Gus Proto is a Python developer currently\"\n    \" working for a London-based Fintech company. He is\"\n    \" interested in learning Natural Language Processing.\"\n    \" There is a developer conference happening on 21 July\"\n    ' 2019 in London. It is titled \"Applications of Natural'\n    ' Language Processing\". There is a helpline number'\n    \" available at +44-1234567891. Gus is helping organize it.\"\n    \" He keeps organizing local Python meetups and several\"\n    \" internal talks at his workplace. Gus is also presenting\"\n    ' a talk. The talk will introduce the reader about \"Use'\n    ' cases of Natural Language Processing in Fintech\".'\n    \" Apart from his work, he is very passionate about music.\"\n    \" Gus is learning to play the Piano. He has enrolled\"\n    \" himself in the weekend batch of Great Piano Academy.\"\n    \" Great Piano Academy is situated in Mayfair or the City\"\n    \" of London and has world-class piano instructors.\"\n)\ncomplete_doc = nlp(complete_text)\n\nNext we create a couple of functions. The first will help with filtering, the second will do some simple preprocessing.\n\ndef is_token_allowed(token):\n    return bool(\n        str(token).strip()\n        and not token.is_stop\n        and not token.is_punct\n    )\n\n\ndef preprocess_token(token):\n    return token.lemma_.strip().lower()\n\nNow we can use a list comprehension to filter and process the tokens in complete_doc.\n\ncomplete_filtered_tokens = [\n    preprocess_token(token)\n    for token in complete_doc\n    if is_token_allowed(token)\n]\n\nNote that complete_filtered_tokens doesn’t contain any stop words or punctuation symbols, and it consists purely of lemmatized lowercase tokens.\n\nprint(complete_filtered_tokens)\n\n['gus', 'proto', 'python', 'developer', 'currently', 'work', 'london', 'base', 'fintech', 'company', 'interested', 'learn', 'natural', 'language', 'processing', 'developer', 'conference', 'happen', '21', 'july', '2019', 'london', 'title', 'application', 'natural', 'language', 'processing', 'helpline', 'number', 'available', '+44', '1234567891', 'gus', 'helping', 'organize', 'keep', 'organize', 'local', 'python', 'meetup', 'internal', 'talk', 'workplace', 'gus', 'present', 'talk', 'talk', 'introduce', 'reader', 'use', 'case', 'natural', 'language', 'processing', 'fintech', 'apart', 'work', 'passionate', 'music', 'gus', 'learn', 'play', 'piano', 'enrol', 'weekend', 'batch', 'great', 'piano', 'academy', 'great', 'piano', 'academy', 'situate', 'mayfair', 'city', 'london', 'world', 'class', 'piano', 'instructor']",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41a_spacy_introduction/spacy_introduction.html#named-entity-recognition",
    "href": "chapters/41a_spacy_introduction/spacy_introduction.html#named-entity-recognition",
    "title": "48  Introduction to spaCy",
    "section": "48.10 Named Entity Recognition",
    "text": "48.10 Named Entity Recognition\nNamed-entity recognition (NER) is the process of locating named entities in unstructured text and then classifying them into predefined categories, such as person names, organizations, locations, monetary values, percentages, and time expressions.\nYou can use NER to learn more about the meaning of your text. For example, you could use it to populate tags for a set of documents in order to improve the keyword search. You could also use it to categorize customer support tickets into relevant categories.\nspaCy has the property .ents on Doc objects. You can use it to extract named entities:\n\npiano_class_text = (\n    \"Great Piano Academy is situated\"\n    \" in Mayfair or the City of London and has\"\n    \" world-class piano instructors.\"\n)\npiano_class_doc = nlp(piano_class_text)\n\nNotice that ent is a Span object with various attributes:\n\n.text gives the Unicode text representation of the entity.\n.label_ gives the label of the entity.\n\nspacy.explain gives descriptive details about each entity label.\n\npd.DataFrame({\n    'entity': [ent.text for ent in piano_class_doc.ents],\n    'label': [ent.label_ for ent in piano_class_doc.ents],\n    'explanation': [spacy.explain(ent.label_) for ent in piano_class_doc.ents],\n})\n\n\n\n\n\n\n\n\nentity\nlabel\nexplanation\n\n\n\n\n0\nGreat Piano Academy\nORG\nCompanies, agencies, institutions, etc.\n\n\n1\nMayfair\nGPE\nCountries, cities, states\n\n\n2\nthe City of London\nGPE\nCountries, cities, states",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Introduction to **spaCy**</span>"
    ]
  },
  {
    "objectID": "chapters/41b_nltk_introduction/nltk_introduction.html",
    "href": "chapters/41b_nltk_introduction/nltk_introduction.html",
    "title": "49  Introduction to nltk",
    "section": "",
    "text": "49.1 Tokenizing\nNatural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP.\nBefore you can analyze text data programmatically, you first need to preprocess it. In this chapter, you’ll take your first look at the kinds of text preprocessing tasks you can do with NLTK.\nThis chapter is based on the following tutorial: https://realpython.com/nltk-nlp-python/\nTokenizing refers to splitting up text, typically by word or by sentence. This will allow you to work with smaller pieces of text that are still relatively coherent and meaningful even outside of the context of the rest of the text. It’s your first step in turning unstructured data into structured data, which is easier to analyze.\nLet’s import the relevant parts of nltk so we can tokenize by word or sentence.\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nNext, let’s create an example string to tokenize.\nexample_string = \"\"\"\nMuad'Dib learned rapidly because his first training was in how to learn.\nAnd the first lesson of all was the basic trust that he could learn.\nIt's shocking to find how many people do not believe they can learn,\nand how many more believe learning to be difficult.\"\"\"\nWe use sent_token() to tokenize by sentences, which results in a list of three strings.\nsent_tokenize(example_string)\n\n[\"\\nMuad'Dib learned rapidly because his first training was in how to learn.\",\n 'And the first lesson of all was the basic trust that he could learn.',\n \"It's shocking to find how many people do not believe they can learn,\\nand how many more believe learning to be difficult.\"]\nNext, let’s tokenize by word, which also results in a list of strings.\nprint(word_tokenize(example_string))\n\n[\"Muad'Dib\", 'learned', 'rapidly', 'because', 'his', 'first', 'training', 'was', 'in', 'how', 'to', 'learn', '.', 'And', 'the', 'first', 'lesson', 'of', 'all', 'was', 'the', 'basic', 'trust', 'that', 'he', 'could', 'learn', '.', 'It', \"'s\", 'shocking', 'to', 'find', 'how', 'many', 'people', 'do', 'not', 'believe', 'they', 'can', 'learn', ',', 'and', 'how', 'many', 'more', 'believe', 'learning', 'to', 'be', 'difficult', '.']\nNotice that “It’s” was split at the apostrophe to give you ‘It’ and “‘s”, but “Muad’Dib” was left whole? This happened because NLTK knows that ’It’ and”’s” (a contraction of “is”) are two distinct words, so it counted them separately. But “Muad’Dib” isn’t an accepted contraction like “It’s”, so it wasn’t read as two separate words and was left intact.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Introduction to **nltk**</span>"
    ]
  },
  {
    "objectID": "chapters/41b_nltk_introduction/nltk_introduction.html#filtering-stop-words",
    "href": "chapters/41b_nltk_introduction/nltk_introduction.html#filtering-stop-words",
    "title": "49  Introduction to nltk",
    "section": "49.2 Filtering Stop Words",
    "text": "49.2 Filtering Stop Words\nStop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like ‘in’, ‘is’, and ‘an’ are often used as stop words since they don’t add a lot of meaning to a text.\nHere’s how to import the relevant parts of nltk in order to filter out stop words:\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n[nltk_data] Downloading package stopwords to /home/pritam/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nLet’s start with an example piece of text.\n\nworf_quote = \"Sir, I protest. I am not a merry man!\"\n\n\nwords_in_quote = word_tokenize(worf_quote)\nwords_in_quote\n\n['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']\n\n\nNext, let’s create a set of stop words.\n\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\n{'s', 'when', 'your', 'a', 'does', 'as', 'some', 'itself', 'so', \"couldn't\", 'no', \"don't\", 'd', 'she', 'the', 'in', 'after', 'shouldn', 'ma', 'against', 'not', 'to', 'weren', 'my', 'but', 'doing', 'with', 'himself', 'own', 'should', \"it's\", 'this', 'out', 'mustn', \"wasn't\", 'its', 'hasn', 'same', 'has', 'off', 'now', 'couldn', 'who', \"won't\", 'am', 'both', 'such', \"doesn't\", 'than', 'any', 'themselves', 'most', 've', 'about', 'herself', \"you've\", 'shan', 'them', \"isn't\", 'what', 'all', 'ourselves', 'will', \"shouldn't\", \"didn't\", 'under', 'few', 'll', \"that'll\", 'm', 'yourself', 'isn', 'aren', 'because', 'over', \"you'll\", 'for', 'nor', \"you'd\", 'hers', 're', 'further', 'can', 'those', 'been', 'through', 'then', 'that', 'was', 'they', 'being', 'very', 'wasn', 'their', \"aren't\", \"needn't\", 'there', 'o', 'ours', 'her', \"hadn't\", 'be', 'too', 'if', 'myself', 'these', 'below', 'hadn', 'why', 'are', 'on', 'by', 'and', 'yourselves', 'an', 'while', 'into', 'having', \"mightn't\", \"weren't\", 'it', 'me', 'at', 'just', 'other', 'our', 'you', 'do', 'did', 'before', 'i', 'more', 'each', 'down', 'again', 'how', 'where', 'won', \"she's\", 'doesn', 'is', 'from', 'didn', 'here', \"haven't\", \"shan't\", \"you're\", 'between', 'only', 't', 'once', 'or', 'above', 'whom', 'he', 'his', 'haven', 'mightn', 'don', 'ain', 'until', 'of', 'him', \"mustn't\", 'theirs', 'y', 'had', 'wouldn', 'during', 'up', 'have', \"hasn't\", \"should've\", 'which', 'needn', 'were', 'we', \"wouldn't\", 'yours'}\n\n\n\n# filtered_list = []\n# for word in words_in_quote:\n#     if word.casefold() not in stop_words:\n#         filtered_list.append(word)\n# filtered_list\n\nNow we can use a list comprehension to fileter out the words in stop_words. Notice we are using .casefold() to ignore whether a word is uppercase or lower case.\n\nfiltered_list = [\n    word for word in words_in_quote if word.casefold() not in stop_words \n]\nfiltered_list\n\n['Sir', ',', 'protest', '.', 'merry', 'man', '!']\n\n\nYou filtered out a few words like ‘am’ and ‘a’, but you also filtered out ‘not’, which does affect the overall meaning of the sentence.\nWords like ‘I’ and ‘not’ may seem too important to filter out, and depending on what kind of analysis you want to do, they can be.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Introduction to **nltk**</span>"
    ]
  },
  {
    "objectID": "chapters/41b_nltk_introduction/nltk_introduction.html#stemming",
    "href": "chapters/41b_nltk_introduction/nltk_introduction.html#stemming",
    "title": "49  Introduction to nltk",
    "section": "49.3 Stemming",
    "text": "49.3 Stemming\nStemming is a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.” Stemming allows you to zero in on the basic meaning of a word rather than all the details of how it’s being used. NLTK has more than one stemmer, but we’ll be using the Porter stemmer.\nHere’s how to import the relevant parts of nltk in order to start stemming:\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nNow that you’re done importing, you can create a stemmer with PorterStemmer():\n\nstemmer = PorterStemmer()\n\nLet’s create an example string to stem. Noticed that this is a bit of a contrived sentence that has a lot of different uses of the word discovery.\n\nstring_for_stemming = \"\"\"\nThe crew of the USS Discovery discovered many discoveries.\nDiscovering is what explorers do.\"\"\"\n\nBefore you we can start stemming, we need to tokenize by word.\n\nwords = word_tokenize(string_for_stemming)\nprint(words)\n\n['The', 'crew', 'of', 'the', 'USS', 'Discovery', 'discovered', 'many', 'discoveries', '.', 'Discovering', 'is', 'what', 'explorers', 'do', '.']\n\n\nCreate a list of the stemmed versions of the words in words by using stemmer.stem() in a list comprehension:\n\nstemmed_words = [stemmer.stem(word) for word in words]\nstemmed_words\n\n['the',\n 'crew',\n 'of',\n 'the',\n 'uss',\n 'discoveri',\n 'discov',\n 'mani',\n 'discoveri',\n '.',\n 'discov',\n 'is',\n 'what',\n 'explor',\n 'do',\n '.']\n\n\nThose results look a little inconsistent. Why would ‘Discovery’ give you ‘discoveri’ when ‘Discovering’ gives you ‘discov’?\nThe Porter stemming algorithm dates from 1979, so it’s a little on the older side. The Snowball stemmer, which is also called Porter2, is an improvement on the original and is also available through NLTK, so you can use that one in your own projects. It’s also worth noting that the purpose of the Porter stemmer is not to produce complete words but to find variant forms of a word.\nFortunately, you have some other ways to reduce words to their core meaning, such as lemmatizing, which you’ll see later in this tutorial. But first, we need to cover parts of speech.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Introduction to **nltk**</span>"
    ]
  },
  {
    "objectID": "chapters/41b_nltk_introduction/nltk_introduction.html#parts-of-speech",
    "href": "chapters/41b_nltk_introduction/nltk_introduction.html#parts-of-speech",
    "title": "49  Introduction to nltk",
    "section": "49.4 Parts of Speech",
    "text": "49.4 Parts of Speech\nPart of speech is a grammatical term that deals with the roles words play when you use them together in sentences. Tagging parts of speech, or POS tagging, is the task of labeling the words in your text according to their part of speech.\nIn English, there are eight parts of speech:\n\nNoun - a person, place, or thing.\nPronoun - replaces a noun.\nAdjective - gives information about what a noun is like.\nVerb - an action or a state of being.\nAdverb - gives information about a verb, an adjective, or another adverb.\nPreposition - gives information about how a noun or pronoun is connected to another word.\nConjunction - connects two other words or phrases.\nInterjection - is an exclamation.\n\nSome sources also include the category articles (like “a” or “the”) in the list of parts of speech, but other sources consider them to be adjectives. nltk uses the word determiner to refer to articles.\n\nfrom nltk.tokenize import word_tokenize\n\nLet’s create some text to tag.\n\nsagan_quote = \"\"\"\nIf you wish to make an apple pie from scratch,\nyou must first invent the universe.\"\"\"\n\nNext, let’s tokenize by word.\n\nwords_in_sagan_quote = word_tokenize(sagan_quote)\n\nNow call nltk.pos_tag() on the list of tokens/words:\n\nnltk.pos_tag(words_in_sagan_quote)\n\n[('If', 'IN'),\n ('you', 'PRP'),\n ('wish', 'VBP'),\n ('to', 'TO'),\n ('make', 'VB'),\n ('an', 'DT'),\n ('apple', 'NN'),\n ('pie', 'NN'),\n ('from', 'IN'),\n ('scratch', 'NN'),\n (',', ','),\n ('you', 'PRP'),\n ('must', 'MD'),\n ('first', 'VB'),\n ('invent', 'VB'),\n ('the', 'DT'),\n ('universe', 'NN'),\n ('.', '.')]\n\n\nWe can examine the meaning of all the parts of speech in the tag set.\n\nnltk.download('tagsets')\nnltk.help.upenn_tagset()\n\n$: dollar\n    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n'': closing quotation mark\n    ' ''\n(: opening parenthesis\n    ( [ {\n): closing parenthesis\n    ) ] }\n,: comma\n    ,\n--: dash\n    --\n.: sentence terminator\n    . ! ?\n:: colon or ellipsis\n    : ; ...\nCC: conjunction, coordinating\n    & 'n and both but either et for less minus neither nor or plus so\n    therefore times v. versus vs. whether yet\nCD: numeral, cardinal\n    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n    fifteen 271,124 dozen quintillion DM2,000 ...\nDT: determiner\n    all an another any both del each either every half la many much nary\n    neither no some such that the them these this those\nEX: existential there\n    there\nFW: foreign word\n    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n    terram fiche oui corporis ...\nIN: preposition or conjunction, subordinating\n    astride among uppon whether out inside pro despite on by throughout\n    below within for towards near behind atop around if like until below\n    next into if beside ...\nJJ: adjective or numeral, ordinal\n    third ill-mannered pre-war regrettable oiled calamitous first separable\n    ectoplasmic battery-powered participatory fourth still-to-be-named\n    multilingual multi-disciplinary ...\nJJR: adjective, comparative\n    bleaker braver breezier briefer brighter brisker broader bumper busier\n    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n    cozier creamier crunchier cuter ...\nJJS: adjective, superlative\n    calmest cheapest choicest classiest cleanest clearest closest commonest\n    corniest costliest crassest creepiest crudest cutest darkest deadliest\n    dearest deepest densest dinkiest ...\nLS: list item marker\n    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n    SP-44007 Second Third Three Two * a b c d first five four one six three\n    two\nMD: modal auxiliary\n    can cannot could couldn't dare may might must need ought shall should\n    shouldn't will would\nNN: noun, common, singular or mass\n    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n    investment slide humour falloff slick wind hyena override subhumanity\n    machinist ...\nNNP: noun, proper, singular\n    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n    Shannon A.K.C. Meltex Liverpool ...\nNNPS: noun, proper, plural\n    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n    Apache Apaches Apocrypha ...\nNNS: noun, common, plural\n    undergraduates scotches bric-a-brac products bodyguards facets coasts\n    divestitures storehouses designs clubs fragrances averages\n    subjectivists apprehensions muses factory-jobs ...\nPDT: pre-determiner\n    all both half many quite such sure this\nPOS: genitive marker\n    ' 's\nPRP: pronoun, personal\n    hers herself him himself hisself it itself me myself one oneself ours\n    ourselves ownself self she thee theirs them themselves they thou thy us\nPRP$: pronoun, possessive\n    her his mine my our ours their thy your\nRB: adverb\n    occasionally unabatingly maddeningly adventurously professedly\n    stirringly prominently technologically magisterially predominately\n    swiftly fiscally pitilessly ...\nRBR: adverb, comparative\n    further gloomier grander graver greater grimmer harder harsher\n    healthier heavier higher however larger later leaner lengthier less-\n    perfectly lesser lonelier longer louder lower more ...\nRBS: adverb, superlative\n    best biggest bluntest earliest farthest first furthest hardest\n    heartiest highest largest least less most nearest second tightest worst\nRP: particle\n    aboard about across along apart around aside at away back before behind\n    by crop down ever fast for forth from go high i.e. in into just later\n    low more off on open out over per pie raising start teeth that through\n    under unto up up-pp upon whole with you\nSYM: symbol\n    % & ' '' ''. ) ). * + ,. &lt; = &gt; @ A[fj] U.S U.S.S.R * ** ***\nTO: \"to\" as preposition or infinitive marker\n    to\nUH: interjection\n    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n    man baby diddle hush sonuvabitch ...\nVB: verb, base form\n    ask assemble assess assign assume atone attention avoid bake balkanize\n    bank begin behold believe bend benefit bevel beware bless boil bomb\n    boost brace break bring broil brush build ...\nVBD: verb, past tense\n    dipped pleaded swiped regummed soaked tidied convened halted registered\n    cushioned exacted snubbed strode aimed adopted belied figgered\n    speculated wore appreciated contemplated ...\nVBG: verb, present participle or gerund\n    telegraphing stirring focusing angering judging stalling lactating\n    hankerin' alleging veering capping approaching traveling besieging\n    encrypting interrupting erasing wincing ...\nVBN: verb, past participle\n    multihulled dilapidated aerosolized chaired languished panelized used\n    experimented flourished imitated reunifed factored condensed sheared\n    unsettled primed dubbed desired ...\nVBP: verb, present tense, not 3rd person singular\n    predominate wrap resort sue twist spill cure lengthen brush terminate\n    appear tend stray glisten obtain comprise detest tease attract\n    emphasize mold postpone sever return wag ...\nVBZ: verb, present tense, 3rd person singular\n    bases reconstructs marks mixes displeases seals carps weaves snatches\n    slumps stretches authorizes smolders pictures emerges stockpiles\n    seduces fizzes uses bolsters slaps speaks pleads ...\nWDT: WH-determiner\n    that what whatever which whichever\nWP: WH-pronoun\n    that what whatever whatsoever which who whom whosoever\nWP$: WH-pronoun, possessive\n    whose\nWRB: Wh-adverb\n    how however whence whenever where whereby whereever wherein whereof why\n``: opening quotation mark\n    ` ``\n\n\n[nltk_data] Downloading package tagsets to /home/pritam/nltk_data...\n[nltk_data]   Package tagsets is already up-to-date!\n\n\n\n# jabberwocky_excerpt = \"\"\"\n# 'Twas brillig, and the slithy toves did gyre and gimble in the wabe:\n# all mimsy were the borogoves, and the mome raths outgrabe.\"\"\"\n\n\n# words_in_excerpt = word_tokenize(jabberwocky_excerpt)\n\n\n# nltk.pos_tag(words_in_excerpt)",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Introduction to **nltk**</span>"
    ]
  },
  {
    "objectID": "chapters/41b_nltk_introduction/nltk_introduction.html#lemmatizing",
    "href": "chapters/41b_nltk_introduction/nltk_introduction.html#lemmatizing",
    "title": "49  Introduction to nltk",
    "section": "49.5 Lemmatizing",
    "text": "49.5 Lemmatizing\nLike stemming, lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like ‘discoveri’.\nNote: A lemma is a word that represents a whole group of words, and that group of words is called a lexeme.\nFor example, if you were to look up the word “blending” in a dictionary, then you’d need to look at the entry for “blend,” but you would find “blending” listed in that entry.\nIn this example, “blend” is the lemma, and “blending” is part of the lexeme. So when you lemmatize a word, you are reducing it to its lemma.\nHere’s how to import the relevant parts of nltk in order to start lemmatizing:\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nLet’s start with lemmatizing a plural noun:\n\nlemmatizer.lemmatize('scarves')\n\n'scarf'\n\n\n“scarves” gave you ‘scarf’, so that’s already a bit more sophisticated than what you would have gotten with the Porter stemmer, which is ‘scarv’. Next, create a string with more than one word to lemmatize:\n\nstring_for_lemmatizing = \"The friends of DeSoto love scarves.\"\n\nNow tokenize that string by word:\n\nwords = word_tokenize(string_for_lemmatizing)\nwords\n\n['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']\n\n\nCreate a list containing all the words in words after they’ve been lemmatized:\n\nlemmatized_words = [lemmatizer.lemmatize(word) for word in words]\nlemmatized_words\n\n['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']\n\n\nBut what would happen if you lemmatized a word that looked very different from its lemma? Try lemmatizing “worst”:\n\nlemmatizer.lemmatize('worst')\n\n'worst'\n\n\nYou got the result ‘worst’ because lemmatizer.lemmatize() assumed that “worst” was a noun. You can make it clear that you want “worst” to be an adjective:\n\nlemmatizer.lemmatize('worst', pos='a')\n\n'bad'",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Introduction to **nltk**</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html",
    "title": "50  Vectorizing Text",
    "section": "",
    "text": "50.1 Importing Packages\nMost machine learning techniques rely on numerical input data. Thus, the first step in any natural language processing exercise is to convert text data into numbers, in particular vectors/arrays of numbers.\nIn this chapter we consider three ways of vectorizing text data:\nLet’s begin by importing the packages that we will need.\nimport numpy as np\nimport pandas as pd",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#reading-in-data",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#reading-in-data",
    "title": "50  Vectorizing Text",
    "section": "50.2 Reading-In Data",
    "text": "50.2 Reading-In Data\nNext we read-in some labeled news data. In this data set, each financial headline is associated with a sentiment, positive or negative. We can think of the headlines as being the features, and the sentiment as being the label. In this chapter, we won’t be concerned with the labels, but rather how to turn our raw text features into meaningful numeric feature vectors.\n\ndf_headline = pd.read_csv('LabelledNewsData.csv', encoding='unicode_escape')\ndf_headline\n\n\n\n\n\n\n\n\ndatetime\nheadline\nticker\nsentiment\n\n\n\n\n0\n1/16/2020 5:25\n$MMM fell on hard times but could be set to re...\nMMM\n0\n\n\n1\n1/11/2020 6:43\nWolfe Research Upgrades 3M $MMM to ¡§Peer Perf...\nMMM\n1\n\n\n2\n1/9/2020 9:37\n3M $MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe ...\nMMM\n1\n\n\n3\n1/8/2020 17:01\n$MMM #insideday follow up as it also opened up...\nMMM\n1\n\n\n4\n1/8/2020 7:44\n$MMM is best #dividend #stock out there and do...\nMMM\n0\n\n\n...\n...\n...\n...\n...\n\n\n9465\n4/11/2019 1:24\n$WMT - Walmart shifts to remodeling vs. new st...\nWMT\n1\n\n\n9466\n4/10/2019 6:05\nWalmart INC $WMT Holder Texas Permanent School...\nWMT\n0\n\n\n9467\n4/9/2019 4:38\n$WMT $GILD:3 Dividend Stocks Perfect for Retir...\nWMT\n1\n\n\n9468\n4/9/2019 4:30\nWalmart expanding use of #robots to scan shelv...\nWMT\n1\n\n\n9469\n4/9/2019 4:11\n$WMT Walmart plans to add thousands of robot h...\nWMT\n1\n\n\n\n\n9470 rows × 4 columns",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#cleaning-up-headlines",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#cleaning-up-headlines",
    "title": "50  Vectorizing Text",
    "section": "50.3 Cleaning up Headlines",
    "text": "50.3 Cleaning up Headlines\nLet’s use this user-defined function to clean our data. In particular, we will lowercase all the words and remove punctuation.\n\nimport re\nimport string\n\ndef process_text(text):\n    text = str(text).lower()\n    text = re.sub(\n        f\"[{re.escape(string.punctuation)}]\", \" \", text\n    )\n    text = \" \".join(text.split())\n    return text\n\nNow we can use the .apply() method to clean all the headlines in a single line of code.\n\ndf_headline['clean_headline'] = df_headline['headline'].apply(process_text)\ndf_headline\n\n\n\n\n\n\n\n\ndatetime\nheadline\nticker\nsentiment\nclean_headline\n\n\n\n\n0\n1/16/2020 5:25\n$MMM fell on hard times but could be set to re...\nMMM\n0\nmmm fell on hard times but could be set to reb...\n\n\n1\n1/11/2020 6:43\nWolfe Research Upgrades 3M $MMM to ¡§Peer Perf...\nMMM\n1\nwolfe research upgrades 3m mmm to ¡§peer perfo...\n\n\n2\n1/9/2020 9:37\n3M $MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe ...\nMMM\n1\n3m mmm upgraded to ¡§peer perform¡¨ by wolfe r...\n\n\n3\n1/8/2020 17:01\n$MMM #insideday follow up as it also opened up...\nMMM\n1\nmmm insideday follow up as it also opened up w...\n\n\n4\n1/8/2020 7:44\n$MMM is best #dividend #stock out there and do...\nMMM\n0\nmmm is best dividend stock out there and down ...\n\n\n...\n...\n...\n...\n...\n...\n\n\n9465\n4/11/2019 1:24\n$WMT - Walmart shifts to remodeling vs. new st...\nWMT\n1\nwmt walmart shifts to remodeling vs new stores\n\n\n9466\n4/10/2019 6:05\nWalmart INC $WMT Holder Texas Permanent School...\nWMT\n0\nwalmart inc wmt holder texas permanent school ...\n\n\n9467\n4/9/2019 4:38\n$WMT $GILD:3 Dividend Stocks Perfect for Retir...\nWMT\n1\nwmt gild 3 dividend stocks perfect for retirees\n\n\n9468\n4/9/2019 4:30\nWalmart expanding use of #robots to scan shelv...\nWMT\n1\nwalmart expanding use of robots to scan shelve...\n\n\n9469\n4/9/2019 4:11\n$WMT Walmart plans to add thousands of robot h...\nWMT\n1\nwmt walmart plans to add thousands of robot he...\n\n\n\n\n9470 rows × 5 columns",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#word-counts---simple-example",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#word-counts---simple-example",
    "title": "50  Vectorizing Text",
    "section": "50.4 Word Counts - Simple Example",
    "text": "50.4 Word Counts - Simple Example\nThe simplest way of vectorizing text is by creating a vocabulary of all the unique words in the corpus (the collection of all headlines in our case) and then counting how many times each word is used in each headline.\nLet’s start with a simple example, a corpus of two headlines.\n\nsentences = [\n    'The stock price of google jumps on the earning data today',\n    'Google plunge on China Data!'\n]\n\nWe can use the CountVectorizer in sklearn to perform this kind of vectorization.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\nUsing the .fit_transform method of our CountVectorizer instance returns a sparse matrix with the word counts in it.\n\ntype(vectorizer.fit_transform(sentences))\n\nscipy.sparse._csr.csr_matrix\n\n\nHowever, we can view this as regular matrix as well. Notice that each headline is represented by a row, and each column represents a particular word.\n\nvectorizer.fit_transform(sentences).todense()\n\nmatrix([[0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1],\n        [1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0]])\n\n\nThe entries in the returned matrix are associated with the .vocabulary_ dict that is constructed when .fit_transform() is run. Our vocabulary consists of 12 words. The indexes associated with each work in the vocabulary correspond to the column number in the matrix representation above. Notice that the word 'the' appears twice in the first headline.\n\nvectorizer.vocabulary_\n\n{'the': 10,\n 'stock': 9,\n 'price': 8,\n 'of': 5,\n 'google': 3,\n 'jumps': 4,\n 'on': 6,\n 'earning': 2,\n 'data': 1,\n 'today': 11,\n 'plunge': 7,\n 'china': 0}",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#word-counts---full-data-set",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#word-counts---full-data-set",
    "title": "50  Vectorizing Text",
    "section": "50.5 Word Counts - Full Data Set",
    "text": "50.5 Word Counts - Full Data Set\nLet’s now vectorize our full headline data set with CountVectorizer. We begin by instantiating a new CountVectorizer, notice that we are explicitly setting the ngram_range input to the constructor. If we were to, for example, set ngram_range=(1, 2) our vectorizer would produce counts for all 1-grams (words) and 2-grams (two-word sequences).\n\nvectorizer = CountVectorizer(ngram_range=(1, 1))\n\nNow we run the .fit_transform() method on all of our headlines.\n\nfeatures = vectorizer.fit_transform(df_headline['clean_headline'])\n\nWe can see that our vocabulary consists of 9464 unique words.\n\nlen(vectorizer.vocabulary_)\n\n9464\n\n\nHere is a list of the first 100 words in our vocabulary_. The ordering is not important.\n\nprint(list(vectorizer.vocabulary_.keys())[:100])\n\n['mmm', 'fell', 'on', 'hard', 'times', 'but', 'could', 'be', 'set', 'to', 'rebound', 'soon', 'wolfe', 'research', 'upgrades', '3m', 'peer', 'perform', 'upgraded', 'by', 'stocks', 'insideday', 'follow', 'up', 'as', 'it', 'also', 'opened', 'with', 'nice', 'candle', 'that', 'closed', 'just', 'over', 'the', 'prior', 'day', 'high', 'and', 'th', 'is', 'best', 'dividend', 'stock', 'out', 'there', 'down', '40', 'in', '2019', 'xli', 'go', 'please', 'fallen', 'king', 'will', 'back', 'read', 'more', 'sign', 'for', 'updates', 'trading', 'economy', 'investing', 'mmmcelebrates', 'new', 'year', 'month', 'close', 'volume', 'above', 'long', 'term', 'support', 'resistance', 'off', 'flag', '180', 'baby', 'going', 'higher', 'mmmhasn', 'really', 'done', 'much', 'this', 'looks', 'like', 'series', 'of', 'highs', 'forming', 'recent', 'ab', 'rating', 'increased', 'neutral', 'at']\n\n\nOur full set of features is a matrix with 9470 rows, one for each of the headlines, and 9464 columns, one for each of the tokens in our vocabulary.\n\nfeatures.shape\n\n(9470, 9464)\n\n\nYou can see that the feature matrix is sparse, i.e. mostly zeros.\n\nfeatures.todense()\n\nmatrix([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]])",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#tfidf---simple-example",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#tfidf---simple-example",
    "title": "50  Vectorizing Text",
    "section": "50.6 Tfidf - Simple Example",
    "text": "50.6 Tfidf - Simple Example\nTF-IDF is a word frequency score that tries to highlight words that are interesting. In particular, it tries to identify words that are frequent in the document (for us a headline) but not across documents (the set of all headlines). Specifially:\n\\[\\begin{align*}\nTF &= \\frac{\\text{number of times term appears in the document}}{\\text{total number of terms in the document}} \\\\[12pt]\nIDF &= \\ln \\bigg( \\frac{\\text{number of documents}}{\\text{number of documents containing the term}} \\bigg) \\\\[12pt]\nTFIDF &= TF * IDF\n\\end{align*}\\]\nNotice that if a term appears in every document, then it gets a TF-IDF score of zero.\nThe specific implementation in sklearn is a bit more nuanced, and can be found in the documentation.\nLet’s explore TF-IDF weighting by way of our simple two headline corpus.\n\nsentences = [\n    'The stock price of google jumps on the earning data today',\n    'Google plunge on China Data!'\n]\n\nAs before, we simply import the TfidfVectorizer class from sklearn, instantiate it, and then use the .fit_transform() method.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nfeatures = vectorizer.fit_transform(sentences)\n\nWe can view the full vocabulary with the .get_feature_names_out() method.\n\nvectorizer.get_feature_names_out()\n\narray(['china', 'data', 'earning', 'google', 'jumps', 'of', 'on',\n       'plunge', 'price', 'stock', 'the', 'today'], dtype=object)\n\n\nAs above, the resulting features is a sparse matrix.\n\nfeatures\n\n&lt;2x12 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 15 stored elements in Compressed Sparse Row format&gt;\n\n\n\nfeatures.shape\n\n(2, 12)\n\n\nBut we can also view it as a dense matrix.\n\nfeatures.todense()\n\nmatrix([[0.        , 0.20964166, 0.29464404, 0.20964166, 0.29464404,\n         0.29464404, 0.20964166, 0.        , 0.29464404, 0.29464404,\n         0.58928809, 0.29464404],\n        [0.53309782, 0.37930349, 0.        , 0.37930349, 0.        ,\n         0.        , 0.37930349, 0.53309782, 0.        , 0.        ,\n         0.        , 0.        ]])",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#tfidf---full-data-set",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#tfidf---full-data-set",
    "title": "50  Vectorizing Text",
    "section": "50.7 Tfidf - Full Data Set",
    "text": "50.7 Tfidf - Full Data Set\nLet’s now create TF-IDF scores for our all the headlines in our data set.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nfeatures = vectorizer.fit_transform(df_headline['clean_headline'])\n\nWe have a total of 9464 words/tokens in our data set.\n\nlen(vectorizer.get_feature_names_out())\n\n9464\n\n\nLet’s view our features matrix. As you can see it is mostly zeros.\n\nfeatures.todense()\n\nmatrix([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]])",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#word-embeddings",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#word-embeddings",
    "title": "50  Vectorizing Text",
    "section": "50.8 Word Embeddings",
    "text": "50.8 Word Embeddings\nWhen using both CountVectorizer and TfidfVectorizer the resulting document matrix was sparse: each headline was represented by a row of length 9464, with most of entries of matrix having a value of zero. And we are dealing with a very small corpus; the problem worsens as you deal with more and more documents. Large, sparse matrices can cause computational strain and instability with a lot of machine learning techniques.\nAnother issue with the above vectorizers is that we are making an implicit assumption that all words are independents of one another, which clearly isn’t true when it comes to words in natural language.\nA tool that can help with both of these shortcomings of count-based vectorization is word embeddings. Word embeddings are meaningful vector representations of words such that similar/related words will have similar vector representations. There are a variety of ways training word embeddings, using deep learning or more standard statistical techniques (e.g. singular value decomposition). In practice, data scientists often use pre-trained word embeddings; popular ones that you will see are word2vec and Glove.\nFor our purposes, we will use the word embedding built into the spaCy library, which includes 20,000 words each represented by a 300-dimensional vector. In order to use it let’s first import spaCy and initialize.\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n\nLet’s see the vector representation of a the single word 'stock'.\n\ndoc = nlp('stock')\n\n\nfor token in doc:\n    print(token.vector)\n\n[-1.6763e+00 -3.4523e+00 -5.6915e+00  8.3472e+00  4.6024e+00  2.2767e+00\n  2.1329e+00  5.3314e+00 -6.0596e-01 -1.7710e+00  3.3695e+00  2.7222e-01\n -7.9550e+00 -2.9420e+00 -4.6784e+00 -3.8186e-01  7.3475e+00 -6.1266e-01\n -2.7009e-01 -3.9311e+00  2.2934e-03  3.7566e+00 -2.2156e+00 -1.7855e+00\n  1.3459e-01  1.8638e+00 -1.5185e+00 -5.1967e+00 -9.7408e-01 -5.9038e-01\n  3.5877e+00 -1.8775e+00 -6.5891e+00  1.9367e+00 -2.6755e+00 -1.4335e+00\n  4.5114e+00  5.2133e+00  1.8360e+00  3.1565e+00 -6.6352e-01  4.3622e+00\n  3.9877e+00  2.5044e-02 -6.4742e-01  4.4283e+00  2.9162e+00 -8.2397e-01\n  3.9111e+00  1.8230e+00  1.5662e+00 -2.8878e+00  5.9252e-01 -4.4401e+00\n -2.8798e+00  2.1201e+00 -1.9458e+00  9.7731e-01  2.2704e+00  4.8463e-01\n  4.6493e+00 -1.6039e+00  2.9300e+00  6.6515e-01  2.1048e-01 -2.3328e-02\n -3.1912e-01  1.7723e-01  4.1515e-01  6.2709e+00 -3.2902e+00 -2.3934e+00\n -6.9308e-01 -5.1802e-02  6.8780e-02  6.7266e+00 -1.6526e+00  1.5962e+00\n  1.7972e+00 -1.4440e+00  6.3026e-01  4.6175e+00  3.3603e+00 -1.1202e+00\n -1.8589e+00 -6.1743e+00  3.5384e+00  2.3665e+00 -9.8189e-01  4.1655e+00\n -2.4285e+00  7.5670e-01 -1.7252e+00 -1.5681e+00 -9.4928e-01 -1.2734e+00\n -1.6606e+00 -2.3173e+00  3.1477e+00  2.6673e+00  2.5674e+00 -3.0276e+00\n  6.4479e-01 -3.1643e+00  2.8483e+00  8.4654e+00 -3.3200e+00  1.2916e-01\n  1.7395e+00 -5.6147e+00  2.4686e+00  1.4046e+00 -1.0230e+00 -1.9217e+00\n -2.3065e+00  1.2972e+00 -9.6079e-01 -2.4651e+00  1.7355e+00  1.3315e-01\n  6.3507e+00 -2.1148e+00  7.8635e-01  7.2813e+00 -1.8639e+00 -1.5139e+00\n  4.4814e-01 -1.1461e+00  3.2486e+00  2.4795e+00  1.9326e+00 -1.9252e+00\n  3.6193e+00  2.4704e+00 -1.4097e+00  1.7363e+00  1.5612e+00 -4.6142e+00\n -9.3577e-01  1.3950e+00 -2.3507e+00 -1.0074e+00  1.1132e-03 -3.4705e+00\n -2.0218e+00  1.6153e+00  2.7409e+00  2.5575e+00 -1.1008e+00  2.2038e+00\n -8.0433e-01  4.0752e-01  2.4420e+00  4.2802e+00  1.4776e+00 -1.0761e+00\n  2.3264e+00  1.2229e+00  3.1610e+00 -1.3399e+00  1.1327e+00 -5.8715e-01\n  4.7389e+00 -3.1430e+00 -1.9311e+00  1.6331e+00 -3.0877e+00  3.8101e+00\n  3.7124e+00  2.3251e-02 -4.5838e+00  1.3238e-01 -2.0460e+00 -1.7474e+00\n -3.9481e+00  3.0944e+00 -9.5598e-01 -3.3975e+00 -2.8340e+00  3.8619e+00\n  2.8206e+00  3.5016e+00  1.1840e+00 -1.9005e+00 -1.3859e+00 -1.8144e+00\n -1.3462e+00 -3.5175e+00  2.0150e+00 -8.7826e-01  3.6011e+00  2.6540e+00\n -1.4246e+00 -1.2586e+00  6.5926e+00 -2.3686e+00  1.4223e-01  2.1828e+00\n -1.5752e+00  9.6421e-01 -3.9250e+00 -1.7863e+00  1.1700e+00 -4.5051e-01\n  2.3646e+00  5.7172e-01 -2.2555e+00  3.2422e+00  6.3018e+00  1.4312e+00\n -7.5344e-01  1.0371e+00  1.6506e-01 -3.0615e+00  3.8182e+00  5.7244e-01\n  2.2231e+00 -5.8718e+00 -3.3305e+00 -2.1815e-01  4.1500e-01 -3.3136e+00\n -7.2296e+00 -2.6852e+00  8.9851e-01 -3.0010e+00  3.3961e+00  4.4562e+00\n  1.2803e+00  1.0441e-01  4.8864e+00 -4.6059e+00 -8.2632e-01  2.5035e+00\n  1.7968e+00 -5.4523e-01 -6.4587e-01  1.5865e+00  9.7701e-01  3.9274e-01\n -3.3580e+00 -3.6542e+00 -2.1191e+00 -5.9851e-01 -2.2086e+00 -2.3177e+00\n  2.6334e+00  4.8581e+00  2.7726e+00 -1.5489e-01 -3.2130e+00  6.9719e-02\n -1.0392e+00 -3.1371e+00  2.1123e+00 -1.9825e+00 -1.3587e+00 -9.0296e-01\n -1.7229e+00  9.7099e-01 -2.8246e+00  8.9627e+00 -3.2845e+00 -6.5034e-01\n  5.7005e+00  1.7704e+00 -2.9753e+00 -4.8882e-01 -1.0026e+00  2.6338e+00\n  2.3477e+00  2.9636e+00 -3.1404e+00 -2.7079e+00 -2.7956e+00 -3.4416e+00\n -5.2232e+00 -1.2552e+00 -1.5143e+00  1.1435e+00  5.8795e+00  4.3935e-01\n -5.5546e-02  2.4019e+00  4.3607e+00  1.6001e-01 -2.4602e+00 -3.1904e+00\n -4.2920e+00  6.8796e-01 -1.4090e+00  1.2255e+00  4.3329e+00 -1.3141e+00\n -4.1550e+00 -3.5444e+00  9.1071e-01 -2.7001e+00 -1.9061e+00 -2.6581e+00]",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#word-embedding---simple-example",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#word-embedding---simple-example",
    "title": "50  Vectorizing Text",
    "section": "50.9 Word Embedding - Simple Example",
    "text": "50.9 Word Embedding - Simple Example\nNow let’s calculate the work embedding for each of the words in the first sentence of our simple two headline corpus.\n\nsentences = [\n    'The stock price of google jumps on the earning data today',\n    'Google plunge on China Data!'\n]\n\nFirst, we create a doc object.\n\ndoc = nlp(sentences[0])\n\nNext, let’s print out the vector representation of each of the words.\n\nfor token in doc:\n    print(token.vector)\n\n[-7.2681e+00 -8.5717e-01  5.8105e+00  1.9771e+00  8.8147e+00 -5.8579e+00\n  3.7143e+00  3.5850e+00  4.7987e+00 -4.4251e+00  1.7461e+00 -3.7296e+00\n -5.1407e+00 -1.0792e+00 -2.5555e+00  3.0755e+00  5.0141e+00  5.8525e+00\n  7.3378e+00 -2.7689e+00 -5.1641e+00 -1.9879e+00  2.9782e+00  2.1024e+00\n  4.4306e+00  8.4355e-01 -6.8742e+00 -4.2949e+00 -1.7294e-01  3.6074e+00\n  8.4379e-01  3.3419e-01 -4.8147e+00  3.5683e-02 -1.3721e+01 -4.6528e+00\n -1.4021e+00  4.8342e-01  1.2549e+00 -4.0644e+00  3.3278e+00 -2.1590e-01\n -5.1786e+00  3.5360e+00 -3.1575e+00 -3.5273e+00 -3.6753e+00  1.5863e+00\n -8.1594e+00 -3.4657e+00  1.5262e+00  4.8135e+00 -3.8428e+00 -3.9082e+00\n  6.7549e-01 -3.5787e-01 -1.7806e+00  3.5284e+00 -5.1114e-02 -9.7150e-01\n -9.0553e-01 -1.5570e+00  1.2038e+00  4.7708e+00  9.8561e-01 -2.3186e+00\n -7.4899e+00 -9.5389e+00  8.5572e+00  2.7420e+00 -3.6270e+00  2.7456e+00\n -6.9574e+00 -1.7190e+00 -2.9145e+00  1.1838e+00  3.7864e+00  2.0413e+00\n -3.5808e+00  1.4319e+00  2.0528e-01 -7.0640e-01 -5.3556e+00 -2.5911e+00\n  4.4922e+00  1.6574e+00  3.9794e+00 -4.3560e+00 -2.7266e+00  1.9581e+00\n -3.4842e+00 -3.9674e+00  3.2690e+00  6.6683e-01  3.9837e+00 -6.5997e+00\n  4.1630e+00  8.0338e+00  3.8102e-01  8.2656e+00  9.7061e-01 -5.0807e+00\n  4.9522e+00  7.5018e+00  3.8305e+00 -3.3233e+00  4.9126e+00  2.4189e-01\n  3.8218e+00 -3.9717e+00  2.4691e+00  1.3721e+01 -8.9664e+00  1.0610e+01\n  6.9425e-01 -1.1082e+01 -5.6883e+00  2.3287e+00  1.6451e+00  3.6006e+00\n  1.2588e-01 -6.1956e+00  1.1455e+01  5.6682e+00 -5.0251e-01 -9.8515e-01\n  8.8902e-02 -4.0213e+00  3.6134e+00 -9.0936e+00 -1.4555e+01 -2.5591e+00\n  4.0959e+00 -3.5929e-01  1.0219e+00  3.9402e+00  8.0495e-01 -3.6023e+00\n  2.6394e+00 -1.5258e-01 -2.6182e+00 -2.6268e-01 -2.1610e+00  2.3950e+00\n  6.8842e+00  3.6034e+00  1.8058e+00  2.4528e+00  4.4088e+00 -1.0598e+00\n  6.4964e+00  5.9196e+00 -1.0261e+00 -1.7013e+00 -4.4151e+00  4.3043e+00\n -1.7138e+00 -4.6690e+00 -5.5212e-01  5.3995e+00  1.8311e+00 -3.5820e-01\n -3.6578e-01 -2.8578e+00 -6.4639e+00 -3.2155e+00  6.7083e-01 -1.2800e+00\n  1.2782e+00  7.8274e-01  1.9839e-01 -1.4163e+00  2.1184e+00  1.5021e+00\n -1.8212e+00  1.6629e+00  4.0354e+00 -4.4648e+00 -3.4897e+00 -2.5765e+00\n -3.6317e+00 -4.1619e-02  4.8660e-01  2.0712e+00 -1.9166e+00 -3.4045e+00\n -7.6609e+00 -2.1940e+00 -2.3919e-03  8.4900e-01  1.3921e+00 -5.7830e+00\n  4.4739e+00  1.0642e+00  5.7864e+00  3.4643e+00 -5.9169e+00 -2.6925e+00\n -1.1271e-01 -6.0462e+00  3.9285e+00 -3.0423e+00 -6.9939e-02  2.2826e-01\n  8.0214e+00  2.2098e+00 -1.1049e+01  7.6001e-02 -1.5970e+00  2.0524e-01\n  2.8063e+00  3.5245e+00 -3.9300e+00 -9.7995e-01  4.0248e+00  1.8447e+00\n -2.0452e+00  1.1419e+00 -4.4600e-01 -9.5551e-01 -1.0224e+00  5.9224e+00\n -6.1688e+00 -8.3840e-01 -7.9102e+00 -8.9575e-02 -2.7741e-01  4.2703e+00\n  4.0212e+00 -1.1166e-01  2.5119e+00 -5.9635e+00 -1.2320e+00  2.8199e-01\n -4.1062e+00 -6.2923e-01 -5.2420e-01  2.5213e+00 -3.5094e+00  6.4333e+00\n  7.9466e+00 -3.3883e+00  5.2535e+00  9.4524e-02 -3.3336e+00  5.9621e+00\n -1.0794e+00 -6.0850e+00 -3.6071e+00 -3.8496e-01  7.6137e+00 -9.1081e+00\n -6.0037e+00 -2.4735e+00 -6.5050e-01 -6.3021e+00  8.5783e+00  1.7250e-01\n  4.3631e+00 -9.3439e+00  2.0984e-01  7.6900e-01  1.0763e+01  4.4598e-01\n -3.6584e+00 -3.0992e+00 -3.8868e+00  4.3337e+00 -5.8037e+00 -1.1337e+00\n -6.1562e+00  3.1820e-01 -1.0612e+00 -1.4809e+00  6.0373e+00  4.6015e-01\n -1.5530e+00 -1.0562e+00  5.8618e-01  3.4431e+00  4.5542e+00 -3.1881e+00\n -1.5832e+00  3.0859e+00  1.3061e+00 -8.0091e+00  7.7996e+00 -5.0644e+00\n  8.8719e+00  7.2337e-01 -1.2350e+00  1.6209e+00  7.8994e+00  1.0741e+01\n  8.1158e-01  9.0156e+00 -1.5913e+00 -5.3166e+00  3.5032e-01 -2.8850e+00]\n[-1.6763e+00 -3.4523e+00 -5.6915e+00  8.3472e+00  4.6024e+00  2.2767e+00\n  2.1329e+00  5.3314e+00 -6.0596e-01 -1.7710e+00  3.3695e+00  2.7222e-01\n -7.9550e+00 -2.9420e+00 -4.6784e+00 -3.8186e-01  7.3475e+00 -6.1266e-01\n -2.7009e-01 -3.9311e+00  2.2934e-03  3.7566e+00 -2.2156e+00 -1.7855e+00\n  1.3459e-01  1.8638e+00 -1.5185e+00 -5.1967e+00 -9.7408e-01 -5.9038e-01\n  3.5877e+00 -1.8775e+00 -6.5891e+00  1.9367e+00 -2.6755e+00 -1.4335e+00\n  4.5114e+00  5.2133e+00  1.8360e+00  3.1565e+00 -6.6352e-01  4.3622e+00\n  3.9877e+00  2.5044e-02 -6.4742e-01  4.4283e+00  2.9162e+00 -8.2397e-01\n  3.9111e+00  1.8230e+00  1.5662e+00 -2.8878e+00  5.9252e-01 -4.4401e+00\n -2.8798e+00  2.1201e+00 -1.9458e+00  9.7731e-01  2.2704e+00  4.8463e-01\n  4.6493e+00 -1.6039e+00  2.9300e+00  6.6515e-01  2.1048e-01 -2.3328e-02\n -3.1912e-01  1.7723e-01  4.1515e-01  6.2709e+00 -3.2902e+00 -2.3934e+00\n -6.9308e-01 -5.1802e-02  6.8780e-02  6.7266e+00 -1.6526e+00  1.5962e+00\n  1.7972e+00 -1.4440e+00  6.3026e-01  4.6175e+00  3.3603e+00 -1.1202e+00\n -1.8589e+00 -6.1743e+00  3.5384e+00  2.3665e+00 -9.8189e-01  4.1655e+00\n -2.4285e+00  7.5670e-01 -1.7252e+00 -1.5681e+00 -9.4928e-01 -1.2734e+00\n -1.6606e+00 -2.3173e+00  3.1477e+00  2.6673e+00  2.5674e+00 -3.0276e+00\n  6.4479e-01 -3.1643e+00  2.8483e+00  8.4654e+00 -3.3200e+00  1.2916e-01\n  1.7395e+00 -5.6147e+00  2.4686e+00  1.4046e+00 -1.0230e+00 -1.9217e+00\n -2.3065e+00  1.2972e+00 -9.6079e-01 -2.4651e+00  1.7355e+00  1.3315e-01\n  6.3507e+00 -2.1148e+00  7.8635e-01  7.2813e+00 -1.8639e+00 -1.5139e+00\n  4.4814e-01 -1.1461e+00  3.2486e+00  2.4795e+00  1.9326e+00 -1.9252e+00\n  3.6193e+00  2.4704e+00 -1.4097e+00  1.7363e+00  1.5612e+00 -4.6142e+00\n -9.3577e-01  1.3950e+00 -2.3507e+00 -1.0074e+00  1.1132e-03 -3.4705e+00\n -2.0218e+00  1.6153e+00  2.7409e+00  2.5575e+00 -1.1008e+00  2.2038e+00\n -8.0433e-01  4.0752e-01  2.4420e+00  4.2802e+00  1.4776e+00 -1.0761e+00\n  2.3264e+00  1.2229e+00  3.1610e+00 -1.3399e+00  1.1327e+00 -5.8715e-01\n  4.7389e+00 -3.1430e+00 -1.9311e+00  1.6331e+00 -3.0877e+00  3.8101e+00\n  3.7124e+00  2.3251e-02 -4.5838e+00  1.3238e-01 -2.0460e+00 -1.7474e+00\n -3.9481e+00  3.0944e+00 -9.5598e-01 -3.3975e+00 -2.8340e+00  3.8619e+00\n  2.8206e+00  3.5016e+00  1.1840e+00 -1.9005e+00 -1.3859e+00 -1.8144e+00\n -1.3462e+00 -3.5175e+00  2.0150e+00 -8.7826e-01  3.6011e+00  2.6540e+00\n -1.4246e+00 -1.2586e+00  6.5926e+00 -2.3686e+00  1.4223e-01  2.1828e+00\n -1.5752e+00  9.6421e-01 -3.9250e+00 -1.7863e+00  1.1700e+00 -4.5051e-01\n  2.3646e+00  5.7172e-01 -2.2555e+00  3.2422e+00  6.3018e+00  1.4312e+00\n -7.5344e-01  1.0371e+00  1.6506e-01 -3.0615e+00  3.8182e+00  5.7244e-01\n  2.2231e+00 -5.8718e+00 -3.3305e+00 -2.1815e-01  4.1500e-01 -3.3136e+00\n -7.2296e+00 -2.6852e+00  8.9851e-01 -3.0010e+00  3.3961e+00  4.4562e+00\n  1.2803e+00  1.0441e-01  4.8864e+00 -4.6059e+00 -8.2632e-01  2.5035e+00\n  1.7968e+00 -5.4523e-01 -6.4587e-01  1.5865e+00  9.7701e-01  3.9274e-01\n -3.3580e+00 -3.6542e+00 -2.1191e+00 -5.9851e-01 -2.2086e+00 -2.3177e+00\n  2.6334e+00  4.8581e+00  2.7726e+00 -1.5489e-01 -3.2130e+00  6.9719e-02\n -1.0392e+00 -3.1371e+00  2.1123e+00 -1.9825e+00 -1.3587e+00 -9.0296e-01\n -1.7229e+00  9.7099e-01 -2.8246e+00  8.9627e+00 -3.2845e+00 -6.5034e-01\n  5.7005e+00  1.7704e+00 -2.9753e+00 -4.8882e-01 -1.0026e+00  2.6338e+00\n  2.3477e+00  2.9636e+00 -3.1404e+00 -2.7079e+00 -2.7956e+00 -3.4416e+00\n -5.2232e+00 -1.2552e+00 -1.5143e+00  1.1435e+00  5.8795e+00  4.3935e-01\n -5.5546e-02  2.4019e+00  4.3607e+00  1.6001e-01 -2.4602e+00 -3.1904e+00\n -4.2920e+00  6.8796e-01 -1.4090e+00  1.2255e+00  4.3329e+00 -1.3141e+00\n -4.1550e+00 -3.5444e+00  9.1071e-01 -2.7001e+00 -1.9061e+00 -2.6581e+00]\n[  0.3557     1.3631    -4.0063     1.3047     3.4156     1.248\n   1.7272    -2.0908    -3.2055    -3.2169     6.1828    -1.5574\n  -7.9111     1.4029    -2.5331     1.4355     9.2825     0.20517\n   1.9476    -2.8229    -0.38039    0.12565   -5.5213     3.6202\n  -1.1251     3.7414    -1.2723    -5.3376     0.36756    0.62385\n   6.7624     2.7573    -3.9128    -2.246      5.2641     1.9675\n   6.608      6.9186     2.8072     9.9484    -5.2882     2.8732\n   5.3347     4.3527     2.718      3.0793     5.5571     3.2451\n   3.3494    -1.687     -1.0169    -0.46895   -3.3112    -8.4233\n  -2.9423     0.51347    2.6923    -1.4597     3.6991     4.3762\n   3.3966    -0.45581    1.8202    -6.0146     3.7722    -4.174\n  -2.8997     0.66879   -3.0293     6.1149     0.29193   -0.99942\n  -1.8822     1.8814     2.1888     9.9848     4.9072     0.9667\n   4.275      0.07853   -0.5207     0.19033    1.601     -2.6658\n  -6.9102    -3.2697    -0.91648    2.4396    -4.8288     0.28618\n   0.69694   -3.2119    -0.22179   -1.3917    -1.8215    -4.6469\n   1.2421    -2.9131    -0.054702  -1.733      9.2526    -4.0448\n  -1.6324    -2.593     -3.0746    10.777     -5.1556    -7.8206\n   0.84753    2.6883    -0.49437   -2.6319     1.123      5.272\n  -2.3725     3.7565     0.96254   -1.7556     2.821      2.1598\n   8.5098    -1.0541    -0.022254   2.4784     1.0896    -2.034\n   1.6052     2.592     -2.4199    -0.092394  -3.5682    -4.9826\n   2.9708     2.0527     1.4273    -4.0424    -1.006     -1.0825\n   6.134      0.29101    3.7386    -3.6462     0.95185   -0.32348\n  -1.0776     3.2268     5.2907    -1.7712     1.9233     1.8335\n  -5.6916     2.8653    -2.9311     6.3942     1.8634     3.1235\n   1.7475    -1.5834     0.60824   -4.6574     7.6431     1.0713\n   2.7241    -0.59305   -3.3593     4.0008    -0.96084    1.3567\n  -3.465     -2.9807    -6.5963    -5.8406    -2.8842     3.3399\n  -0.24778    5.2411    -1.1303     3.2103     1.1316    -0.05312\n   0.75668    6.3266     3.6192    -1.0126    -3.1958    -3.9106\n  -1.8257     0.28288    0.31673    5.1804     0.50546   -1.1171\n   0.96381   -2.5305     3.8939    -3.7867     3.4579     1.2807\n   3.5908    -0.61353   -2.9841    -3.5663    -4.5179     6.3934\n  -0.23934   -3.7291    -2.7146     6.3909    -0.96268   -4.7113\n   1.0771    -1.7987     4.224     -3.8159     4.5057    -0.63373\n   7.5862    -9.4451   -12.606     -0.45135   -0.45428   -2.998\n  -5.3169    -6.116      2.4852    -4.6526     0.026633  11.858\n   0.62126    3.0615     2.0035    -6.058      1.9203     4.5115\n   5.1407     4.838      0.78278    3.1427     1.6331     4.624\n  -6.6955    -9.6968    -1.6107     6.2106     0.59678   -3.4263\n   4.489      5.1999     2.7298     0.43538   -5.0758     0.69616\n   2.097     -2.1618     1.4015    -1.8726     1.4163     4.2153\n   1.4484    -0.33199   -2.9034     8.4834    -2.6578     0.35398\n   5.4085     0.19749   -2.2276     3.9314    -1.2313    -1.5076\n  -0.36787    1.5373    -3.3647    -0.8658    -3.1567    -4.8183\n  -2.7521    -1.8892    -2.5976    -2.2553     7.5713    -5.6404\n  -2.7381    -2.0259     6.1568     0.45229   -3.266     -1.9599\n  -2.1476     3.6248    -2.5404     2.0778     9.4407     1.4884\n  -3.2711     3.6786     1.8381    -3.5799    -2.7689     1.4255  ]\n[-12.667     -6.568     -0.61537    4.9492    22.389      2.0801\n   2.3228    11.243      9.9503    -8.4631    15.482      7.1873\n  -5.0023     0.78472   -0.5293     3.687     -0.25719    1.6776\n  -4.9576    -4.4928    -2.8919    -6.0213    -6.5132    12.358\n  14.433     -2.3726     7.4689     5.6612    -0.83442    8.3075\n   1.8904    -8.7446     2.9264   -12.088     -5.9273    -6.8889\n  -1.5864     6.1506    -4.5834     3.9353    -5.0807     1.7025\n  -6.686      2.1081    -7.6174     3.625      3.4636    10.978\n   2.6462     0.16217   -9.3867     9.8213    -0.88525   -9.6741\n   3.9125    -0.84682   -1.2667     1.8843     3.0548   -12.389\n  -2.1142    -3.0401     5.1057    -0.029813  -4.1447     4.6207\n  -3.556     -5.2786     5.2225     4.4204    -2.5684     4.42\n  -5.5672    -7.9199    -1.5292    -4.3125   -12.033     14.956\n -13.107      4.7516   -13.247     12.929     -1.7277     3.3617\n   5.4739     6.7001   -13.022    -11.402      1.5567     1.357\n  -8.5561    -2.7115     3.1369     1.2579    -5.3425   -12.055\n   3.3399    -1.3634    -6.4028     8.3323     7.5483     6.0207\n   2.711     17.107     -9.2652     6.2524    11.207     -8.1336\n  -0.63469    7.4826     5.9851     7.843    -10.373      2.8563\n   2.3344    -0.79072   10.623     13.145     -8.1474    -0.91547\n  -3.9093    -1.7907    12.998      4.3126    -7.9588   -12.959\n  -4.2593    -5.126      9.9037     1.1696     0.2393     5.3139\n   9.1737     5.1071     4.7454     8.6958    -4.7283    -7.3881\n  -5.353     -6.5763    -0.27503    2.7373     3.3596     5.2776\n   4.4331     2.6809    -8.1981    -3.8707    -2.5763     4.1216\n   2.0074     1.862     -3.4484     7.9878    -4.0771    -2.2948\n  -3.7501    -5.4446    -2.7036     3.7922     1.3469    11.879\n   6.4672     8.5983     3.8564    -0.67028    4.2223    -7.381\n   9.4206     6.688      2.8492   -11.221      9.7632    -3.3357\n  -7.2258    -4.9063     6.8963     5.2122    -7.218     -0.77411\n  -0.9108    -3.3884     5.7078    -0.5396    -0.52968  -10.289\n  -5.5984     9.9026     0.56485    0.033487  -2.4532   -13.305\n   7.4826    -8.3255     0.60487    5.0644    -1.2985     4.2398\n  -0.59503   -1.3722     1.6634    -5.8088    -7.5909    -3.5636\n  12.503      6.9943   -17.327     12.41      13.097     -1.6856\n   3.9169    14.395    -11.583     -2.9242     5.6316     4.5767\n   1.4372    -2.0338    -6.7832     5.1318    -1.8165    15.465\n  -0.87271    3.2441    -5.1716     5.3247     0.62725    5.6402\n   1.8897    -1.8938    11.417     -5.8655     3.8948    11.085\n   7.7894    18.608     -3.1164    -0.64892   -0.096788  -6.2979\n   7.0022    -0.21949   -2.4844     8.2987    -1.7999     4.5687\n -12.051     -5.9399    -3.8548     3.7942    12.426     -8.4208\n -16.285      1.4604     0.82687   -6.3887    10.292      3.2191\n  -3.3132     3.3773     3.5908    15.121     13.526      5.2244\n   2.6965     2.3931    -2.5204     2.4566     3.1298     7.3385\n  -3.8662     4.1851     9.5609    -7.1911     4.298      9.0596\n   8.894     -0.62772    4.0362     3.0782     0.82646    6.0641\n   8.9245     2.9563    -1.5334   -12.728     -1.9228     5.9504\n   7.8449    -7.1788    -5.2482     9.6856    -0.45098   -2.2526\n  -4.305      1.1597    -4.9613    -8.0021    -0.31712   -7.7062  ]\n[ 3.7562e+00  3.4803e-02  4.0002e+00 -5.2799e-01 -2.6371e+00  2.5117e+00\n  3.9916e-01 -1.0409e+00 -1.4306e+00  3.6571e-01  4.4401e+00  7.4257e+00\n -4.8991e+00  3.6940e+00  4.2101e+00 -2.5017e-01  5.0258e+00  7.6315e-01\n  1.1029e+00 -3.1254e+00  2.1470e+00 -1.4717e+00 -5.4909e-01 -2.4578e-01\n -2.8937e+00 -2.0240e+00 -6.5440e-01 -2.6084e+00 -1.1944e+00  4.4417e+00\n -1.9446e+00 -1.3871e+00 -3.1931e+00  1.2637e+00  2.9838e+00  1.4758e+00\n -7.2859e-01 -1.3881e+00  5.7673e+00  1.1898e+00  2.2369e+00 -5.0228e-01\n  1.4391e-01  3.0712e-01 -3.1392e-01  1.8436e+00  2.0167e+00  3.1291e-02\n  2.3520e+00 -1.2220e+00  8.0562e-01 -5.6068e-02 -4.1789e-01 -4.0340e+00\n -3.2475e+00  2.2961e+00  1.5997e+00  2.7329e+00 -5.1136e-01 -1.3352e+00\n  2.6919e+00  2.0845e+00  5.4995e-01  5.5354e-02 -4.5093e+00  1.7626e+00\n -1.4643e+00  2.4265e-01 -2.9281e+00  3.2560e+00 -1.1969e+00  8.5225e-01\n  3.3154e+00 -1.2883e+00  2.3801e+00  2.0882e+00  2.0996e+00 -6.0895e-01\n -1.6119e+00  3.6668e+00  3.2123e+00 -8.4523e-02 -1.0055e+00  2.6612e+00\n -2.0493e+00  2.1835e+00  4.4001e+00  3.7187e+00  4.2110e+00 -1.0782e+00\n -2.2224e-01  5.9114e-01  8.3816e-01  2.6305e+00 -1.1104e+00 -1.1614e+00\n  6.9444e-01 -2.1641e+00  1.2863e+00 -6.9882e-01  1.7770e+00  1.8824e+00\n -3.4370e+00  4.1212e-01  5.7856e-03  4.6937e+00 -9.1218e-01 -2.0725e+00\n -1.3350e+00  3.7038e+00 -9.9131e-02 -2.7486e-01  2.6787e+00 -5.7369e-01\n  4.5646e-02  3.0808e+00  3.9165e-01 -3.4704e+00 -8.5082e-01 -3.6921e-01\n -3.9185e+00 -2.1492e+00  1.5389e+00  3.8514e+00  3.0442e+00 -1.9820e+00\n  2.0034e+00  6.6019e-01 -2.9951e+00  1.4453e+00  3.9662e-01  3.0621e+00\n -3.7669e+00  1.8265e+00  1.4839e+00 -1.2520e-01 -1.4711e+00  1.5144e+00\n  1.4587e+00  1.0615e+00  1.2814e+00  1.0061e+00  2.1538e+00  1.4074e+00\n -3.2053e+00  7.2722e-01  2.6438e+00  2.4254e+00  1.5829e+00 -2.5160e-01\n -1.3759e+00  3.6925e+00  2.7448e+00 -1.1481e+00 -2.3910e-01 -4.7128e+00\n  3.1542e+00  2.3102e+00  1.8641e-01 -6.4117e-01 -1.7444e+00 -2.7944e+00\n -9.8485e-01  8.7328e-01 -3.1924e+00 -1.2416e+00  1.7226e+00  2.3581e+00\n -3.3320e+00 -2.1890e+00 -2.1090e+00 -4.5392e-01 -4.3516e-01  4.2681e+00\n  8.5564e-01  1.6462e-01 -8.7050e-02 -1.2204e-01  1.2532e+00 -3.8592e-01\n -3.9586e+00 -3.8751e-01 -2.7934e+00 -3.2662e+00  2.7568e+00 -5.6368e-01\n -1.7393e+00 -3.7245e+00  2.5803e+00 -1.3199e+00 -1.2522e+00  6.8973e-01\n  2.1079e-01  5.8928e-01 -2.6544e+00  3.0252e+00  1.2452e+00 -2.1458e+00\n  3.7247e+00  2.3491e+00 -4.1666e+00  2.4885e-01 -1.7296e+00 -2.0431e+00\n  4.4783e-01  3.1395e+00 -6.2119e-01  1.4993e+00  4.0942e+00 -2.3410e+00\n  3.0062e+00 -2.0455e+00  2.1208e-01  1.8874e+00 -2.4508e+00 -1.9091e+00\n -1.1851e+00 -3.1529e+00 -3.8249e-01  2.3645e+00 -2.8707e+00 -7.0267e-01\n -5.8342e-01  3.6364e-01  6.7889e-01 -5.3429e-01  4.2512e+00 -2.6974e+00\n  2.6619e+00  4.6094e-02 -1.9923e+00 -1.4211e+00 -4.9816e-01 -2.1013e-01\n  6.6052e-01  2.6483e+00  8.7052e-01  2.9066e-01  1.7776e+00  2.1811e+00\n -5.8400e+00 -2.7271e+00 -1.6872e+00  2.9853e+00  7.4001e-01  2.3683e+00\n  1.1071e+00 -2.2598e+00  4.1640e-01 -3.0519e+00 -1.5452e+00 -1.5713e+00\n -2.9552e+00 -4.4784e+00 -3.3045e+00 -1.5405e+00  6.7254e-01 -2.9037e-01\n -4.0867e-01  2.4950e+00  3.9767e+00 -1.4753e+00 -1.8757e+00 -7.3339e-02\n  9.0422e-01  5.2428e+00 -7.6204e-01  2.8163e+00 -1.2522e+00  1.8647e+00\n -8.1845e-01 -3.6948e+00  4.0117e-01 -3.2185e+00 -3.2638e+00 -3.6831e+00\n  2.0234e+00  2.7771e-01  1.4944e+00  7.8395e-01 -1.0725e+00 -2.5679e-01\n  5.2524e-01  1.8357e-01  9.5195e-01  3.1257e-01  2.1916e-01  9.9122e-01\n -2.2145e+00  1.3172e+00  2.1119e+00 -2.9454e+00  2.0363e+00  1.3672e+00\n  2.0630e+00  3.5270e+00  4.0723e+00 -2.7806e+00 -1.9204e+00  1.5667e-01]\n[ 2.0422e+00  3.6346e+00  4.9222e-01  7.4497e-02  1.1854e+00  2.8426e+00\n -4.2565e-02  5.6348e+00  9.4834e-01  4.0261e-01  4.3465e-01  1.7376e+00\n  4.0527e-01  1.1172e+00  5.4746e+00 -2.3276e+00  2.1381e+00  2.3085e+00\n  2.0555e+00 -1.7726e+00  4.2804e+00 -1.4200e+00  2.3052e+00 -6.0744e+00\n -1.4287e+00  1.2752e+00  3.5857e-01 -4.2725e+00  3.6192e+00  4.8287e+00\n -3.1139e-02 -9.0207e-01  3.7440e+00 -8.9083e-01 -2.0998e+00 -3.1481e+00\n  1.9391e+00  2.0281e-01  3.7052e+00  7.0365e-01 -1.1994e+00 -1.4727e+00\n  3.8257e+00 -6.9901e-01 -1.1809e+00  3.1240e-01  1.2199e+00 -2.2577e+00\n -1.8996e+00  6.4813e-01  4.5217e+00  4.2572e-01 -7.3895e-01  5.2566e+00\n -7.9290e-01 -6.7808e-01  1.8614e-01 -1.7170e-01  1.0292e-01  2.3485e+00\n  1.6793e-02 -3.6535e-01 -6.1471e+00 -9.5033e-01 -1.5207e+00  2.4138e+00\n -3.7263e+00 -1.5107e-01 -2.0475e+00  7.0573e-01  3.0160e-01  2.4095e-01\n  1.1373e+00 -1.3373e+00 -1.4344e+00  1.5159e+00  2.2226e-01  1.7311e+00\n  1.0218e+00  6.2252e-01 -2.1289e+00 -4.5954e+00  3.0004e+00  3.7559e+00\n -2.3800e-01 -2.2834e+00  4.3520e+00 -6.3055e-02 -2.9043e+00  1.0590e+00\n  2.8713e-01  2.2957e+00 -3.2069e+00 -2.6453e+00  9.0519e-01  5.1194e-01\n -9.7198e-01  3.6428e-01  1.7075e+00 -1.1570e+00 -9.8879e-03  5.3510e-01\n  4.8175e+00  2.1203e+00 -9.6235e-01  2.0231e-01 -7.4170e-01  1.5036e+00\n -2.2327e+00  3.7524e+00 -9.6491e-01 -1.5290e+00 -3.3576e+00 -8.4948e-02\n -1.4797e+00  1.3899e+00 -9.5358e-01  1.9259e+00  2.7260e-01  9.5617e-01\n -1.9593e+00  3.1726e-01 -2.7526e+00 -1.3472e-01 -1.8519e+00  7.7540e-02\n -1.7106e+00  1.0220e+00  2.1262e+00  4.3310e+00 -2.1949e-01 -1.9634e+00\n  4.3043e-01 -3.5626e-01  1.1517e+00 -1.0549e+00 -1.1410e+00 -1.0864e+00\n -9.8271e-01 -6.7784e-01  3.4532e-01 -3.0567e+00  3.3635e-01  3.5164e-01\n -1.2283e+00  1.6819e+00 -3.2943e-01  4.5687e-01 -1.9834e-01  3.5895e+00\n -1.2799e+00  9.1851e-05  2.6489e+00  5.6560e-02 -3.1150e-01  6.4388e-02\n  2.4537e+00  2.0087e+00  3.6751e+00  1.8458e+00  2.5777e+00 -1.0673e+00\n -4.0811e-01 -5.2377e+00 -2.4283e+00 -1.1013e+00  1.9618e-01 -6.7115e-01\n -6.1514e-02  1.6295e+00 -5.4137e-01  1.4206e+00  3.0398e+00 -1.1454e+00\n -2.9884e-01 -1.0886e+00 -6.1042e+00 -3.2982e+00 -2.5653e+00 -3.8787e+00\n  1.3304e+00 -4.2444e-01 -5.6405e-01  3.7064e+00 -4.0894e+00 -2.7264e-01\n  2.0102e+00 -3.8565e+00 -4.5422e+00 -2.8851e+00  1.7712e+00  2.6698e-01\n -5.8633e-01 -3.1803e+00  2.6766e+00 -3.2201e+00  3.0615e+00 -8.2156e-01\n -2.1781e+00  1.6617e+00  2.4462e+00  1.1216e+00 -1.4162e+00 -3.7063e+00\n -1.3161e+00  2.0183e+00  4.6751e-02 -2.2082e+00 -6.3190e-01  8.8167e-01\n -1.8496e+00  7.5423e-01 -2.1512e+00 -2.7285e+00 -8.0884e-01 -2.4515e+00\n  4.3430e+00  4.3969e-01 -3.2489e+00  1.0244e-01 -2.0012e+00 -2.5771e+00\n  1.8901e-01  1.7906e+00  1.2404e-01 -2.8066e-01  3.2975e+00  2.1132e+00\n  2.0694e-01  2.1942e+00 -3.5933e+00 -2.7637e+00  6.5172e-01  1.9192e+00\n -1.1689e+00  1.1859e+00 -6.4784e-01  4.1976e+00  3.3291e+00 -4.7149e-01\n  1.0472e+00 -8.6213e-01 -2.6966e+00 -9.9733e-01  4.7274e+00 -2.2283e+00\n -4.2499e+00  3.0542e+00 -2.1578e+00 -9.2449e-01 -3.6035e+00  1.0427e+00\n -2.2682e+00  6.7648e-01 -3.7834e+00  4.0890e+00 -1.2867e+00  1.4972e+00\n  1.7077e+00  3.9418e+00 -3.8720e-02  3.2017e-01 -6.5839e-03  2.3846e+00\n -6.6669e-01 -5.2288e+00 -3.6621e+00 -1.2506e+00 -8.0501e-01  1.2608e+00\n  1.0312e+00 -2.0205e+00 -1.6286e-01 -9.7795e-01 -5.6440e-01 -1.8761e+00\n -1.6149e+00 -1.0624e+00 -1.8890e+00  1.5189e-01 -3.7796e-02  9.4888e-01\n -5.9715e-01 -1.3438e+00  3.1229e+00  2.2294e+00 -3.7116e+00 -2.1821e+00\n -3.6491e+00  1.9087e-01 -5.3816e-01  3.1824e+00  1.9921e+00 -1.4301e+00\n -1.6458e-01  2.5330e+00 -5.1668e-02  1.0078e+00 -2.9046e-01 -1.6856e+00]\n[ -5.1644     0.37588  -12.979      6.4501    12.282     -6.894\n  -2.1295     6.3783     6.8268    -1.9674    -3.0459    15.604\n  -4.4114     2.6695     6.5306     7.9306     6.462     -4.5929\n   6.6779    -3.3701     0.7235   -14.288     11.688     -1.5876\n   4.6762    -0.12351    0.40614    1.6251    -9.0457     2.2636\n  -9.9742    -0.99827   -7.3057   -16.441     10.57      -8.9467\n  -6.1169     1.0966     1.1533     5.0548     6.9433     2.7976\n   4.0385   -10.254      1.6543    -2.6761     9.4312    -6.7178\n  -1.1029    12.858      0.16087    1.9361   -20.521     -9.4667\n  -3.79       7.6723     4.1902     4.4285    -7.7376     8.0799\n -13.336      4.2537     0.6928     5.3262   -12.469     -6.8207\n   0.94208   -0.65119   -8.0272     2.5939   -13.054     -6.9314\n -12.771     -3.9418     1.2421    -3.778      9.7632    -3.0477\n  -6.3561     0.32582    0.23982   -8.7349     1.7952     6.4242\n  -4.1705     0.31118   11.281      1.0591    -7.1079   -11.568\n  -2.4303    -4.8778    -1.9376     0.19282   -4.9268   -10.487\n   6.6852    -8.0822    -1.1265     4.6279    -1.0307    -2.5203\n  10.88       8.8841    10.141     -1.8135    -0.99151    1.7099\n  -4.9591   -13.291      8.7431     5.3298    -7.1581    -5.6555\n  11.085      1.7293    -2.6437    -6.3318    -2.0692    -8.9679\n  -0.96731   -5.0771     7.6998   -12.223      2.9264     5.1296\n  10.782      1.5613     5.2318    -6.3391    -4.2381     3.8213\n  11.121      5.038      6.2836     5.8484    -9.0569    -2.2652\n   4.5106     1.5482     7.0571    -4.4808    -5.1193    -3.8884\n   6.2556    11.714    -11.033     10.66       4.6303     3.4309\n  -3.7548    12.328     -3.8103    -5.1119    -1.6713    -0.29489\n  -0.83736    1.7867    -2.6334     8.4909     1.1187   -11.104\n  -1.1288    -0.4442    -7.2217     4.8274    -7.3283     6.6037\n   1.7418     8.3055    -9.1563    -6.2955     0.70904   -1.5316\n  -3.9727     2.1384   -10.73      -2.5498   -13.581    -13.251\n   7.9649     3.5058     4.5801    -1.2867     0.82607    2.4572\n   0.84826    1.9776     5.7439    -3.0336     7.0412    -2.1809\n  -2.5958    -5.3713    -6.036     -3.6573     2.3026     3.5773\n   2.8893     8.1103   -11.148    -10.017      3.0348   -12.237\n  11.329      3.1368   -12.82      13.55      -6.0564   -10.197\n  -0.13422   -2.2805    -4.8628     1.5579     2.9909    -2.8118\n  -5.1179     1.2207     5.3502    -0.53956   -2.9435    -6.3866\n   1.8024    -5.7598    -0.57791   -4.8671     0.19418   -0.51796\n   2.3978    -8.1706     5.201     -5.3545     3.8467    -1.9522\n   6.2328     3.1725    -7.438      3.9426    -4.1872    -2.1013\n  -5.882     -5.9477    -2.2833    -6.6005    -5.1908     2.3649\n  -2.5351    -2.9763   -11.257     10.302      5.0335     0.50626\n  -7.8366     4.5668     0.79684    1.8886     6.0998     5.546\n   5.6222     9.4147     8.7242    23.678     11.87       1.5342\n  10.826     -3.0042    -5.0521     3.8545    -3.1165     0.44016\n  -4.2141    -7.2481     1.9607    -7.6449     5.5237    -7.5878\n   9.7467     3.4266     3.5488    -1.7198    17.089      4.7331\n  -1.3297     7.8634    -4.4486     8.4561   -10.377      0.37455\n   2.3249     3.2802    -0.63255   -3.8492     2.7231     1.8247\n -11.873      0.32242   10.403     -0.083841  -5.4552   -11.212   ]\n[-5.1043e+00  2.3496e+00  3.2472e+00  2.8424e+00  1.1459e+01 -2.4137e+00\n  5.1057e-01  7.0312e+00  3.6459e+00  6.2332e-01  1.3633e+01  4.5813e+00\n -1.0584e+01  1.2630e+00  6.3362e-01  7.4645e+00  6.1468e+00  3.9474e-01\n  1.4378e+00 -4.1540e+00  2.0000e+00 -3.8488e+00  7.3414e-01  2.1209e+00\n  2.1068e+00  1.8713e+00 -7.8175e+00 -4.4352e+00  2.2135e-01  3.9262e+00\n  2.8473e+00  2.0265e+00 -1.8189e+00 -9.2866e+00 -8.2191e+00 -1.7172e+00\n -1.7196e+00  3.9313e+00  2.5888e+00  8.2825e-01  1.3177e+00  1.1566e+00\n -5.2680e-01 -5.3276e-01 -1.2835e+00 -2.4743e-01 -3.7231e+00 -4.5196e-01\n -3.3093e+00 -1.2523e+00 -3.0767e+00  8.5522e+00 -1.6251e+00 -5.5941e+00\n  5.3161e-01  5.5142e-01 -5.0327e-01  1.8156e+00 -2.9011e+00 -1.8576e+00\n  1.4638e+00 -5.4034e+00 -8.1768e-01 -2.0847e+00  1.6838e+00  1.6817e+00\n -6.3836e+00 -3.2180e+00  7.2152e+00  3.5279e+00 -2.3648e+00  4.0313e-01\n -5.7476e+00 -1.0917e+00 -1.1058e-01  2.7065e+00 -4.5004e+00  5.5528e+00\n -7.4396e+00 -1.8884e+00 -5.0159e+00  7.8553e-01 -4.2467e-02  3.2823e+00\n  5.8371e+00  2.8223e+00 -4.7819e+00 -4.0965e+00  3.8425e-01  1.3066e+00\n -1.5229e+00  4.3565e-01  3.8789e+00 -4.8557e+00  5.8081e+00 -7.7726e-01\n  4.3211e+00  4.6135e+00  4.2278e+00  5.8454e+00  4.0931e+00  7.6313e-01\n  8.2298e+00  3.2500e+00  1.1713e+00  4.9618e+00  3.8678e+00 -4.8282e+00\n -9.8280e-01 -3.0029e+00  2.0229e+00  4.2098e+00 -7.2674e+00  5.8495e+00\n  1.1730e+00 -1.5662e+00  1.4491e+00  1.4227e+00  1.2867e-01  4.0703e+00\n  4.9251e-01 -4.4845e+00  4.0733e+00  3.0339e+00 -3.3840e+00 -5.2706e+00\n  3.3339e+00 -7.2218e+00  5.6753e+00 -4.5924e+00 -1.1599e+01 -1.0203e+00\n  5.8945e+00  1.7762e+00 -1.0363e+00  2.5350e+00  1.4402e+00 -3.1572e+00\n  2.4663e+00 -4.5771e+00 -7.4574e+00 -6.9813e-01 -1.9212e+00  6.5128e+00\n  6.4388e+00  6.4317e+00 -6.9971e+00  1.8715e+00  6.2365e+00 -2.5091e-01\n -1.7363e+00  9.5132e+00  1.4306e+00 -2.5112e+00  4.0763e-01  3.7422e+00\n  8.8706e-01 -4.8168e+00 -7.4616e-01 -7.8736e-02 -7.9536e-01 -6.0894e+00\n  2.7413e+00  2.1358e+00 -2.7505e+00 -1.1987e+00 -6.4908e+00  1.3424e+00\n  1.4893e+00 -1.8716e+00  2.8296e+00 -5.3080e+00  5.1784e+00 -1.8339e-01\n  3.5250e-01  1.8829e+00  6.9355e-01 -4.1332e+00 -2.2120e+00 -2.8912e-01\n -5.2159e+00 -3.3565e+00 -2.6745e-01  3.9148e+00 -2.7248e+00  3.7187e-01\n -6.4571e+00 -9.2446e-02  2.0403e+00  5.5114e-01  1.1552e+00 -1.9635e+00\n  7.6531e+00 -3.4696e+00  4.7709e+00  1.0621e+00 -6.7008e+00 -1.9478e+00\n -2.0715e+00 -5.2312e-01  1.4389e+00 -5.0093e+00  4.3909e-01 -7.2692e+00\n  8.7121e+00 -7.6601e-01 -8.1995e+00  1.4594e+00 -2.6537e-01  3.5587e+00\n  5.4031e+00  3.7896e+00 -9.2572e+00  3.4277e+00  2.5215e+00  4.1383e+00\n  1.8763e-01 -4.2169e+00  4.0743e-01 -1.1800e+00 -5.2174e+00  8.7247e+00\n -2.5949e+00  1.4874e+00 -7.0560e+00 -1.6384e+00  2.9549e-03  6.3286e+00\n  1.4910e+00  4.4328e-01  6.1122e+00 -8.0740e+00 -1.7362e+00  6.3054e+00\n  2.6566e+00 -6.5135e-01 -9.6859e-01  5.0822e-01 -1.1677e+00 -4.1731e-01\n  1.4866e+00  1.3182e+00  1.3838e+00  6.2296e-01  1.3744e+00  5.9588e+00\n -5.2347e+00 -2.6428e+00  9.5187e-01 -5.3128e-01  2.2787e+00 -1.0626e+01\n -1.2064e+01 -1.6146e-02  2.7706e+00 -4.3249e+00  6.7169e+00 -2.9383e-01\n  1.1804e+00 -1.6421e+00  3.7709e+00  1.2545e+01  9.6831e+00  3.1599e+00\n  3.0962e+00 -4.5380e+00 -5.6059e-01  4.3790e+00 -7.2785e+00  2.7534e-01\n  1.0549e-02 -2.9399e-01  8.3850e-01 -3.7402e+00  3.6582e+00  3.0861e-01\n  1.3038e-01 -5.0155e+00 -7.4336e-01  3.2874e+00  1.0577e-01  3.7003e+00\n  2.4759e+00  3.9932e+00  2.7957e+00 -4.1682e+00  3.0442e+00  1.2622e+00\n  1.0991e-01 -3.9761e-01  1.3023e-01  1.3929e+00  3.3822e-01  7.0100e+00\n -1.6916e+00  3.6008e+00 -1.2658e+00 -7.6875e+00 -2.5128e+00  6.9342e-01]\n[-2.6573    -2.2333     0.24297    1.0893     2.6696     2.8717\n  2.3943     4.515      1.285     -2.1434     7.1599     2.209\n -3.2531     1.2878    -1.5959     2.762      2.1417     2.035\n -2.3278     4.2454     0.72451    3.1563    -1.8469     0.60129\n  2.1329    -1.4862    -2.4525     0.82174    1.1064     2.9941\n  2.2801     0.97022   -0.94543   -4.0298     2.116      3.0328\n  4.4398     1.6577    -1.5098     1.0423    -1.6867    -1.1772\n  0.05132    1.9956    -4.4841     1.8187    -0.07756   -1.211\n  1.7472     0.55819   -1.7786     5.3067    -3.8526    -1.6331\n -0.48314    2.7069    -2.0914     1.8936     0.8116     0.53803\n  2.1251     3.605      2.0399    -0.2849     6.1807     1.3722\n -1.8512    -1.1725     1.2391     4.0021     1.341     -1.9913\n -0.38604    1.4614     0.75746    1.9254     0.32269    2.0928\n  0.85716    0.23886   -2.5459    -0.26838   -0.69352    2.4137\n -1.4295     2.3808    -3.3158    -4.2146    -1.9733     2.7123\n  0.99122    2.1041     1.4529    -2.9534    -1.0998    -1.9947\n -3.3274    -3.3428     3.3945     0.14719    2.2184    -3.5532\n  2.7752     4.1941    -2.3031     7.1906    -1.4513    -4.6422\n -4.3084    -4.7511     4.0886     0.85352   -6.8752    -0.36999\n -0.82232    2.9039    -3.5014     2.4729     0.089301  -0.69415\n -2.1801    -1.4528     0.20236   -1.7719     0.062285  -2.0696\n  0.15126   -0.95776    0.17609   -1.7372     2.6623    -0.43102\n  4.6213     1.996     -0.8106     2.1821    -2.6667     0.5609\n  0.068      0.17101    0.95314    2.9501    -1.1563     4.6308\n -0.34141    3.1117    -7.2582    -3.9503     2.181      2.1172\n -2.6645     0.69881    4.0974     4.5322    -1.5001    -0.84013\n  3.2086     3.8295    -0.48351   -5.7707     0.42409   -0.85428\n  0.81668   -1.1723     0.055825  -1.4835     0.84162   -0.89602\n -1.3832     2.0191    -0.19713    3.2358    -0.54111   -2.4256\n  3.203      1.8766    -1.2918    -0.024341  -2.332     -1.931\n -4.9661    -0.050245   4.5246    -0.28508   -4.0573     5.5033\n  1.0877    -0.4062     1.3483     2.0855     3.5651     1.7818\n -4.4976    -2.294      0.85486   -2.7657    -5.3997    -4.826\n  0.63104    0.63905    0.70116   -0.0085923 -1.9124    -1.1782\n  6.7943    -1.0679    -4.746      2.576      2.4306    -2.6342\n  1.9347     1.5216     0.2828     4.0907     1.9828     1.1614\n  2.5657    -2.5352    -3.8284     2.2401    -1.6832     2.0061\n  2.5767    -2.1239    -0.45356    0.3444     1.172      0.50635\n  1.0608     2.6211     4.4951    -1.6358    -3.7337     4.3311\n  0.019937   3.3425     1.1097    -0.14822   -4.3812     2.5143\n -5.0601    -0.59193   -0.073379   3.8859     2.0429     1.6931\n -1.979     -0.53888    0.23195    0.22736    0.3455    -1.0801\n  0.74794    2.2799    -1.2702     0.35309    3.7984    -1.8064\n  1.5222     2.9657    -0.92432    3.2568     0.43544    2.3663\n  0.76823   -2.6215    -2.4762     1.4601     0.97098    0.43849\n  0.035409   0.78302    3.9008    -0.51365   -3.7536    -0.98216\n -1.9535    -2.144      1.2227     0.81662    6.4869     1.0549\n  0.69135   -1.3061    -0.90899   -0.54174    2.018      0.30658\n -4.6192    -3.1059    -0.51138    2.1492    -3.7527    -1.5615\n  0.0081658  2.1216     0.82213   -0.45668   -1.9818     0.64121  ]\n[  8.433     -0.62196    4.5699     4.2771     7.5471    -3.4932\n  -2.7744    11.539     -0.41976   -5.7145    13.106      5.9285\n  -5.0834     2.0165    -1.3187    -3.1088     8.6808    -0.37767\n -12.834     -7.7306    -1.3145     0.19578  -12.683     -4.448\n -12.181     -6.8301     3.1909    -4.2021    -1.1167     5.6552\n   4.5593     1.1842    -8.839      5.1291     8.8025    -7.7602\n  -7.9684    -7.8937    10.286      5.7079     3.1926    -0.52753\n  -2.0976     6.7266    -5.971     -0.52903    8.3433    -1.7521\n   4.4892    -2.456      5.7153    -0.29028   -2.1078    -4.7923\n  -2.8459     2.5139    -7.3703     0.25596    5.1344   -12.615\n   5.0602     6.3371    -6.1839     0.71782    5.8044     4.7112\n   0.20968   -3.2077    -0.62559    0.22191    2.8759     5.6576\n  -4.5906     5.0782    -4.0207     2.1161     1.1608    -0.98217\n  -3.0099    -4.3483    -3.1446     0.84766    1.8647     3.0901\n   4.6203    -3.0004    -0.85704   -5.1906    10.277     -3.6286\n  -5.7289     6.3868     3.8284    -1.4642    -4.3758    -1.4041\n   1.4546    -0.60059   -0.97276    1.7021     2.4357    -0.03073\n  -2.2956    -3.2183     0.15623    7.0984     0.12684   -3.8298\n   0.53441    1.1493     6.4126     4.8323    -0.094176   1.9462\n   4.7889     0.67953   -8.7185     1.0232    -6.4934    -6.6018\n  -6.4868    -1.3891     0.60352   -1.9618    -0.83308   -3.1635\n   2.0983    -2.6888     2.5529    -4.9758     0.55982    2.3738\n  -2.209      1.207      2.2095     4.4969    -6.5407     0.20055\n   3.2746    -2.978     -4.1244    -2.7862     0.71469    1.871\n   2.8257     3.665      2.7051     6.5865    -4.0485    13.891\n -10.094      7.6961     3.9371     4.3329     5.7443    -2.7688\n   2.6549     7.1357    -0.91995   -3.7897    -2.269     -0.32706\n   1.1645     7.8238    -4.0648    -1.5901    -2.4371     4.4673\n  -0.03972  -11.881    -12.262     -3.1402     0.20803   -0.63618\n  17.097      4.5217    -1.2882    -3.4139     1.2379    -6.8088\n   4.0803     5.2487     2.3712    -9.4767     1.3788    -0.70852\n  -5.7741    -3.1941     5.0739    -1.0391     4.5978    -0.87431\n  -0.7292    -4.7764     2.4639    11.421    -12.604      2.7578\n   0.45109   -0.44204   -6.4576    -7.993      3.0739    -1.5658\n  15.768      4.377     -3.5032    -5.8835     5.3726    -0.48193\n   0.89183   -1.4306     2.4475     6.7812     5.7412     2.988\n  11.668     -8.2028    -4.0986     3.8376     0.20099    1.5494\n  -4.7032    -1.3643     7.3142    -9.5133     5.4749     0.43895\n   2.405     -6.0876     4.2026    -8.6914    -4.7631     6.0201\n   4.0167     1.1332    13.218     -4.2672    11.077      1.6958\n  -8.4125    -2.4895    -2.8994     4.3663    -2.147      2.4737\n  -4.8413     7.8004    -1.1266    -0.61805    3.4348    -2.6864\n  -7.3901   -10.186      1.0281    -1.7216     7.3696    -3.9895\n   3.0035     4.8118    -2.8967    14.909     -1.1002    -3.6253\n   4.3881     0.38806   -0.34066    8.0022    -5.9174    -8.3849\n  -0.44124   -0.92531    2.9864     1.8466    -2.0044    -2.7273\n   0.2723    -1.7172    -2.0582     5.6129     6.0383     4.9308\n   4.895     -2.3096     2.9366    -7.4218    -2.0618    -1.1766\n  -7.5635    -0.69364    2.3743     0.99954    6.5449     3.7741\n   0.91835    5.4069     6.4281    -9.2167     6.6996    10.385   ]\n[ 1.5333e+00  1.6226e+00  1.0552e+00 -1.3615e+00  9.7952e-01 -8.2806e-01\n  1.4854e+00  2.8817e+00 -3.5400e-01  3.4457e-01  1.1555e+00 -7.7349e-01\n -4.4370e+00 -7.4365e-01 -1.8453e+00  6.6557e-01  4.5156e+00  1.1663e-01\n -1.1103e+00 -3.7642e+00  3.5045e+00 -3.5772e-01 -1.4795e-01 -1.0267e+00\n -1.8431e+00  1.6858e+00 -4.1045e-01 -1.7576e+00 -4.4430e-01  1.3732e+00\n  4.4730e+00 -8.4136e-01 -1.2591e+00 -4.5007e+00  9.9301e-02 -1.8817e+00\n -1.0262e+00 -2.8644e-01 -3.1175e-01  6.6088e-02 -2.0104e+00  6.3836e+00\n  1.0506e+00 -8.3638e-01 -1.0489e+00  1.1551e+00  1.1049e+00 -1.0953e+00\n -5.2687e-01  2.7210e+00 -8.4603e-01 -6.3820e-01 -1.5794e+00 -2.8231e+00\n -2.0703e+00 -1.6891e-01  4.4551e-01  3.5131e+00 -2.3713e+00  2.1279e+00\n  1.9854e+00  3.3579e+00  3.7267e+00  2.5286e-01  3.9620e-01  2.9198e+00\n -3.9707e+00 -2.8748e+00 -1.7798e+00  4.0547e+00 -1.2656e+00 -1.0919e+00\n -2.4868e+00 -5.4464e-01  6.3169e-01  1.6076e+00 -1.8257e+00 -2.4311e-02\n -2.6166e+00  8.4514e-01  8.9733e-01  2.2244e-01  2.8666e+00 -3.6442e+00\n  3.4054e+00 -2.2138e-01  2.5062e+00 -1.4775e+00  3.1202e-01 -2.5424e+00\n -4.2573e+00  1.0585e+00  5.4167e-01 -1.9668e+00 -2.7912e+00  2.5255e-01\n  8.3470e-01 -3.5377e+00  1.6103e+00 -1.6252e+00  3.9974e+00  7.8686e-01\n  7.7480e-02 -2.8996e+00 -1.2887e+00 -7.4930e-02 -2.0082e+00  1.1402e+00\n  2.7711e+00 -2.7484e+00  1.4476e+00 -1.0817e+00  7.9127e-01 -3.0431e+00\n -7.4377e-01  3.0145e+00 -2.0162e+00 -3.1396e+00  2.3760e+00  1.2135e+00\n  3.4358e-01 -1.0964e+00  7.9150e-01  2.7684e+00  2.8317e+00 -2.3924e+00\n  4.2739e+00 -1.8818e+00  4.4559e+00 -4.8621e+00 -4.3723e+00  2.2087e-03\n -1.0028e+00 -1.0392e+00 -8.0561e-01 -2.3396e-01 -6.4389e-01  6.5302e-01\n  4.3187e+00  5.2496e-01 -7.6312e-01 -1.8412e-01  1.6807e+00  1.0531e+00\n -1.9743e+00  7.1533e-02 -3.8678e+00 -3.3721e-01 -4.5415e-01  7.8711e-01\n  3.6046e-01  2.3413e+00  2.3752e+00  6.2192e-01  1.4567e+00  4.2083e-01\n  4.5502e+00  1.4567e+00 -1.4848e+00 -8.0541e-01  1.3399e+00 -1.4840e+00\n -4.0671e+00  2.1042e+00 -4.5714e+00  3.1372e-01 -3.2736e+00 -2.3693e-01\n -5.6649e-01  4.5668e+00  5.9184e-03 -7.2018e-01 -1.0209e-01 -7.1549e-01\n  1.1501e+00  6.2826e-01 -4.1710e-01  1.6110e+00  2.2790e+00  5.1121e-01\n -4.5710e-01  3.6609e+00  4.9027e+00  8.2970e-01  2.9291e-01  1.1920e+00\n -1.9468e+00 -2.0846e+00  3.8010e+00  4.1106e+00 -6.6125e-01  1.6773e+00\n  7.6820e-01 -6.0399e-01 -1.4137e+00 -2.0390e-01 -1.8717e+00  1.9584e+00\n -4.6047e+00  3.1937e+00 -1.8786e+00 -1.5766e+00 -4.2211e+00 -1.5822e+00\n  1.1744e+00  7.0951e-01 -1.0602e+00  4.1790e+00 -1.6788e+00  8.4937e-01\n  2.3151e+00 -1.4950e+00 -2.0715e-01 -1.5350e-01  1.7614e+00  1.5534e+00\n  1.6782e+00 -2.0392e+00 -2.8717e+00 -3.1403e+00 -6.7740e-01  8.1072e-01\n -1.9641e+00  1.3971e+00 -3.5342e+00 -2.3179e+00  2.4111e+00  2.2855e+00\n  1.7346e+00  4.0629e-01  1.1109e+00  1.9651e+00 -1.8063e+00 -2.7924e+00\n -3.0214e+00  2.9043e+00 -2.5850e+00 -9.3914e-01  2.6608e+00  2.5745e+00\n -3.4091e+00 -3.3903e+00  2.0456e+00 -1.4286e+00 -3.5824e+00 -3.0827e-02\n -1.4285e+00 -7.8142e-01  3.9295e+00  2.4841e+00 -5.8915e-01  6.4438e-01\n -7.0542e+00  4.7075e-01 -2.0759e+00 -4.3141e+00 -2.6736e-01 -1.5867e+00\n -7.6548e-01  3.5318e+00  2.2463e-02  1.6463e+00  3.3072e+00  2.4164e+00\n  2.5735e+00  1.6752e+00 -2.9857e+00  1.1735e+00 -4.2637e+00  1.6798e+00\n -9.0915e-01  3.9030e-01 -9.7496e-01 -6.2360e+00  1.2844e+00 -3.2688e+00\n  6.2895e-01 -3.0321e+00 -6.7734e-01  1.8979e+00  1.1453e+00 -5.7289e-01\n -2.4477e+00 -2.7195e+00  2.2955e-01 -6.7627e-01  2.5683e+00  4.5139e-01\n -1.4533e+00 -6.7582e-01  3.2434e+00 -1.5378e+00  2.3661e+00 -2.5112e+00\n -1.2672e+00 -1.4808e+00 -1.2825e+00  1.4143e+00 -2.5045e+00  2.7744e+00]\n\n\nWe can use the following code to put all the word vectors into a document array. Notice that there are 11 total words, and each word is represented by a 300-dimensional vector.\n\nnp.array([token.vector for token in doc]).shape\n\n(11, 300)\n\n\nNow we have a matrix representing a headline, but we would like a single vector to represent a headline (like we get with a count-based vectorization). A simple way to do this is to take the mean of all the word vectors.\n\nnp.array([token.vector for token in doc]).mean(axis=0)\n\narray([-1.67427254e+00, -3.95649701e-01, -3.52179974e-01,  2.67473698e+00,\n        6.60974836e+00, -5.14187276e-01,  8.85469437e-01,  5.00070000e+00,\n        1.94902003e+00, -2.36047196e+00,  5.78760433e+00,  3.53501201e+00,\n       -5.29743910e+00,  8.60979080e-01,  1.62974551e-01,  1.90474904e+00,\n        5.13615561e+00,  7.06369221e-01, -8.54807645e-02, -3.06247258e+00,\n        3.30119342e-01, -2.01464462e+00, -1.07013643e+00,  5.12255311e-01,\n        7.67499208e-01, -1.41396359e-01, -8.70485485e-01, -2.18154192e+00,\n       -7.69820929e-01,  3.40282440e+00,  1.39036822e+00, -6.79862797e-01,\n       -2.90976644e+00, -3.73797679e+00, -2.55181849e-01, -2.72300005e+00,\n       -2.77262658e-01,  1.46237183e+00,  2.09034085e+00,  2.50623512e+00,\n        9.90345255e-02,  1.39819002e+00,  3.58493626e-01,  6.11728609e-01,\n       -1.93930364e+00,  8.43867302e-01,  2.41608524e+00,  1.39169157e-01,\n        3.17911834e-01,  7.89771795e-01, -1.64458126e-01,  2.41038370e+00,\n       -3.48086071e+00, -4.50294542e+00, -1.26656735e+00,  1.48386467e+00,\n       -5.31292677e-01,  1.76347911e+00,  1.36431456e-01, -1.01937628e+00,\n        4.57578391e-01,  6.55694544e-01,  4.47306365e-01,  2.20349222e-01,\n       -3.28209937e-01,  5.58670223e-01, -2.77355099e+00, -2.27309918e+00,\n        3.82878214e-01,  3.44640374e+00, -2.05058837e+00,  8.29191133e-02,\n       -3.32992935e+00, -8.61222029e-01, -2.49131814e-01,  1.97858167e+00,\n        2.04586312e-01,  2.20670629e+00, -2.70643091e+00,  3.89133662e-01,\n       -1.94709170e+00,  4.72986996e-01,  5.14855802e-01,  1.36070907e+00,\n        6.52045429e-01,  1.00554533e-01,  6.51261926e-01, -1.92875946e+00,\n       -3.43801945e-01, -5.42956293e-01, -2.42319536e+00, -1.03637286e-01,\n        8.95858169e-01, -1.09974086e+00, -1.06548071e+00, -3.60317922e+00,\n        1.52500546e+00, -1.02814627e+00,  6.54396117e-01,  2.39761543e+00,\n        3.07453823e+00, -7.51740038e-01,  2.52026987e+00,  2.87220168e+00,\n        1.14469536e-01,  4.03908014e+00,  5.03068209e-01, -2.41837740e+00,\n       -4.30759043e-01, -1.32758176e+00,  2.91629004e+00,  2.97059655e+00,\n       -3.68380952e+00,  1.35318828e+00,  1.12694609e+00,  4.01155502e-01,\n       -1.00510728e+00,  4.68718171e-01, -7.72059023e-01, -4.92273688e-01,\n       -3.27167302e-01, -2.40791273e+00,  3.39762497e+00,  1.20934391e+00,\n       -5.85454941e-01, -2.46936440e+00,  1.71046376e+00, -1.56436992e+00,\n        2.86989927e+00, -2.02429032e+00, -2.97831368e+00,  1.53789893e-01,\n        3.17711186e+00,  1.79265010e+00,  1.29646277e+00,  2.17984033e+00,\n       -2.13165832e+00, -1.84245694e+00,  1.59989274e+00, -9.06376421e-01,\n       -3.83026332e-01, -8.57157290e-01, -1.05426975e-01,  1.43790543e+00,\n        1.54442632e+00,  3.50267768e+00, -2.04521155e+00,  1.55283272e+00,\n        1.14406455e+00,  2.76475453e+00, -1.68518817e+00,  4.30222034e+00,\n        7.69100010e-01,  1.61211622e+00, -1.14960879e-01, -3.02092806e-02,\n        1.33466375e+00,  2.94236302e-01, -1.72071755e-01,  2.22307593e-01,\n        1.14594817e+00, -1.06504452e+00,  1.06345820e+00,  7.35211790e-01,\n       -2.91556144e+00,  2.49127466e-02, -1.44770992e+00,  8.61200094e-01,\n        7.99488842e-01,  4.62962925e-01, -2.68752646e+00, -2.69153810e+00,\n        1.36439180e+00, -2.37332776e-01,  4.67620164e-01,  1.38327086e+00,\n       -9.43580031e-01, -1.03366172e+00, -2.57548189e+00, -2.32501435e+00,\n       -1.98847294e-01,  1.32680786e+00,  2.15920925e+00, -6.58661783e-01,\n       -1.14953637e+00, -1.03990626e+00, -2.58203077e+00, -6.27887785e-01,\n        1.72178984e+00,  3.32196981e-01,  1.75113726e+00, -1.65036392e+00,\n        1.06535184e+00, -2.74151921e+00,  1.59453893e+00,  7.30427206e-01,\n       -2.14383364e+00,  3.23921800e-01,  1.36081520e-02,  7.20088184e-01,\n       -1.85288537e+00, -3.40343118e+00, -1.24911356e+00, -2.45220470e+00,\n        5.95992708e+00,  1.59944725e+00, -5.84085846e+00,  3.39010024e+00,\n        1.82764065e+00, -1.37498629e+00,  1.69217908e+00,  1.45197570e+00,\n       -2.24181008e+00,  3.71031761e-01,  2.70167804e+00,  8.20800960e-01,\n        2.12189364e+00, -3.15412831e+00, -2.89437795e+00,  6.53779089e-01,\n       -1.64278102e+00,  1.68185008e+00, -2.26050162e+00, -9.64069009e-01,\n       -1.20023906e+00, -1.92961133e+00,  1.87058246e+00,  3.15290356e+00,\n        1.79731822e+00, -6.71526015e-01,  3.30500007e+00, -4.40620899e+00,\n       -3.89296353e-01,  2.90936899e+00,  1.81972325e+00,  3.27335382e+00,\n        5.00913104e-03,  9.26009119e-01,  7.37483799e-01,  1.01161277e+00,\n       -1.92496371e+00, -2.87720466e+00, -6.51925325e-01,  1.53084946e+00,\n       -7.98255503e-01,  1.58058846e+00, -2.28812742e+00, -2.83182096e-02,\n       -9.97380018e-01,  1.05249739e+00,  1.55504990e+00, -2.77577114e+00,\n       -5.45920515e+00, -1.18169224e+00, -1.95299134e-01, -2.01057363e+00,\n        3.82100701e+00,  5.25485456e-01,  1.14884090e+00,  1.83555448e+00,\n        9.73378420e-01,  8.01964188e+00,  3.69635987e+00,  1.23061645e+00,\n        2.91242385e+00, -6.20422781e-01, -2.49540854e+00,  2.78798914e+00,\n       -2.41546655e+00,  4.45944607e-01, -1.21348667e+00, -3.64107251e-01,\n        9.94940758e-01, -2.97548199e+00,  4.78463680e-01, -1.68698168e+00,\n        7.81730056e-01, -1.28138268e+00,  1.28043652e-01,  1.47639644e+00,\n        4.41694880e+00,  1.11029530e+00,  7.96417594e-01,  9.79942858e-01,\n        1.36084640e+00, -1.99406707e+00, -7.40921795e-01, -3.85187328e-01,\n       -6.17053688e-01, -2.02488199e-01, -3.86805505e-01,  1.27285814e+00,\n        3.04273129e+00,  1.55780935e+00, -2.08421683e+00,  2.39458346e+00,\n        1.39288843e+00, -3.40017486e+00, -1.14612353e+00, -9.15518224e-01],\n      dtype=float32)",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41c_vectorizing_text/vectorizing_text.html#word-embedding---full-data-set",
    "href": "chapters/41c_vectorizing_text/vectorizing_text.html#word-embedding---full-data-set",
    "title": "50  Vectorizing Text",
    "section": "50.10 Word Embedding - Full Data Set",
    "text": "50.10 Word Embedding - Full Data Set\nWe can use the following nested list comprehension to calculate all the vector representations of all of our headlines at once. (This code takes about 45 seconds to run.)\n\n%%time\nall_vectors = np.array([np.array([token.vector for token in nlp(s)]).mean(axis=0) for s in df_headline['clean_headline']])\n\nCPU times: user 46.1 s, sys: 11.2 ms, total: 46.2 s\nWall time: 46.2 s\n\n\nNotice that we now have an array that represents our 9470 headlines, each with a 300-dimensional vector. And each 300-dimensional vector is the mean of the vector representations of each of the words in the headline.\n\nall_vectors.shape\n\n(9470, 300)",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Vectorizing Text</span>"
    ]
  },
  {
    "objectID": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html",
    "href": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html",
    "title": "51  Sentiment Analysis on Stock Headlines",
    "section": "",
    "text": "51.1 Importing Packages\nIn this tutorial we use various methods to calculate the sentiment (positive or negative) associated with headlines related to various stocks. We compare this sentiment to the event return of the stock, which is the return of the stock around the date of the headline. Ultimately we are interested in seeing if there is a positive correlation with the sentiments calculated and the event return. If there is a significant positive correlation, we could build a trading strategy around the sentiment analysis.\nLet’s begin my importing some initial packages.\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Sentiment Analysis on Stock Headlines</span>"
    ]
  },
  {
    "objectID": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#reading-in-data",
    "href": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#reading-in-data",
    "title": "51  Sentiment Analysis on Stock Headlines",
    "section": "51.2 Reading-In Data",
    "text": "51.2 Reading-In Data\nNext, let’s read in and examine our data. It consists of 2,768 headlines related to various stocks. Associated with each headline is the event return of the relevant stock. Event return is defined as:\n\\[\\begin{align*}\nR_{t–1} + R_{t} + R_{t+1}\n\\end{align*}\\]\nwhere \\(R_{t–1}\\), \\(R_{t+1}\\) are the returns the days before and after the news data, and \\(R_t\\) is the return on the day of the news. We do this because at times the news is reported late (i.e., after market participants are already aware of the announcement) or after market close. Having a slightly wider window ensures that we capture the essence of the event.\nUltimately, we are interested to see if we can calculate sentiments for the headlines that are positively correlated with the eventRet.\n\ndf_news_return = pd.read_csv('Step3_NewsAndReturnData.csv', delimiter='|')\ndf_news_return\n\n\n\n\n\n\n\n\nticker\nheadline\ndate\neventRet\nClose\n\n\n\n\n0\nAMZN\nWhole Foods (WFMI) -5.2% following a downgrade...\n2011-05-02\n0.017650\n201.19\n\n\n1\nNFLX\nNetflix (NFLX +1.1%) shares post early gains a...\n2011-05-02\n-0.013003\n33.88\n\n\n2\nMSFT\nThe likely winners in Microsoft's (MSFT -1.4%)...\n2011-05-10\n-0.019823\n20.63\n\n\n3\nMSFT\nMicrosoft (MSFT -1.2%) and Skype signed their ...\n2011-05-10\n-0.019823\n20.63\n\n\n4\nMSFT\nNaN\n2011-05-10\n-0.019823\n20.63\n\n\n...\n...\n...\n...\n...\n...\n\n\n2763\nTSLA\nResponding to a comment on Twitter, Elon Musk ...\n2018-12-24\n0.041608\n295.39\n\n\n2764\nAMZN\nAmazon (NASDAQ:AMZN) is among the companies th...\n2018-12-24\n0.013062\n1343.96\n\n\n2765\nAAPL\nChinese companies urge employees to boycott (N...\n2018-12-24\n0.005634\n143.92\n\n\n2766\nTSLA\nElon Musk tweets that Tesla's (NASDAQ:TSLA) Su...\n2018-12-26\n-0.002856\n326.09\n\n\n2767\nFB\n\"Backing up the sleigh,\" on Facebook (NASDAQ:F...\n2018-12-26\n0.076984\n134.18\n\n\n\n\n2768 rows × 5 columns",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Sentiment Analysis on Stock Headlines</span>"
    ]
  },
  {
    "objectID": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#using-textblob",
    "href": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#using-textblob",
    "title": "51  Sentiment Analysis on Stock Headlines",
    "section": "51.3 Using textblob",
    "text": "51.3 Using textblob\nWe’ll begin by calculating sentiments using the built-in sentiment analyzer in the textblob package.\nThe texblob sentiment function is pretrained model based on Naïve-Bayes classification algorithm to convert a sentence to a numerical value of sentiment between -1 to +1 and map adjectives frequently found in movie reviews (source code: https://textblob.readthedocs.io/en/dev/_modules/textblob/en/sentiments.html) to sentiment polarity scores, ranging from -1 to +1 (negative ↔︎ positive) and a similar subjectivity score (objective ↔︎ subjective).\n\nfrom textblob import TextBlob\n\nLet’s apply the TextBlob() function to an example headline.\n\nexample_headline = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share in Frankfurt, touching their \\\nhighest level in 14 months, after the U.S. government said a $25M glyphosate decision against the \\\ncompany should be reversed.\"\n\nTextBlob(example_headline).sentiment.polarity\n\n0.5\n\n\nLet’s compute the sentiment for all the headlines in the data.\n\ndf_news_return['sentiment_textblob'] = [TextBlob(str(s)).sentiment.polarity for s in df_news_return['headline']]\ndf_news_return\n\n\n\n\n\n\n\n\nticker\nheadline\ndate\neventRet\nClose\nsentiment_textblob\n\n\n\n\n0\nAMZN\nWhole Foods (WFMI) -5.2% following a downgrade...\n2011-05-02\n0.017650\n201.19\n0.262500\n\n\n1\nNFLX\nNetflix (NFLX +1.1%) shares post early gains a...\n2011-05-02\n-0.013003\n33.88\n-0.043750\n\n\n2\nMSFT\nThe likely winners in Microsoft's (MSFT -1.4%)...\n2011-05-10\n-0.019823\n20.63\n0.166667\n\n\n3\nMSFT\nMicrosoft (MSFT -1.2%) and Skype signed their ...\n2011-05-10\n-0.019823\n20.63\n-0.030556\n\n\n4\nMSFT\nNaN\n2011-05-10\n-0.019823\n20.63\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2763\nTSLA\nResponding to a comment on Twitter, Elon Musk ...\n2018-12-24\n0.041608\n295.39\n-0.156250\n\n\n2764\nAMZN\nAmazon (NASDAQ:AMZN) is among the companies th...\n2018-12-24\n0.013062\n1343.96\n0.200000\n\n\n2765\nAAPL\nChinese companies urge employees to boycott (N...\n2018-12-24\n0.005634\n143.92\n-0.012500\n\n\n2766\nTSLA\nElon Musk tweets that Tesla's (NASDAQ:TSLA) Su...\n2018-12-26\n-0.002856\n326.09\n0.000000\n\n\n2767\nFB\n\"Backing up the sleigh,\" on Facebook (NASDAQ:F...\n2018-12-26\n0.076984\n134.18\n0.000000\n\n\n\n\n2768 rows × 6 columns\n\n\n\nLet’s see how the TextBlob() sentiments correlate with the eventRet. As we can see the correlation is +4.2%, which is directionally correct but quite low.\n\ncorrelation = df_news_return['eventRet'].corr(df_news_return['sentiment_textblob'])\nprint(correlation)\n\n0.04257233738490116",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Sentiment Analysis on Stock Headlines</span>"
    ]
  },
  {
    "objectID": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#supervised-techniques---sklearn",
    "href": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#supervised-techniques---sklearn",
    "title": "51  Sentiment Analysis on Stock Headlines",
    "section": "51.4 Supervised Techniques - sklearn",
    "text": "51.4 Supervised Techniques - sklearn\nNext, we develop our own models for sentiment analysis, based on some labeled financial headline data from Kaggle. Let’s read-in the data that we will use to train our models.\n\ndf_news_labeled = pd.read_csv('LabelledNewsData.csv', encoding='unicode_escape')\ndf_news_labeled\n\n\n\n\n\n\n\n\ndatetime\nheadline\nticker\nsentiment\n\n\n\n\n0\n1/16/2020 5:25\n$MMM fell on hard times but could be set to re...\nMMM\n0\n\n\n1\n1/11/2020 6:43\nWolfe Research Upgrades 3M $MMM to ¡§Peer Perf...\nMMM\n1\n\n\n2\n1/9/2020 9:37\n3M $MMM Upgraded to ¡§Peer Perform¡¨ by Wolfe ...\nMMM\n1\n\n\n3\n1/8/2020 17:01\n$MMM #insideday follow up as it also opened up...\nMMM\n1\n\n\n4\n1/8/2020 7:44\n$MMM is best #dividend #stock out there and do...\nMMM\n0\n\n\n...\n...\n...\n...\n...\n\n\n9465\n4/11/2019 1:24\n$WMT - Walmart shifts to remodeling vs. new st...\nWMT\n1\n\n\n9466\n4/10/2019 6:05\nWalmart INC $WMT Holder Texas Permanent School...\nWMT\n0\n\n\n9467\n4/9/2019 4:38\n$WMT $GILD:3 Dividend Stocks Perfect for Retir...\nWMT\n1\n\n\n9468\n4/9/2019 4:30\nWalmart expanding use of #robots to scan shelv...\nWMT\n1\n\n\n9469\n4/9/2019 4:11\n$WMT Walmart plans to add thousands of robot h...\nWMT\n1\n\n\n\n\n9470 rows × 4 columns\n\n\n\nAs we can see, the Kaggle data is fairly balanced with about 58% positive sentiments, consists of 30 tickers, each of which are fairly uniformly represented.\n\ndf_news_labeled['sentiment'].mean()\n\n0.5788806758183738\n\n\n\nprint(\"Number of Tickers: \", len(df_news_labeled['ticker'].unique()))\n\nNumber of Tickers:  30\n\n\n\ndf_news_labeled['ticker'].value_counts()\n\nticker\nHD      350\nUTX     350\nJPM     349\nINTC    348\nMMM     347\nIBM     346\nMRK     345\nWMT     344\nCVX     343\nUNH     339\nPFE     337\nCSCO    337\nVZ      335\nPG      334\nCAT     332\nV       326\nTRV     326\nMCD     326\nMSFT    324\nJNJ     322\nAXP     320\nBA      318\nKO      309\nDIS     300\nWBA     283\nNKE     273\nAAPL    271\nXOM     259\nDOW     190\nGS      187\nName: count, dtype: int64\n\n\nIn order to run a supervised learning model, we first need to convert the news headlines into a vector feature representation. For this exercise, we will use the word embedding built into the spaCy package. The word embedding model includes 20,000 unique vectors with 300 dimensions. We apply this model to all the headlines in df_news_labeled.\nLet’s first import the spaCy package.\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n\nNext, let’s see the word embedding for a single word.\n\ndoc = nlp('fell')\ntype(doc)\n\nspacy.tokens.doc.Doc\n\n\n\nfor token in doc:\n    print(token.vector)\n\n[  1.1734    -7.2025     0.27734   -5.4665     2.8386    -2.496\n   1.1509     9.5497     4.8894    -2.7378    -1.0221     3.1214\n  -1.5272     1.2968     0.78371    3.3423    -4.3773    -6.473\n   3.9741     2.4101    -0.91515   -4.1784    -1.9259    -5.7423\n  -5.1056    -2.5617     3.7718    -4.4208     9.0647     1.4237\n   1.2726     0.5712    -1.4489    -2.7199    -5.1065    -1.6203\n  -3.0038     1.116     -0.44994    1.5537     2.9035     2.288\n   6.2687    -0.10803    0.36491   -2.2931    -3.9193     2.7295\n   1.989      1.8773    -0.70424    2.7992    -4.6728     1.1616\n  -0.06419    2.7225     4.4624    -4.8996     2.086      4.1729\n  -1.074     -3.9116     1.0264     2.848     -0.97624    0.48979\n   2.5099     4.0977     2.7164    -2.7599     2.1031     0.16324\n  -1.5091     3.9319     1.5492     3.1728     0.44398    5.1455\n  -1.7672    -0.12588   -3.2237    -0.96587    2.5144     2.616\n   1.7276     0.3823     2.1233    -1.3669    -1.9021     1.01\n   0.13405    3.0058    -3.887      3.9119     1.2379     7.797\n   1.0274   -10.084      6.5854    -1.3986     0.058625  -0.6915\n   2.0257     0.85324   -3.1623     3.5685     2.3699     3.3838\n  -6.4677    -0.73642   -0.75226   -5.686     -3.8741    -1.7943\n   0.40222    2.8348    -2.6308     4.0367    -7.4351     2.396\n   2.5281    -1.6755    -2.4794     2.5454    -2.026      2.7354\n   1.8119     2.4876     0.82002    3.4847    -0.23689   -0.1508\n   4.5248     2.1016    -0.83938    0.86797    1.4937    -6.4949\n   1.0099    -2.7664    -4.861      1.9063    -2.6432     5.9116\n   2.7117     4.0489    -8.2296    -5.1775    -2.5998    -2.6773\n  -0.41113   -2.8494     6.1497    -0.4032    -1.4203    -3.6775\n  -2.2973     2.2945     1.5869    -1.1298     1.9766    -1.3601\n   0.9683    -6.2481    -0.040676   3.6518     0.75282    1.628\n  -2.7672     5.6425     4.4535    -0.75748    3.4292    -7.3582\n   0.23872   -5.1985    -2.373     -4.378     -0.97711    0.24911\n  -2.4109    -4.1346     0.018835   9.3948    -3.5975    -5.4591\n   5.7961     2.9058    -2.4857    -5.5542     3.1893    -2.4025\n  -5.3554    -2.7439     1.2465   -12.615      2.2201    -1.4977\n  -2.9704     5.3191     3.2996    -1.3074     1.8646    -4.9133\n  -1.7506     0.48      -2.021     -0.96144   -2.5537     1.667\n  -4.3866     3.3491    -2.9308     1.3915     2.426      1.8428\n  -0.2902     1.5573    -5.8349    -3.1051    -1.3296    -1.6882\n   0.63074    3.8912    -4.129      3.1995     1.9571     5.7802\n   0.13258   -2.5778     0.65669    0.26942    5.4798     3.1075\n  -2.7361     9.4319    -2.4395    -1.693      4.2018    -0.3257\n   6.0126     5.0842     0.27366   -2.3441     1.6358    -0.90506\n  -6.7565     6.9635     2.3309    -2.2213    -6.1234    -5.0523\n  -0.7981    -1.5737     5.1085     2.6834    -1.4149    -4.6759\n   6.3007     5.3383     1.841      0.38306   -2.2447    -0.53224\n   0.034658  -4.1074     1.3425    -1.5725     1.1915     1.6954\n   7.0669     0.72362   -7.5257    -1.9819    -0.35572   -0.75719\n   0.82741   -6.3317     6.7808     1.3056     1.494      1.5701\n  -1.1684    -2.2983    -0.38407    0.60411   -5.5468    -2.7808\n  -6.7024     2.9155    -5.426      2.5842    -1.2399     0.63733\n  -2.3217     3.1628    -1.0898     1.7661     2.2106    -0.079949]\n\n\nIn order to associate a single vector with headline, we will calculate the mean vector of all the words in the headline. In the following code we are calculating the word-embedding for each of the words in the headline and taking the average of all of them. This results in a single 300-dimensional vector for each headline.\n\n%%time\nall_vectors = np.array([np.array([token.vector for token in nlp(s)]).mean(axis=0) for s in df_news_labeled['headline']])\n\nCPU times: user 51.6 s, sys: 24.3 ms, total: 51.6 s\nWall time: 51.6 s\n\n\n\nall_vectors.shape\n\n(9470, 300)\n\n\nNow we have our headlines featurized and we can use them as training data for a variety of classification models.\nWe next import the functions that we will need from sklearn.\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\n\nLet’s organize our features and our labels as well as create a training and test sets.\n\nX = all_vectors\nY = df_news_labeled['sentiment']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\nNext, let’s create a list of models that we are going to fit to our data.\n\nmodels = []\nmodels.append(('LR', LogisticRegression(max_iter=1000)))\nmodels.append(('KNN', KNeighborsClassifier(n_jobs=-1)))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('NN', MLPClassifier()))\nmodels.append(('RF', RandomForestClassifier(n_jobs=-1)))\n\nThe following for-loop iterates through all the models, fits each one, and stores the results for further analysis.\n\n#results = []\nnames = []\ntest_results = []\ntrain_results = []\nfor name, model in models:\n    # collecting names of models\n    names.append(name)\n    \n    # fitting on training data\n    model.fit(X_train, Y_train)\n    train_result = accuracy_score(model.predict(X_train), Y_train)\n    train_results.append(train_result)\n    \n    # checking accuracy on testing data\n    test_result = accuracy_score(model.predict(X_test), Y_test)\n    test_results.append(test_result)    \n    \n    msg = f'{name}: {train_result:.4f} {test_result:.4f}'\n    print(msg)\n\nLR: 0.8974 0.8793\nKNN: 0.8457 0.7628\nCART: 0.9998 0.6998\nSVM: 0.8603 0.8434\nNN: 0.9968 0.9071\nRF: 0.9998 0.7955\n\n\nLet’s compare the various models by plotting their training and testing accuracy. As you can see the neural network model seems to perform the best.\n\n# compare algorithms\nfrom matplotlib import pyplot\nfig = pyplot.figure()\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.bar(ind - width/2, train_results,  width=width, label='Train Error')\npyplot.bar(ind + width/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(8,5)\npyplot.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\npyplot.show()",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Sentiment Analysis on Stock Headlines</span>"
    ]
  },
  {
    "objectID": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#ltsm---keras",
    "href": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#ltsm---keras",
    "title": "51  Sentiment Analysis on Stock Headlines",
    "section": "51.5 LTSM - keras",
    "text": "51.5 LTSM - keras\nWe will also include LSTM, which is an RNN-based model, in the list of models considered. RNNs process sequences of data (in our case words in a headline) and thus are well suited for NLP. In particular, LSTMs maintain a memory of past information, which enables the model to predict the current output conditioned on long distance features and looks at the words in the context of the entire sentence, rather than simply looking at the individual words.\nLet’s begin by importing some of the keras objects that we will need.\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\n#from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\nfrom keras.layers import Dense, LSTM\nfrom keras.layers import Embedding\n\n2024-11-05 16:07:00.013169: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-11-05 16:07:00.204742: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-11-05 16:07:00.204789: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-11-05 16:07:00.206701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-11-05 16:07:00.304883: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-11-05 16:07:00.305864: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-11-05 16:07:01.417417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\nAs with any NLP modeling excercise, we will need to process our headlines to be usable in our LSTM model. For us to be able to feed the data into our LSTM model, all input documents must have the same length. We use the keras Tokenizer function to tokenize the strings and then use .texts_to_sequences() to make sequences of words. We will limit the maximum review length to maxlen=50 by truncating longer reviews and pad shorter reviews with a null value (0). We can accomplish this using the pad_sequences function, also in keras.\n\nvocabulary_size = 20000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(df_news_labeled['headline'])\nsequences = tokenizer.texts_to_sequences(df_news_labeled['headline'])\nX_LSTM = pad_sequences(sequences, maxlen=50)\n\nLet’s separate out our labels and create a holdout set.\n\nY_LSTM = df_news_labeled['sentiment']\nX_train_LSTM, X_test_LSTM, Y_train_LSTM, Y_test_LSTM = \\\n    train_test_split(X_LSTM, Y_LSTM, test_size=0.3, random_state=0)\n\nIn the following code snippet, we use the keras library to build an artificial neural network classifier based on an LSTM model.\nNotice that the network starts with an Embedding layer. Unlike our previous models where we were using the pre-trained embedding in the spaCy package, in this model we are learning our own word embedding. The Embedding layer lets the system expand each token to a larger vector, allowing the network to represent a word in a meaningful way. The layer takes 20,000 as the first argument (i.e., the size of our vocabulary) and 300 as the second input parameter (i.e., the dimension of the embedding).\nFinally, given that this is a classification problem and the output needs to be labeled as zero or one, the KerasClassifier() function is used as a wrapper over the LSTM model to produce a binary (zero or one) output.\n\nfrom scikeras.wrappers import KerasClassifier\ndef create_model(input_length=50):\n    model = Sequential()\n    model.add(Embedding(20000, 300, input_length=50))\n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])    \n    return model    \nmodel_LSTM = KerasClassifier(model=create_model, epochs=3, verbose=True, validation_split=0.4)\nmodel_LSTM.fit(X_train_LSTM, Y_train_LSTM)\n\nEpoch 1/3\n125/125 [==============================] - 15s 104ms/step - loss: 0.4059 - accuracy: 0.8182 - val_loss: 0.1415 - val_accuracy: 0.9491\nEpoch 2/3\n125/125 [==============================] - 13s 100ms/step - loss: 0.0521 - accuracy: 0.9849 - val_loss: 0.1021 - val_accuracy: 0.9664\nEpoch 3/3\n125/125 [==============================] - 13s 100ms/step - loss: 0.0141 - accuracy: 0.9965 - val_loss: 0.1461 - val_accuracy: 0.9559\n\n\nKerasClassifier(\n    model=&lt;function create_model at 0x7d0b4f11bb50&gt;\n    build_fn=None\n    warm_start=False\n    random_state=None\n    optimizer=rmsprop\n    loss=None\n    metrics=None\n    batch_size=None\n    validation_batch_size=None\n    verbose=True\n    callbacks=None\n    validation_split=0.4\n    shuffle=True\n    run_eagerly=False\n    epochs=3\n    class_weight=None\n)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. KerasClassifieriFittedKerasClassifier(\n    model=&lt;function create_model at 0x7d0b4f11bb50&gt;\n    build_fn=None\n    warm_start=False\n    random_state=None\n    optimizer=rmsprop\n    loss=None\n    metrics=None\n    batch_size=None\n    validation_batch_size=None\n    verbose=True\n    callbacks=None\n    validation_split=0.4\n    shuffle=True\n    run_eagerly=False\n    epochs=3\n    class_weight=None\n) \n\n\nLet’s calculate the a in-sample and out-of-sample accuracy and store them for plotting below.\n\ntrain_result_LSTM = accuracy_score(model_LSTM.predict(X_train_LSTM), Y_train_LSTM)\ntest_result_LSTM = accuracy_score(model_LSTM.predict(X_test_LSTM), Y_test_LSTM)\ntrain_results.append(train_result_LSTM)\ntest_results.append(test_result_LSTM)\nnames.append('LSTM')\n\n208/208 [==============================] - 2s 9ms/step\n89/89 [==============================] - 1s 9ms/step\n\n\n\nprint('LTSM Training Accuracy: ', train_result_LSTM)\nprint('LTSM Testing Accuracy: ', test_result_LSTM)\n\nLTSM Training Accuracy:  0.9806909036053704\nLTSM Testing Accuracy:  0.9644491376275959\n\n\nLet’s add in our LSTM model results to our previous results and plot again.\n\n# compare algorithms\nfrom matplotlib import pyplot\nfig = pyplot.figure()\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.bar(ind - width/2, train_results,  width=width, label='Train Error')\npyplot.bar(ind + width/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(9, 5)\npyplot.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\npyplot.show()\n\n\n\n\n\n\n\n\nThe LSTM model has the best performance on the test set as compared to all other models. The performance of the ANN is comparable to the LSTM-based model. The performances of random forest (RF), SVM, and logistic regression (LR) are reasonable as well. CART and KNN do not perform as well as other models. CART shows significant overfitting.",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Sentiment Analysis on Stock Headlines</span>"
    ]
  },
  {
    "objectID": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#ltsm-model-correlations-with-eventret",
    "href": "chapters/41d_sentiment_analysis_stock_headlines/sentiment_analysis_stock_headlines.html#ltsm-model-correlations-with-eventret",
    "title": "51  Sentiment Analysis on Stock Headlines",
    "section": "51.6 LTSM Model Correlations with eventRet",
    "text": "51.6 LTSM Model Correlations with eventRet\nOf all the custom sentiment models that we fit to our Kaggle data, the LTSM seems to perform the best. So in this final section, let’s use our LSTM model to predict sentiments on our headlines in df_news_return and check the correlation with the eventRet.\nAs a first step, we need to get the headlines in a format that can be fed into the LSTM model.\n\nsequences_LSTM = tokenizer.texts_to_sequences(df_news_return['headline'].astype(str))\nX_LSTM = pad_sequences(sequences_LSTM, maxlen=50)\nX_LSTM\n\narray([[   0,    0,    0, ...,   46, 3856,  307],\n       [   1,   36,   16, ...,  273,    1,  801],\n       [   0,    0,    0, ...,  159,  224, 3293],\n       ...,\n       [   0,    0,    0, ..., 7131, 2035, 1422],\n       [   0,    0,    0, ..., 1881,  232,  154],\n       [   0,    0,    0, ..., 9673, 5060, 2057]], dtype=int32)\n\n\n\nX_LSTM.shape\n\n(2768, 50)\n\n\nNext, we use the .predict() method of our fitted model make our predictions and store them in df_news_return.\n\nY_LSTM = model_LSTM.predict(X_LSTM)\ndf_news_return['sentiment_LSTM'] = Y_LSTM \ndf_news_return.head()\n\n87/87 [==============================] - 1s 9ms/step\n\n\n\n\n\n\n\n\n\nticker\nheadline\ndate\neventRet\nClose\nsentiment_textblob\nsentiment_LSTM\n\n\n\n\n0\nAMZN\nWhole Foods (WFMI) -5.2% following a downgrade...\n2011-05-02\n0.017650\n201.19\n0.262500\n0\n\n\n1\nNFLX\nNetflix (NFLX +1.1%) shares post early gains a...\n2011-05-02\n-0.013003\n33.88\n-0.043750\n1\n\n\n2\nMSFT\nThe likely winners in Microsoft's (MSFT -1.4%)...\n2011-05-10\n-0.019823\n20.63\n0.166667\n1\n\n\n3\nMSFT\nMicrosoft (MSFT -1.2%) and Skype signed their ...\n2011-05-10\n-0.019823\n20.63\n-0.030556\n1\n\n\n4\nMSFT\nNaN\n2011-05-10\n-0.019823\n20.63\n0.000000\n0\n\n\n\n\n\n\n\nLet’s now check the correlation between sentiment_LSTM and eventRet. As we can see, there is improvement over the Textblob approach.\n\ncorrelation = df_news_return['eventRet'].corr(df_news_return['sentiment_LSTM'])\nprint(correlation)\n\n0.07874624006776532\n\n\nLet’s also check the correlation with the predicted probabilities - the correlation is quite similar.\n\nprobs = model_LSTM.predict_proba(X_LSTM)[:, 1]\nnp.corrcoef(df_news_return['eventRet'], probs)\n\n87/87 [==============================] - 1s 9ms/step\n\n\narray([[1.        , 0.08612546],\n       [0.08612546, 1.        ]])",
    "crumbs": [
      "Natural Language Processing",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Sentiment Analysis on Stock Headlines</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html",
    "href": "chapters/42_virtual_environments/virtual_environments.html",
    "title": "52  Virtual Enviroments",
    "section": "",
    "text": "52.1 Where Do Packages Live on Your Machine?\nMany data scientists aren’t particularly careful about knowing where their packages live on their machine. They usually just install their packages at the system level and then get on with their data analysis.\nIn a high stakes context, you will want to make sure that your work is completely reproducible. Insuring reproducibility will also be important when you start collaborating with others.\nVirtual environments are a way to insure reproducibility of your work; they are a specific example of the general concept of a project specific library.\nWhere does Python look for packages to use in your projects? You can see the default set of paths that it looks for using the following code.\nimport sys\nsys.path\n\n['/home/pritam/Desktop/p4dsf/chapters/42_virtual_environments',\n '/usr/lib/python310.zip',\n '/usr/lib/python3.10',\n '/usr/lib/python3.10/lib-dynload',\n '',\n '/home/pritam/.local/lib/python3.10/site-packages',\n '/usr/local/lib/python3.10/dist-packages',\n '/usr/lib/python3/dist-packages']",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#dist-packages-and-site-packages",
    "href": "chapters/42_virtual_environments/virtual_environments.html#dist-packages-and-site-packages",
    "title": "52  Virtual Enviroments",
    "section": "52.2 Dist-packages and Site-packages",
    "text": "52.2 Dist-packages and Site-packages\nThere are two types of packages: dist-packages and site-packages. Dist-packages are the standard library that comes out-of-the-box with every Python distribution. Site-packages are the third party packages that you install using pip.\nFrom what I can tell, dist-packages mostly live in /usr/lib/python3.10. And site-packages mostly live in /home/pritam/.local/lib/python3.10/site-packages.\nLet’s hop into the terminal and take a look.\n$$ ls /usr/lib/python3.8\n\n$$ ls /home/pritam/.local/lib/python3.8/site-packages\n(Of course, the specific file paths will be different on your machine.)",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#the-need-for-virtual-environments",
    "href": "chapters/42_virtual_environments/virtual_environments.html#the-need-for-virtual-environments",
    "title": "52  Virtual Enviroments",
    "section": "52.3 The Need for Virtual Environments",
    "text": "52.3 The Need for Virtual Environments\nConsider the following scenario where you have two projects: ProjectA and ProjectB, both of which have a dependency on the same library, ProjectC. The problem becomes apparent when we start requiring different versions of ProjectC. Maybe ProjectA needs v1.0.0, while ProjectB requires the newer v2.0.0, for example.\nThis is a real problem for Python since it can’t differentiate between versions in the site-packages directory. So both v1.0.0 and v2.0.0 would reside in the same directory with the same name\nA virtual envorionment will solve this problem.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#what-is-a-virtual-environment",
    "href": "chapters/42_virtual_environments/virtual_environments.html#what-is-a-virtual-environment",
    "title": "52  Virtual Enviroments",
    "section": "52.4 What is a Virtual Environment?",
    "text": "52.4 What is a Virtual Environment?\nA virtual environment is simply a directory with three important components:\n\nA site-packages/ folder where third party libraries are installed.\nSymlinks to Python executables installed on your system.\nScripts that ensure executed Python code uses the Python interpreter and site packages installed inside the given virtual environment.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#creating-a-virtual-environment",
    "href": "chapters/42_virtual_environments/virtual_environments.html#creating-a-virtual-environment",
    "title": "52  Virtual Enviroments",
    "section": "52.5 Creating a Virtual Environment",
    "text": "52.5 Creating a Virtual Environment\nLet’s begin by creating a simple project to demonstrate these concepts.\n$$ mkdir test_project\n\n$$ cd test_project\nNext, let’s create a simple script inside our project and call it script.py:\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\n\ndf = pdr.get_data_yahoo('SPY')\nprint(df)\nLet’s try running our script at the terminal:\n$$ python3 script.py\nThis runs just fine because our project is looking for system-wide installations of pandas-datareader.\nWe can see a list of all site-packages with the following:\n$$ pip list\nNotice that pandas-datareader is in this list.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#creating-a-virtual-environment-1",
    "href": "chapters/42_virtual_environments/virtual_environments.html#creating-a-virtual-environment-1",
    "title": "52  Virtual Enviroments",
    "section": "52.6 Creating a Virtual Environment",
    "text": "52.6 Creating a Virtual Environment\nNow let’s create a virtual environment in test_project. There are many different ways of creating virtual environments in Python. We are going to use the venv package which is part of the standard library.\nThe command for creating a virtual environment is simply:\n$$ python3 -m venv .venv\nLet’s take a look at the contents of our project directory with ls -la. As you can see, there is a new directory called .venv. If we look at the content of .venv we see a number of subdirectories.\nHere is what the important folders contains:\n\nbin: files that interact with the virtual environment\ninclude: C headers that compile the Python packages\nlib: a copy of the Python version along with a site-packages folder where each dependency is installed\n\nNow that we have created our virtual environment inside our test_project, let’s try rerunning our script. As you can see it works fine.\n$$ python3 script.py\nAs you can see the script still runs fine.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#activating-the-virtual-environment",
    "href": "chapters/42_virtual_environments/virtual_environments.html#activating-the-virtual-environment",
    "title": "52  Virtual Enviroments",
    "section": "52.7 Activating the Virtual Environment",
    "text": "52.7 Activating the Virtual Environment\nEven though we have created our virtual environment, we still have not activated it. Let’s do so now.\n$$ source .venv/bin/activate\n\n(venv) $$\nNotice the (.venv) decorator at the terminal prompt. This indicates that the virtual environment has been activated.\nLet’s try running our script again:\n$$ python3 script.py\nThis time it fails. To see why, let’s look at the search path in the Python console.\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; import pprint\n&gt;&gt;&gt; pprint.pprint(sys.path)\nAs you can see, sys.path is much shorter now. Now, all the site packages associated with the project are in /home/pritam/Desktop/test_project/.venv/lib/python3.8/site-packages.\nIn the terminal, let’s look at the contents of this directory with ls -la. As you can see, the content of this directory are minimal and, in particular, pandas-datareader is not present.\nAnother way to verify this is to run:\n$$ pip list",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#installing-packages-into-a-virtual-environment",
    "href": "chapters/42_virtual_environments/virtual_environments.html#installing-packages-into-a-virtual-environment",
    "title": "52  Virtual Enviroments",
    "section": "52.8 Installing Packages into a Virtual Environment",
    "text": "52.8 Installing Packages into a Virtual Environment\nLet’s now use pip to install pandas-datareader into our virtual environment.\n$$ pip install pandas-datareader\n\n$$ ls /home/pritam/Desktop/test_project/.venv/lib/python3.10/site-packages\nNow we can see that /home/pritam/Desktop/test_project/venv/lib/python3.8/site-packages contains pandas-datareader and all its dependencies. We also have a longer pip list.\n$$ pip list\nIf we try to rerun our script, it now works.\n$$ python3 script.py",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#creating-a-requirements.txt",
    "href": "chapters/42_virtual_environments/virtual_environments.html#creating-a-requirements.txt",
    "title": "52  Virtual Enviroments",
    "section": "52.9 Creating a requirements.txt",
    "text": "52.9 Creating a requirements.txt\nIn order to make our project reproducible we will need to make a requirements.txt which will detail all the packages in our virtual environment. This is done as follows:\n$$ pip freeze &gt; requirements.txt\nWe now have a text file called requirements.txt which details all the packages in our project as well as their version numbers. Let’s check the contents of this text file:\n$$ cat requirements.txt",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#deactivating-a-virtual-environment",
    "href": "chapters/42_virtual_environments/virtual_environments.html#deactivating-a-virtual-environment",
    "title": "52  Virtual Enviroments",
    "section": "52.10 Deactivating a Virtual Environment",
    "text": "52.10 Deactivating a Virtual Environment\nDeactivating a virtual enviroment is straight forward.\n(venv) $$ deactivate\n\n$$\nWe can verify that our virtual environment is deactivated by running:\n$$ pip list\nNotice the much longer list of system-wide site package installations.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#reproducing-a-development-environment-with-requirements.txt",
    "href": "chapters/42_virtual_environments/virtual_environments.html#reproducing-a-development-environment-with-requirements.txt",
    "title": "52  Virtual Enviroments",
    "section": "52.11 Reproducing a Development Environment with requirements.txt",
    "text": "52.11 Reproducing a Development Environment with requirements.txt\nLet’s delete the .venv/ virtual enviroment in our test_project and pretend that we cloned this project from github. (Conventional wisdom is that it is best to put virtual environments in your .gitignore)\n$$ rm -rf .venv\nBecause we have the requirements.txt we can reproduce the set of site package installations that we had before.\nFirst, let’s create a new virtual enviroment and activate it:\n$$ python3 -m venv .venv/\n\n$$ source .venv/bin/activate\n\n(venv) $ \nWe can see that we once again have a minimal number of site-packages in our virtual environment.\n$$ pip list\nAlso, our script.py once again doesn’t run because pandas-datareader is not in the virtual environments site library.\n$$ python3 script.py\nAll we have to do to reproduce the development environment is run the following code:\n$$ pip install -r requirements.txt\nNow we can check pip list and also verify that our code runs.\n$$ pip list\n\n$$ python3 script.py",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#virtual-environments-in-jupyter-notebooks",
    "href": "chapters/42_virtual_environments/virtual_environments.html#virtual-environments-in-jupyter-notebooks",
    "title": "52  Virtual Enviroments",
    "section": "52.12 Virtual Environments in Jupyter Notebooks",
    "text": "52.12 Virtual Environments in Jupyter Notebooks\nWorking with virtual environments in Jupyter is a little more tricky.\nLet’s begin by creating fresh virtual environment in our project and activating it.\n$$ deactivate\n\n$$ rm -rf .venv/\n\n$$ python3 -m venv .venv\n\n$$ source .venv/bin/activate\nYou can verify that our script once again does not run in the project.\nNext, let’s create a blank Jupyter notebook in our project and call it notebook.ipynb. And let’s type the following code in a cell and run it:\nimport pandas as pd\nimport yfinance as yf\nyf.pdr_override()\nfrom pandas_datareader import data as pdr\n\ndf = pdr.get_data_yahoo('SPY')\nprint(df)\nNotice that this is the same code that is in script.py, yet it runs even though the virtual environment is activated and script.py doesn’t run.\nThe reason for this is that behind the scenes of a Jupyter notebook, the computational engine is an IPython kernel. In order to use a virtual environment in a Jupyter notebook you have to take the extra step of registering it as a kernel. That is what we will do next with the following commands:\n$$ pip install ipykernel\n\n$$ python3 -m ipykernel install --user --name=.venv\n\n$$ cat /home/pritam/.local/share/jupyter/kernels/.venv/kernel.json\nNotice that .venv is now available in the JupyterLab launcher.\nGo into notebook.ipynb and choose the .venv kernel that appears in the drop down menu. Now, rerun the code chunk from above and you will find that it doesn’t work. This is because we are now working in the context of .venv which doesn’t have pandas-datareader in it.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/42_virtual_environments/virtual_environments.html#removing-a-virtual-environment-from-a-jupyter-notebook",
    "href": "chapters/42_virtual_environments/virtual_environments.html#removing-a-virtual-environment-from-a-jupyter-notebook",
    "title": "52  Virtual Enviroments",
    "section": "52.13 Removing a Virtual Environment from a Jupyter Notebook",
    "text": "52.13 Removing a Virtual Environment from a Jupyter Notebook\nIt is straight forward to remove a virtual environment kernel. To see all the available kernels run the following:\n$$ jupyter kernelspec list\nNotice that .venv isn’t the list.\nTo remove the .venv kernel simply run the following code:\n$$ jupyter kernelspec uninstall .venv\nYou can rerun jupyter kernelspec list to verify that the kernel has been removed. It is also no longer available in the JupyterLab launcher.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Virtual Enviroments</span>"
    ]
  },
  {
    "objectID": "chapters/43_pipelines_pickles/pipelines_pickles.html",
    "href": "chapters/43_pipelines_pickles/pipelines_pickles.html",
    "title": "53  Pipelines and pickles",
    "section": "",
    "text": "53.1 Reading and Examining the Historical Data\nThis chapter is an introduction the Pipelines and pickle files. These are tools you will need as you starting moving towards real world applications of machine learning, including the productionalizing of models.\nWe will use the classic Titanic data set (sorry!) to predict which passengers survived (label) its sinking based on several numerical features of each passenger; the features require preprocessing, and we will see how to affect that preprocessing using Pipelines. We will also see how to combine the preprocessing with the actual machine learning estimator (e.g LogisticRegression) into a Pipeline which can then be fitted in a single line of code.\nFinally we will save our pipelined structure (model) to disk as a pickle object. This will allow us to use our model object at a later time on new/unobserved data.\nThere are two data sets in question for this tutorial, historical/observed data that will be used for fitting our model, and new/unobserved data that the model will be used on to generate predictions.\nLet’s start by reading in the historical data and take a quick look (the new data is in the same format). The label is Survived and the rest of the columns are potential features.\nimport pandas as pd\n\ndf_historical = pd.read_csv('titanic_historical.csv')\ndf_historical.head().T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nPassengerId\n1\n2\n3\n4\n5\n\n\nSurvived\n0\n1\n1\n1\n0\n\n\nPclass\n3\n1\n3\n1\n3\n\n\nName\nBraund, Mr. Owen Harris\nCumings, Mrs. John Bradley (Florence Briggs Th...\nHeikkinen, Miss. Laina\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nAllen, Mr. William Henry\n\n\nSex\nmale\nfemale\nfemale\nfemale\nmale\n\n\nAge\n22.0\n38.0\n26.0\n35.0\n35.0\n\n\nSibSp\n1\n1\n0\n1\n0\n\n\nParch\n0\n0\n0\n0\n0\n\n\nTicket\nA/5 21171\nPC 17599\nSTON/O2. 3101282\n113803\n373450\n\n\nFare\n7.25\n71.2833\n7.925\n53.1\n8.05\n\n\nCabin\nNaN\nC85\nNaN\nC123\nNaN\n\n\nEmbarked\nS\nC\nS\nS\nS\nWe will also check the data type of each of the columns. (For details on this data set check out: https://www.kaggle.com/competitions/titanic/data.)\ndf_historical.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nLet’s retain only the numeric features and also separate out our labels.\ndf_X = df_historical[['Age', 'SibSp', 'Parch', 'Fare']]\ndf_y = df_historical['Survived']\nFinally, we note that Age has some missing values.\ndf_X.isnull().sum()\n\nAge      177\nSibSp      0\nParch      0\nFare       0\ndtype: int64",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>`Pipelines` and `pickles`</span>"
    ]
  },
  {
    "objectID": "chapters/43_pipelines_pickles/pipelines_pickles.html#creating-the-pipeline",
    "href": "chapters/43_pipelines_pickles/pipelines_pickles.html#creating-the-pipeline",
    "title": "53  Pipelines and pickles",
    "section": "53.2 Creating the Pipeline",
    "text": "53.2 Creating the Pipeline\nWe are using four numeric features Age, SibSp, Parch and Fare to make predictions. We see from above that Age has missing values, which we’ll need to address. We will also want to standardize all the variables so they are on the same order of magnitude.\nTo instantiate a Pipeline you give it a list of steps that are executed in order. The list consists of 2-tuples where the first element is a string name and the second element is a transformer object or a estimator object - don’t worry too much about what that means at the moment.\nOur Pipeline will consist of two transformers, followed by an estimator. In particular:\n\nSimpleImputer() - a built-in transformer that fills NaN values; our particular instance fills them with the median value.\nStandardScaler() - a built-in transform that scales each column by subtracting the mean and dividing by the standard deviation.\nLogisticRegression() - the estimator that we are familiar with.\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = Pipeline(steps=[\n    ('fill_na', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n    ('logistic_regression', LogisticRegression()),\n])\n\nNote: our transformers are being applied to both columns. If we wanted different transformers to be applied to each column we would need to use a ColumnTransformer object. We will cover those is a later chapter.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>`Pipelines` and `pickles`</span>"
    ]
  },
  {
    "objectID": "chapters/43_pipelines_pickles/pipelines_pickles.html#fitting-the-pipeline-model",
    "href": "chapters/43_pipelines_pickles/pipelines_pickles.html#fitting-the-pipeline-model",
    "title": "53  Pipelines and pickles",
    "section": "53.3 Fitting the Pipeline (Model)",
    "text": "53.3 Fitting the Pipeline (Model)\nWe can now fit our entire Pipeline to df_X. Notice that other than selecting variables, we’re not doing any preprocessing on the original data, the preprocessing is all built into the Pipeline.\n\nmodel.fit(df_X, df_y)\n\nPipeline(steps=[('fill_na', SimpleImputer(strategy='median')),\n                ('scaler', StandardScaler()),\n                ('logistic_regression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('fill_na', SimpleImputer(strategy='median')),\n                ('scaler', StandardScaler()),\n                ('logistic_regression', LogisticRegression())])SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()LogisticRegressionLogisticRegression()\n\n\nLet’s take a look at the in-sample accuracy as a sanity check that we’re on the right track. The performance looks decent, but of course this in-sample accuracy overstates out-of-sample performance.\n\nmodel.score(df_X, df_y)\n\n0.691358024691358",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>`Pipelines` and `pickles`</span>"
    ]
  },
  {
    "objectID": "chapters/43_pipelines_pickles/pipelines_pickles.html#saving-the-model-to-disk",
    "href": "chapters/43_pipelines_pickles/pipelines_pickles.html#saving-the-model-to-disk",
    "title": "53  Pipelines and pickles",
    "section": "53.4 Saving the Model to Disk",
    "text": "53.4 Saving the Model to Disk\nNext, let’s use the joblib package to save our model to disk as a pickle file. This has the effect of saving our model for future use.\n\nimport joblib\n\njoblib.dump(model, 'titanic_model_1.pkl')\n\n['titanic_model_1.pkl']",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>`Pipelines` and `pickles`</span>"
    ]
  },
  {
    "objectID": "chapters/43_pipelines_pickles/pipelines_pickles.html#loading-the-model-from-disk",
    "href": "chapters/43_pipelines_pickles/pipelines_pickles.html#loading-the-model-from-disk",
    "title": "53  Pipelines and pickles",
    "section": "53.5 Loading the Model from Disk",
    "text": "53.5 Loading the Model from Disk\nNow let’s read-in our saved model pickle file using the joblib package.\n\nsaved_model = joblib.load('titanic_model_1.pkl')",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>`Pipelines` and `pickles`</span>"
    ]
  },
  {
    "objectID": "chapters/43_pipelines_pickles/pipelines_pickles.html#making-new-predictions",
    "href": "chapters/43_pipelines_pickles/pipelines_pickles.html#making-new-predictions",
    "title": "53  Pipelines and pickles",
    "section": "53.6 Making New Predictions",
    "text": "53.6 Making New Predictions\nFinally, let’s make predictions on the new data with our saved model object.\n\ndf_new = pd.read_csv('titanic_new.csv')\n\nIn order to make predictions with our model, we need to first select only the columns that we will need. This is a form of preprocessing that really should be taken care of in the Pipeline. We will do this in a subsequent chapter.\n\ndf_new = df_new[['Age', 'SibSp', 'Parch', 'Fare']].copy()\n\nWe will now append the predictions to the feature DataFrame associated with the predictions. Notice that the last column of df_new consists of our predictions generated by saved_model.\n\ndf_new['prediction'] = saved_model.predict(df_new)\ndf_new\n\n\n\n\n\n\n\n\nAge\nSibSp\nParch\nFare\nprediction\n\n\n\n\n0\n34.5\n0\n0\n7.8292\n0\n\n\n1\n47.0\n1\n0\n7.0000\n0\n\n\n2\n62.0\n0\n0\n9.6875\n0\n\n\n3\n27.0\n0\n0\n8.6625\n0\n\n\n4\n22.0\n1\n1\n12.2875\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n413\nNaN\n0\n0\n8.0500\n0\n\n\n414\n39.0\n0\n0\n108.9000\n1\n\n\n415\n38.5\n0\n0\n7.2500\n0\n\n\n416\nNaN\n0\n0\n8.0500\n0\n\n\n417\nNaN\n1\n1\n22.3583\n0\n\n\n\n\n418 rows × 5 columns",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>`Pipelines` and `pickles`</span>"
    ]
  },
  {
    "objectID": "chapters/43_pipelines_pickles/pipelines_pickles.html#references",
    "href": "chapters/43_pipelines_pickles/pipelines_pickles.html#references",
    "title": "53  Pipelines and pickles",
    "section": "53.7 References",
    "text": "53.7 References\nhttps://medium.com/p/a27721fdff1b\nhttps://towardsdatascience.com/creating-custom-transformers-for-sklearn-pipelines-d3d51852ecc1\nhttps://towardsdatascience.com/step-by-step-tutorial-of-sci-kit-learn-pipeline-62402d5629b6",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>`Pipelines` and `pickles`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html",
    "href": "chapters/44_columntransformers/columntransformers.html",
    "title": "54  ColumnTransformers",
    "section": "",
    "text": "54.1 Reading and Examining the Historical Data\nThis chapter demonstrates the use of Pipelines, ColumnTransformers, and pickle files. These are tools you will need as you starting moving towards real world applications of machine learning, including the productionalizing of models.\nWe will use the classic Titanic dataset (sorry!) to predict which passengers survived (label) its sinking based on various aspects (features) of each passenger. Most of the features require preprocessing, and we will see how to affect that preprocessing using Pipelines and ColumnTransformers. We will also see how to combine the preprocessing with the actual machine learning technique into a Pipeline to create and object that can read-in the raw data and the .fit() the model in a single line of code. This makes for much more organized and DRY code.\nFinally, we will save our pipelined structure (model) to disk as a pickle object. This will allow us to use our object at a later time on new/unobserved data, and will have the added benefit that the data will not require any preprocessing because it is built into the pipelined model object.\nThere are two data sets in question for this tutorial, historical/observed data that will be used for fitting our model, and new/unobserved data that the model will be used on to generate predictions.\nLet’s start by reading in the historical data and take a quick look (the new data is in the same format). The label is Survived and the rest of the columns are potential features.\nimport pandas as pd\n\ndf_historical = pd.read_csv('titanic_historical.csv')\ndf_historical.head().T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nPassengerId\n1\n2\n3\n4\n5\n\n\nSurvived\n0\n1\n1\n1\n0\n\n\nPclass\n3\n1\n3\n1\n3\n\n\nName\nBraund, Mr. Owen Harris\nCumings, Mrs. John Bradley (Florence Briggs Th...\nHeikkinen, Miss. Laina\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nAllen, Mr. William Henry\n\n\nSex\nmale\nfemale\nfemale\nfemale\nmale\n\n\nAge\n22.0\n38.0\n26.0\n35.0\n35.0\n\n\nSibSp\n1\n1\n0\n1\n0\n\n\nParch\n0\n0\n0\n0\n0\n\n\nTicket\nA/5 21171\nPC 17599\nSTON/O2. 3101282\n113803\n373450\n\n\nFare\n7.25\n71.2833\n7.925\n53.1\n8.05\n\n\nCabin\nNaN\nC85\nNaN\nC123\nNaN\n\n\nEmbarked\nS\nC\nS\nS\nS\nWe’ll also do a bit of inspection to better understand our data. For details on this data set check out: https://www.kaggle.com/competitions/titanic/data.\ndf_historical.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nAs we can see below, the data isn’t too imbalanced.\ndf_historical['Survived'].value_counts()\n\nSurvived\n0    549\n1    342\nName: count, dtype: int64\ndf_historical['Embarked'].value_counts()\n\nEmbarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\ndf_historical['Pclass'].value_counts()\n\nPclass\n3    491\n1    216\n2    184\nName: count, dtype: int64\nLet’s see how many null values are in each column.\ndf_historical.isnull().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\nLet’s now separate out our features from our labels. Notice that at this stage we’re keeping all the columns; we will select for the ones we want in the Pipeline we create below.\ndf_X = df_historical.drop(columns=['Survived'])\ndf_y = df_historical['Survived']\nLet’s identify the features that we want to keep for making predictions; this list will be fed into our Pipeline.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#creating-a-custom-transformer-featureselector",
    "href": "chapters/44_columntransformers/columntransformers.html#creating-a-custom-transformer-featureselector",
    "title": "54  ColumnTransformers",
    "section": "54.2 Creating a Custom Transformer: FeatureSelector",
    "text": "54.2 Creating a Custom Transformer: FeatureSelector\nThe first step in the Pipeline is selecting only the features that we want to keep. For some reason there isn’t a good built-in way to do this so we’ll create a custom transformer to do the job.\nThis amounts to creating a custom class called FeatureSelector which inherits from BaseEstimator and TransformerMixin - this is standard for creating custom transformers. Implementing this inheritance gives us the .fit_transform() method for free.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X[self.columns]\n\nLet’s verify that our FeatureSelector is working.\n\nFeatureSelector(features).fit_transform(df_X).head()\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\n\n\n\n\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\n\n\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\n\n\n2\n3\nfemale\n26.0\n0\n0\n7.9250\nS\n\n\n3\n1\nfemale\n35.0\n1\n0\n53.1000\nS\n\n\n4\n3\nmale\n35.0\n0\n0\n8.0500\nS",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#building-the-data-preprocessing-pipeline",
    "href": "chapters/44_columntransformers/columntransformers.html#building-the-data-preprocessing-pipeline",
    "title": "54  ColumnTransformers",
    "section": "54.3 Building the Data Preprocessing Pipeline",
    "text": "54.3 Building the Data Preprocessing Pipeline\nThis section is where we do most of the heavy lifting. In particular, we define the preprocessing Pipelines for the various columns.\nTo instantiate a Pipeline you give it a list of steps that are executed in order. The list consists of 2-tuples where the first element is a string name and the second coordinate is a transformer object or a estimator object - don’t worry too much about what that means at the moment.\nWe start with the numeric features Age, Fare, SibSp, Parch. For these columns we will fill the NaN values with the median of each column and then standardize; so, this Pipeline consists of two built-in tranformers.\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler \n\n# Age, Fare, SibSp, Parch\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n])\n\nSex and Embarked are both categorical feature, so we fill the NaN values with the mode of each column and then one hot encode each of them. This Pipeline also consists of two built-in transformers.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Sex, Embarked\ncategorical_transformer_1 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\n\nFinally, we address Pclass. Notice that it is an ordinal categorical feature, but it is already set to integer values, so the only thing we will do is fill the NaNvalues. This Pipeline consists of a single built-in transformer.\n\n# Pclass\ncategorical_transformer_2 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n])\n\nNow we can put these processing Pipelines together in a ColumnTransformer object. We need to use a ColumnTransformer because we are performing different transformations on different features. To instantiate a ColumnTransformer you give it a list of three-tuples where elements are as follows:\n\na string name\na transformer object\na list of the columns to perform the transform on.\n\nThe ColumnTransformer below consists of the three transformer Pipelines that we created above.\n\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('numerical', numerical_transformer, ['Age', 'Fare', 'SibSp', 'Parch']),\n    ('categorical_1', categorical_transformer_1, ['Sex', 'Embarked']),\n    ('categorical_2', categorical_transformer_2, ['Pclass'])],\n)",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#testing-the-output-of-our-preprocessing",
    "href": "chapters/44_columntransformers/columntransformers.html#testing-the-output-of-our-preprocessing",
    "title": "54  ColumnTransformers",
    "section": "54.4 Testing the Output of Our Preprocessing",
    "text": "54.4 Testing the Output of Our Preprocessing\nFor testing purposes, let’s create a Pipeline that we will call testing_output; it will consist of our custom FeatureSelector and our ColumnTransformer which we called preprocessor. The reason that we are doing this is to make sure that feeding in df_X yields a reasonable output.\nNotice that there were originally 7 columns, and now there are 10 columns. The three extra columns result from the one-hot encoding of Sex and Embark. The numerical columns seem scaled and reasonable. (I’ll leave it to the reader to check that there are no NaNs.)\nOf course the resulting DataFrame is hard to decipher because in this situation the .fit_transform() method results in and np.ndarray, and we are converting it to a DataFrame with pd.DataFrame(). But by inspection the result seems reasonable and that will suffice for our purposes.\n\ntesting_output = Pipeline(steps=[\n    ('feature_selector', FeatureSelector(features)),\n    ('preprocessor', preprocessor)\n])\npd.DataFrame(testing_output.fit_transform(df_X))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n-0.565736\n-0.502445\n0.432793\n-0.473674\n0.0\n1.0\n0.0\n0.0\n1.0\n3.0\n\n\n1\n0.663861\n0.786845\n0.432793\n-0.473674\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n2\n-0.258337\n-0.488854\n-0.474545\n-0.473674\n1.0\n0.0\n0.0\n0.0\n1.0\n3.0\n\n\n3\n0.433312\n0.420730\n0.432793\n-0.473674\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n4\n0.433312\n-0.486337\n-0.474545\n-0.473674\n0.0\n1.0\n0.0\n0.0\n1.0\n3.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n-0.181487\n-0.386671\n-0.474545\n-0.473674\n0.0\n1.0\n0.0\n0.0\n1.0\n2.0\n\n\n887\n-0.796286\n-0.044381\n-0.474545\n-0.473674\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n888\n-0.104637\n-0.176263\n0.432793\n2.008933\n1.0\n0.0\n0.0\n0.0\n1.0\n3.0\n\n\n889\n-0.258337\n-0.044381\n-0.474545\n-0.473674\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n890\n0.202762\n-0.492378\n-0.474545\n-0.473674\n0.0\n1.0\n0.0\n1.0\n0.0\n3.0\n\n\n\n\n891 rows × 10 columns",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#creating-the-final-pipeline",
    "href": "chapters/44_columntransformers/columntransformers.html#creating-the-final-pipeline",
    "title": "54  ColumnTransformers",
    "section": "54.5 Creating the Final Pipeline",
    "text": "54.5 Creating the Final Pipeline\nWith the hard work we did in the previous section, we are now ready to put together the final Pipeline. All that remains to be added is the LogisticRegression() estimator. Thus, this final PipeLine has three sequential components:\n\nA custom FeatureSelector that grabs only the columns that we will use for our preditions.\nA ColumnTransformer object called preprocesser which itself consists of three different Pipelines, which in turn consist of one or more built-in transformers.\nAn estimator object LogisticRegression().\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = Pipeline(steps=[\n    ('feature_selector', FeatureSelector(features)),\n    ('preprocessor', preprocessor),\n    ('logistic_regression', LogisticRegression())\n])",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#fitting-the-pipeline-model",
    "href": "chapters/44_columntransformers/columntransformers.html#fitting-the-pipeline-model",
    "title": "54  ColumnTransformers",
    "section": "54.6 Fitting the Pipeline (Model)",
    "text": "54.6 Fitting the Pipeline (Model)\nWe can now fit our entire Pipeline to the original data. Notice that we’re not doing any preprocessing on the original data, the preprocessing is all built into the Pipeline.\n\nmodel.fit(df_X, df_y)\n\nPipeline(steps=[('feature_selector',\n                 FeatureSelector(columns=['Pclass', 'Sex', 'Age', 'SibSp',\n                                          'Parch', 'Fare', 'Embarked'])),\n                ('preprocessor',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Age', 'Fare', 'SibSp',\n                                                   'Parch']),\n                                                 ('categorical_1',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one_hot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Embarked']),\n                                                 ('categorical_2',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['Pclass'])])),\n                ('logistic_regression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('feature_selector',\n                 FeatureSelector(columns=['Pclass', 'Sex', 'Age', 'SibSp',\n                                          'Parch', 'Fare', 'Embarked'])),\n                ('preprocessor',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Age', 'Fare', 'SibSp',\n                                                   'Parch']),\n                                                 ('categorical_1',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('one_hot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Embarked']),\n                                                 ('categorical_2',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent'))]),\n                                                  ['Pclass'])])),\n                ('logistic_regression', LogisticRegression())])FeatureSelectorFeatureSelector(columns=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n                         'Embarked'])preprocessor: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Age', 'Fare', 'SibSp', 'Parch']),\n                                ('categorical_1',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('one_hot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Sex', 'Embarked']),\n                                ('categorical_2',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent'))]),\n                                 ['Pclass'])])numerical['Age', 'Fare', 'SibSp', 'Parch']SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()categorical_1['Sex', 'Embarked']SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(handle_unknown='ignore')categorical_2['Pclass']SimpleImputerSimpleImputer(strategy='most_frequent')LogisticRegressionLogisticRegression()\n\n\nLet’s take a look at the in-sample accuracy as a sanity check that we’re on right track. The performace looks good, but of course this in-sample accuracy overstates out-of-sample performance.\n\nmodel.score(df_X, df_y)\n\n0.7991021324354658",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#saving-the-model-to-disk",
    "href": "chapters/44_columntransformers/columntransformers.html#saving-the-model-to-disk",
    "title": "54  ColumnTransformers",
    "section": "54.7 Saving the Model to Disk",
    "text": "54.7 Saving the Model to Disk\nNext, let’s use the joblib package to save our model to disk as a pickle file. This has the effect of saving our model for future use.\n\nimport joblib\n\njoblib.dump(model, 'titanic_model_2.pkl')\n\n['titanic_model_2.pkl']",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#loading-the-model-from-disk",
    "href": "chapters/44_columntransformers/columntransformers.html#loading-the-model-from-disk",
    "title": "54  ColumnTransformers",
    "section": "54.8 Loading the Model from Disk",
    "text": "54.8 Loading the Model from Disk\nNow let’s read-in our saved model object using the joblib package.\n\nsaved_model = joblib.load('titanic_model_2.pkl')",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#making-new-predictions",
    "href": "chapters/44_columntransformers/columntransformers.html#making-new-predictions",
    "title": "54  ColumnTransformers",
    "section": "54.9 Making New Predictions",
    "text": "54.9 Making New Predictions\nFinally, let’s make predictions on the new data with our saved model object.\n\ndf_new = pd.read_csv('titanic_new.csv')\n\nWe will append the predictions to the feature DataFrame associated with the predictions. Notice that the last column of df_new consists of our predictions generated by saved_model.\n\ndf_new['prediction'] = saved_model.predict(df_new)\ndf_new.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n\n\n\n\nPassengerId\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n...\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n\n\nPclass\n3\n3\n2\n3\n3\n3\n3\n2\n3\n3\n...\n3\n3\n3\n1\n3\n3\n1\n3\n3\n3\n\n\nName\nKelly, Mr. James\nWilkes, Mrs. James (Ellen Needs)\nMyles, Mr. Thomas Francis\nWirz, Mr. Albert\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nSvensson, Mr. Johan Cervin\nConnolly, Miss. Kate\nCaldwell, Mr. Albert Francis\nAbrahim, Mrs. Joseph (Sophie Halaut Easu)\nDavies, Mr. John Samuel\n...\nRiordan, Miss. Johanna Hannah\"\"\nPeacock, Miss. Treasteall\nNaughton, Miss. Hannah\nMinahan, Mrs. William Edward (Lillian E Thorpe)\nHenriksson, Miss. Jenny Lovisa\nSpector, Mr. Woolf\nOliva y Ocana, Dona. Fermina\nSaether, Mr. Simon Sivertsen\nWare, Mr. Frederick\nPeter, Master. Michael J\n\n\nSex\nmale\nfemale\nmale\nmale\nfemale\nmale\nfemale\nmale\nfemale\nmale\n...\nfemale\nfemale\nfemale\nfemale\nfemale\nmale\nfemale\nmale\nmale\nmale\n\n\nAge\n34.5\n47.0\n62.0\n27.0\n22.0\n14.0\n30.0\n26.0\n18.0\n21.0\n...\nNaN\n3.0\nNaN\n37.0\n28.0\nNaN\n39.0\n38.5\nNaN\nNaN\n\n\nSibSp\n0\n1\n0\n0\n1\n0\n0\n1\n0\n2\n...\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\nParch\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nTicket\n330911\n363272\n240276\n315154\n3101298\n7538\n330972\n248738\n2657\nA/4 48871\n...\n334915\nSOTON/O.Q. 3101315\n365237\n19928\n347086\nA.5. 3236\nPC 17758\nSOTON/O.Q. 3101262\n359309\n2668\n\n\nFare\n7.8292\n7.0\n9.6875\n8.6625\n12.2875\n9.225\n7.6292\n29.0\n7.2292\n24.15\n...\n7.7208\n13.775\n7.75\n90.0\n7.775\n8.05\n108.9\n7.25\n8.05\n22.3583\n\n\nCabin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nC78\nNaN\nNaN\nC105\nNaN\nNaN\nNaN\n\n\nEmbarked\nQ\nS\nQ\nS\nS\nS\nQ\nS\nC\nS\n...\nQ\nS\nQ\nQ\nS\nS\nC\nS\nS\nC\n\n\nprediction\n0\n0\n0\n0\n1\n0\n1\n0\n1\n0\n...\n1\n1\n1\n1\n1\n0\n1\n0\n0\n0\n\n\n\n\n12 rows × 418 columns",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  },
  {
    "objectID": "chapters/44_columntransformers/columntransformers.html#references",
    "href": "chapters/44_columntransformers/columntransformers.html#references",
    "title": "54  ColumnTransformers",
    "section": "54.10 References",
    "text": "54.10 References\nhttps://www.youtube.com/watch?v=URdnFlZnlaE\nhttps://medium.com/p/a27721fdff1b\nhttps://towardsdatascience.com/creating-custom-transformers-for-sklearn-pipelines-d3d51852ecc1\nhttps://towardsdatascience.com/step-by-step-tutorial-of-sci-kit-learn-pipeline-62402d5629b6",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>`ColumnTransformers`</span>"
    ]
  }
]