{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637ab380-104d-4d96-9b98-85b3615fd0b6",
   "metadata": {},
   "source": [
    "# Introduction to **spaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb38a9-2ee4-4438-81e1-a666e5c51042",
   "metadata": {},
   "source": [
    "If you want to do natural language processing (NLP) in Python, then look no further than **spaCy**, a free and open-source library with a lot of built-in capabilities. It’s becoming increasingly popular for processing and analyzing data in the field of NLP.\n",
    "\n",
    "This chapter is an introduction to various aspects of the **spaCy** package.  It is based on the following tutorial: https://realpython.com/natural-language-processing-spacy-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ee2e7-51ee-4244-b219-3eb7befaf603",
   "metadata": {},
   "source": [
    "## Importing the Package and Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07242fd-fda4-4991-a289-18476894e232",
   "metadata": {},
   "source": [
    "There are various **spaCy** models for different languages. The default model for the English language is designated as `en_core_web_sm`. Since the models are quite large, it’s best to install them separately — including all languages in one package would make the download too massive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c8a67-9845-4c01-8dfd-b3830aff19f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54830499-6e03-4d77-87d2-7a986496bfc5",
   "metadata": {},
   "source": [
    "## The `Doc` Object for Processed Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e469a1-3702-4a71-abb6-c300c9d445c2",
   "metadata": {},
   "source": [
    "In this section, you’ll use **spaCy** to deconstruct a given input string.\n",
    "\n",
    "To start processing your input, you construct a `Doc` object. A `Doc` object is a sequence of `Token` objects representing a lexical token. Each `Token` object has information about a particular piece — typically one word — of text. You can instantiate a `Doc` object by calling the `Language` object with the input string as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197fb386-35ff-4c1f-b60b-6bb72d11325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_doc = nlp(\n",
    "    \"This tutorial is about Natural Language Processing in spaCy.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f5c4c-2b97-4e2b-87f0-fcfeb0e40ca0",
   "metadata": {},
   "source": [
    "We can check the type of the `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3d58d-3219-489d-89d5-df721187da7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(introduction_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a21fa-4af1-48ec-bc03-2cbb294b1907",
   "metadata": {},
   "source": [
    "We can use a `list` comprehension to see all the tokens in the `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b21a0-c249-4541-ad63-c57b5f92ea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'tutorial',\n",
       " 'is',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'in',\n",
       " 'spaCy',\n",
       " '.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in introduction_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addef2a-af4e-4c34-a27c-c2c0f7d88654",
   "metadata": {},
   "source": [
    "## Sentence Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41984c64-b0bf-43bc-8aad-02e42b86c674",
   "metadata": {},
   "source": [
    "*Sentence detection* is the process of locating where sentences start and end in a given text. This allows you to you divide a text into linguistically meaningful units. You’ll use these units when you’re processing your text to perform tasks such as part-of-speech (POS) tagging and named-entity recognition, which you’ll come to later in the tutorial.\n",
    "\n",
    "In **spaCy**, the `.sents` property is used to extract sentences from the Doc object. Here’s how you would extract the total number of sentences and the sentences themselves for a given input.\n",
    "\n",
    "Let's start with a simple two sentence piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c8085-ae27-4967-9f89-44952ff4deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f05a0b-5ada-43d8-824f-8f1458a92e20",
   "metadata": {},
   "source": [
    "We can create a `Doc` object from `about_text` and then extract the sentences from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d7d18-59e3-44ec-99c0-759f61458215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f1908f-fa46-490d-a6a0-4e36feeded7e",
   "metadata": {},
   "source": [
    "Let's print out the beginning of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ced1a-18ba-4203-8113-7157b1cfddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Proto is a Python...\n",
      "He is interested in learning...\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(f'{sentence[:5]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe7064-572b-4d7b-9a73-3245fe1a84de",
   "metadata": {},
   "source": [
    "Each element of a sentence is a `Span` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ff29a-5dfc-4b65-bdbd-d00aaff9070c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e09f0-7744-4edf-abfc-1b26ec781d91",
   "metadata": {},
   "source": [
    "## Tokens in **spaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f3407-4288-4ea3-a2c2-03cb3a7060de",
   "metadata": {},
   "source": [
    "Building the `Doc` container involves tokenizing the text. The process of tokenization breaks a text down into its basic units — or tokens — which are represented in **spaCy** as `Token` objects.\n",
    "\n",
    "As you’ve already seen, with **spaCy**, you can print the tokens by iterating over the `Doc` object. But `Token` objects also have other attributes available for exploration. For instance, the token’s original index position in the string is available as an attribute on `Token`.\n",
    "\n",
    "Let's begin with our same two sentence piece of text, create a `Doc` from it, and then print each token along with its index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9617a65-35c7-4712-b9d5-3f5d4f1b2527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gus Proto is a Python developer currently working for a London-based Fintech company. He is interested in learning Natural Language Processing.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "about_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f6deb-b641-4c8e-8f45-9021fcfcdc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    "about_doc = nlp(about_text)\n",
    "for token in about_doc:\n",
    "    print(token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72175910-8aba-4c0b-95db-19969d07bb22",
   "metadata": {},
   "source": [
    "There are many other pieces of information that can be gleaned from tokens.  In the code below, we use `list` comprehensions and a **pandas** `DataFrame` to display some of this other information.\n",
    "\n",
    "- `.text_with_ws` prints the token text along with any trailing space, if present.\n",
    "- `.is_alpha` indicates whether the token consists of alphabetic characters or not.\n",
    "- `.is_punct` indicates whether the token is a punctuation symbol or not.\n",
    "- `.is_stop` indicates whether the token is a stop word or not. We'll be covering stop words a bit later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dc854-fba3-4e79-9c73-c576303e200b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_whitespace</th>\n",
       "      <th>alphanumeric</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>stop_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gus</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Proto</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>developer</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>currently</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>working</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>London</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>based</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fintech</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>company</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>He</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>interested</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>in</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>learning</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Natural</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Language</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Processing</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_whitespace alphanumeric punctuation stop_word\n",
       "0             Gus          True       False     False\n",
       "1           Proto          True       False     False\n",
       "2              is          True       False      True\n",
       "3               a          True       False      True\n",
       "4          Python          True       False     False\n",
       "5       developer          True       False     False\n",
       "6       currently          True       False     False\n",
       "7         working          True       False     False\n",
       "8             for          True       False      True\n",
       "9               a          True       False      True\n",
       "10          London         True       False     False\n",
       "11               -        False        True     False\n",
       "12          based          True       False     False\n",
       "13        Fintech          True       False     False\n",
       "14         company         True       False     False\n",
       "15              .         False        True     False\n",
       "16             He          True       False      True\n",
       "17             is          True       False      True\n",
       "18     interested          True       False     False\n",
       "19             in          True       False      True\n",
       "20       learning          True       False     False\n",
       "21        Natural          True       False     False\n",
       "22       Language          True       False     False\n",
       "23      Processing         True       False     False\n",
       "24               .        False        True     False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({\n",
    "    'text_whitespace': [str(token.text_with_ws) for token in about_doc],\n",
    "    'alphanumeric': [str(token.is_alpha) for token in about_doc],\n",
    "    'punctuation': [str(token.is_punct) for token in about_doc],\n",
    "    'stop_word': [str(token.is_stop) for token in about_doc],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e340d3a-590d-4ba0-b57f-9a230e13644e",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48378dfc-3d8c-4a17-a9dd-947eb716c8c5",
   "metadata": {},
   "source": [
    "*Stop words* are typically defined as the most common words in a language. In the English language, some examples of stop words are the, are, but, and they. Most sentences need to contain stop words in order to be full sentences that make grammatical sense.\n",
    "\n",
    "With NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. **spaCy** stores a  `set` of stop words for the English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6daa27-3072-464f-bd76-d3bcfa8a0af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359e965-1e13-42c4-9935-a0112cca22a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea2b59-bdb5-4214-8b97-00bde90bd5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a275ce6-8615-4867-962b-72e51c02ae56",
   "metadata": {},
   "source": [
    "Let's observe a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dc053-4aba-49d8-8029-9dbe5a16909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "using\n",
      "were\n",
      "since\n",
      "‘ll\n",
      "'ve\n",
      "although\n",
      "one\n",
      "’re\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182344a8-e41f-4cdb-ba62-feda7db96273",
   "metadata": {},
   "source": [
    "As we can see below, we can remove stop words from text by making use of the `.is_stop` attribute of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f10968-bee5-4ce4-aac0-8fd0026810a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(custom_about_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174718c-99d2-4e56-af0f-bb3f55e33e8d",
   "metadata": {},
   "source": [
    "Let's use a `list` comprehension with a conditional expression to produce a `list` of all the words that are not stop words in `custom_about_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682aff1-8c3a-46eb-ac9e-dc8fd1adb1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
     ]
    }
   ],
   "source": [
    "print([token for token in about_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d6173-17e6-42a8-9840-d51800eed10b",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611473b3-5525-4a4e-98aa-d2279ab04d06",
   "metadata": {},
   "source": [
    "*Lemmatization* is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form, or root word, is called a *lemma*.\n",
    "\n",
    "For example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma. The inflection of a word allows you to express different grammatical categories, like tense (organized vs organize), number (trains vs train), and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you *normalize* the text.\n",
    "\n",
    "**spaCy** puts a `lemma_` attribute on the `Token` class. This attribute has the lemmatized form of the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7700f-b0e6-4db7-ac0b-ab561d21501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_help_text = (\n",
    "    \"Gus is helping organize a developer\"\n",
    "    \" conference on Applications of Natural Language\"\n",
    "    \" Processing. He keeps organizing local Python meetups\"\n",
    "    \" and several internal talks at his workplace.\"\n",
    ")\n",
    "conference_help_doc = nlp(conference_help_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad183a91-2011-4722-a2fd-28d7f3bb6b93",
   "metadata": {},
   "source": [
    "Let's print out all the words that are different from their lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc9e93-0abb-4eed-905f-5d1387bef4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is : be\n",
      "He : he\n",
      "keeps : keep\n",
      "organizing : organize\n",
      "meetups : meetup\n",
      "talks : talk\n"
     ]
    }
   ],
   "source": [
    "for token in conference_help_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(f'{str(token)} : {str(token.lemma_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0a3ab-7faa-479e-84d2-75230a6769c7",
   "metadata": {},
   "source": [
    "Notice that this is not perfect, *helping* is not lemmaztized to *help*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535959a6-7cac-41e9-9a82-471fa22ccffc",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4cddb-6a48-4a07-a14a-15d407e81dd0",
   "metadata": {},
   "source": [
    "We can now convert a given text into tokens and perform statistical analysis on it. This analysis can give you various insights, such as common words or unique words in the text.  In this section we'll do a simple word-frequency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6915d8-d707-421a-aeb1-de2c62b74d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \" Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "complete_doc = nlp(complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd60da-f47d-44b7-a27c-2bc8936fd59d",
   "metadata": {},
   "source": [
    "Let's first try counting word frequencies without removing stop words.  Notice the prominence of uninformative works such as *is* and *a*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917dcf9c-8d7f-40d9-a1ed-a74527dc3fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(\n",
    "    [token.text for token in complete_doc if not token.is_punct]\n",
    ").most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9740c7-ef1f-491c-ae9b-677f98250523",
   "metadata": {},
   "source": [
    "So next, let's remove the stop words with a `list` comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264f9bc-267d-4dda-bb06-de8daef5df47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gus', 'Proto', 'Python', 'developer', 'currently', 'working', 'London', 'based', 'Fintech', 'company', 'interested', 'learning', 'Natural', 'Language', 'Processing', 'developer', 'conference', 'happening', '21', 'July', '2019', 'London', 'titled', 'Applications', 'Natural', 'Language', 'Processing', 'helpline', 'number', 'available', '+44', '1234567891', 'Gus', 'helping', 'organize', 'keeps', 'organizing', 'local', 'Python', 'meetups', 'internal', 'talks', 'workplace', 'Gus', 'presenting', 'talk', 'talk', 'introduce', 'reader', 'Use', 'cases', 'Natural', 'Language', 'Processing', 'Fintech', 'Apart', 'work', 'passionate', 'music', 'Gus', 'learning', 'play', 'Piano', 'enrolled', 'weekend', 'batch', 'Great', 'Piano', 'Academy', 'Great', 'Piano', 'Academy', 'situated', 'Mayfair', 'City', 'London', 'world', 'class', 'piano', 'instructors']\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    token.text\n",
    "    for token in complete_doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b1011-6ec0-4ec1-bf95-6390bfb760d5",
   "metadata": {},
   "source": [
    "Now, counting words is much more meaningful.  We can guess that this text has a lot to do with Gus and natural language processing.  Of course, this is a crude analysis and a lot of context is being excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cf75a-c47c-4c87-8d8b-2d1874850d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(words).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd800ead-5650-4084-aa6c-9a27e6d92604",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449f53f-f7e5-4687-a9ac-3554c60bce49",
   "metadata": {},
   "source": [
    "*Part of speech* or *POS* is a grammatical role that explains how a particular word is used in a sentence. There are typically eight parts of speech:\n",
    "\n",
    "1. Noun\n",
    "1. Pronoun\n",
    "1. Adjective\n",
    "1. Verb\n",
    "1. Adverb\n",
    "1. Preposition\n",
    "1. Conjunction\n",
    "1. Interjection\n",
    "\n",
    "Part-of-speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word.\n",
    "\n",
    "In **spaCy**, POS tags are available as an attribute on the Token object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc022b14-d699-4758-a479-06b80797ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234c174-fdff-4e1b-8d5f-531d13938eb2",
   "metadata": {},
   "source": [
    "In the code below, two attributes of the `Token` class are accessed and printed using `DataFrames` and `list` comprehensions:\n",
    "\n",
    "1. `.tag_` displays a fine-grained tag.\n",
    "1. `.pos_` displays a coarse-grained tag, which is a reduced version of the fine-grained tags.\n",
    "   \n",
    "We also use `spacy.explain()` to give descriptive details about a particular POS tag, which can be a valuable reference tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99233cdb-8788-4c37-8e95-c03a41c7f75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tag</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gus</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Proto</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>AUX</td>\n",
       "      <td>verb, 3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>developer</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>currently</td>\n",
       "      <td>RB</td>\n",
       "      <td>ADV</td>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>working</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb, gerund or present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation mark, hyphen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>based</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb, past participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fintech</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>company</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun, singular or mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun, personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>AUX</td>\n",
       "      <td>verb, 3rd person singular present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>interested</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective (English), other noun-modifier (Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>conjunction, subordinating or preposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>learning</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb, gerund or present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Natural</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Language</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Processing</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>noun, proper singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation mark, sentence closer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token   tag part_of_speech   \n",
       "0          Gus   NNP          PROPN  \\\n",
       "1        Proto   NNP          PROPN   \n",
       "2           is   VBZ            AUX   \n",
       "3            a    DT            DET   \n",
       "4       Python   NNP          PROPN   \n",
       "5    developer    NN           NOUN   \n",
       "6    currently    RB            ADV   \n",
       "7      working   VBG           VERB   \n",
       "8          for    IN            ADP   \n",
       "9            a    DT            DET   \n",
       "10      London   NNP          PROPN   \n",
       "11           -  HYPH          PUNCT   \n",
       "12       based   VBN           VERB   \n",
       "13     Fintech   NNP          PROPN   \n",
       "14     company    NN           NOUN   \n",
       "15           .     .          PUNCT   \n",
       "16          He   PRP           PRON   \n",
       "17          is   VBZ            AUX   \n",
       "18  interested    JJ            ADJ   \n",
       "19          in    IN            ADP   \n",
       "20    learning   VBG           VERB   \n",
       "21     Natural   NNP          PROPN   \n",
       "22    Language   NNP          PROPN   \n",
       "23  Processing   NNP          PROPN   \n",
       "24           .     .          PUNCT   \n",
       "\n",
       "                                          explanation  \n",
       "0                               noun, proper singular  \n",
       "1                               noun, proper singular  \n",
       "2                   verb, 3rd person singular present  \n",
       "3                                          determiner  \n",
       "4                               noun, proper singular  \n",
       "5                              noun, singular or mass  \n",
       "6                                              adverb  \n",
       "7                  verb, gerund or present participle  \n",
       "8           conjunction, subordinating or preposition  \n",
       "9                                          determiner  \n",
       "10                              noun, proper singular  \n",
       "11                           punctuation mark, hyphen  \n",
       "12                              verb, past participle  \n",
       "13                              noun, proper singular  \n",
       "14                             noun, singular or mass  \n",
       "15                  punctuation mark, sentence closer  \n",
       "16                                  pronoun, personal  \n",
       "17                  verb, 3rd person singular present  \n",
       "18  adjective (English), other noun-modifier (Chin...  \n",
       "19          conjunction, subordinating or preposition  \n",
       "20                 verb, gerund or present participle  \n",
       "21                              noun, proper singular  \n",
       "22                              noun, proper singular  \n",
       "23                              noun, proper singular  \n",
       "24                  punctuation mark, sentence closer  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'token': [token for token in about_doc],\n",
    "    'tag': [token.tag_ for token in about_doc],\n",
    "    'part_of_speech': [token.pos_ for token in about_doc],\n",
    "    'explanation': [spacy.explain(token.tag_) for token in about_doc],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae8511-22c0-48ec-825d-d8f02bff9761",
   "metadata": {},
   "source": [
    "By using POS tags, you can extract a particular category of words.  You can use this type of word classification to derive insights. For instance, you could gauge sentiment by analyzing which adjectives are most commonly used alongside nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc95ff-18f1-444b-bc8a-560265d6c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in about_doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjectives.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1e35e-cfe1-48ce-9b9e-5458abb7db67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[developer, company]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502322ec-3c3b-4af8-b053-744273cd4308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[interested]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c0a58-2e01-4f2b-aa25-af094c0d0cde",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d394e5-8985-4388-8a79-3276e5c0dbea",
   "metadata": {},
   "source": [
    "To bring your text into a format ideal for analysis, you can write preprocessing functions to encapsulate your cleaning process. For example, in this section, you’ll create a preprocessor that applies the following operations:\n",
    "\n",
    "- Lowercases the text\n",
    "- Lemmatizes each token\n",
    "- Removes punctuation symbols\n",
    "- Removes stop words\n",
    "\n",
    "A preprocessing function converts text to an analyzable format. It’s typical for most NLP tasks. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890fed8-131a-4f19-8a43-6e9dda97e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \" Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "complete_doc = nlp(complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67dd3b8-9b66-499c-85ab-2dfc127b1410",
   "metadata": {},
   "source": [
    "Next we create a couple of functions.  The first will help with filtering, the second will do some simple preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdb7a4-cc15-4b7c-9600-14aa36efad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_allowed(token):\n",
    "    return bool(\n",
    "        str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35898c00-3d28-48af-90bd-947f089f12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4565f81-b0df-461e-bab3-f03d4c2ce3a4",
   "metadata": {},
   "source": [
    "Now we can use a `list` comprehension to filter and process the tokens in `complete_doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5686f-f34d-45bc-9afe-2400c7f6f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_filtered_tokens = [\n",
    "    preprocess_token(token)\n",
    "    for token in complete_doc\n",
    "    if is_token_allowed(token)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fbbd2-003a-42b2-8b87-c91e3d8946cd",
   "metadata": {},
   "source": [
    "Note that `complete_filtered_tokens` doesn’t contain any stop words or punctuation symbols, and it consists purely of lemmatized lowercase tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad1ede-549d-4ffb-b774-ad42bcbafa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gus', 'proto', 'python', 'developer', 'currently', 'work', 'london', 'base', 'fintech', 'company', 'interested', 'learn', 'natural', 'language', 'processing', 'developer', 'conference', 'happen', '21', 'july', '2019', 'london', 'title', 'application', 'natural', 'language', 'processing', 'helpline', 'number', 'available', '+44', '1234567891', 'gus', 'helping', 'organize', 'keep', 'organize', 'local', 'python', 'meetup', 'internal', 'talk', 'workplace', 'gus', 'present', 'talk', 'talk', 'introduce', 'reader', 'use', 'case', 'natural', 'language', 'processing', 'fintech', 'apart', 'work', 'passionate', 'music', 'gus', 'learn', 'play', 'piano', 'enrol', 'weekend', 'batch', 'great', 'piano', 'academy', 'great', 'piano', 'academy', 'situate', 'mayfair', 'city', 'london', 'world', 'class', 'piano', 'instructor']\n"
     ]
    }
   ],
   "source": [
    "print(complete_filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4cb259-43ae-4bb8-b2e4-564b1d89b947",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6461f-bfc9-4903-894a-eeae9d70a3b4",
   "metadata": {},
   "source": [
    "*Named-entity recognition* (NER) is the process of locating *named entities* in unstructured text and then classifying them into predefined categories, such as person names, organizations, locations, monetary values, percentages, and time expressions.\n",
    "\n",
    "You can use NER to learn more about the meaning of your text. For example, you could use it to populate tags for a set of documents in order to improve the keyword search. You could also use it to categorize customer support tickets into relevant categories.\n",
    "\n",
    "**spaCy** has the property `.ents` on `Doc` objects. You can use it to extract named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823a34d-ac74-45a1-ab95-fb25ec69c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_class_text = (\n",
    "    \"Great Piano Academy is situated\"\n",
    "    \" in Mayfair or the City of London and has\"\n",
    "    \" world-class piano instructors.\"\n",
    ")\n",
    "piano_class_doc = nlp(piano_class_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307262d3-3d35-42f5-a557-46fd267dd155",
   "metadata": {},
   "source": [
    "Notice that `ent` is a `Span` object with various attributes:\n",
    "\n",
    "- `.text` gives the Unicode text representation of the entity.\n",
    "- `.label_` gives the label of the entity.\n",
    "\n",
    "`spacy.explain` gives descriptive details about each entity label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed553b4f-0e8f-47e2-880a-274846f976e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great Piano Academy</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Companies, agencies, institutions, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mayfair</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Countries, cities, states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the City of London</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Countries, cities, states</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                entity label                              explanation\n",
       "0  Great Piano Academy   ORG  Companies, agencies, institutions, etc.\n",
       "1              Mayfair   GPE                Countries, cities, states\n",
       "2   the City of London   GPE                Countries, cities, states"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'entity': [ent.text for ent in piano_class_doc.ents],\n",
    "    'label': [ent.label_ for ent in piano_class_doc.ents],\n",
    "    'explanation': [spacy.explain(ent.label_) for ent in piano_class_doc.ents],\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
